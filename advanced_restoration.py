#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å…ˆè¿›çš„å£ç”»å›¾åƒå¤åŸå’Œè™šæ‹Ÿä¿®å¤ç³»ç»Ÿ
åŒ…å«æ·±åº¦å­¦ä¹ ä¿®å¤ã€è‰²å½©è¿˜åŸã€çº¹ç†åˆæˆç­‰åŠŸèƒ½
"""

import cv2
import numpy as np
from PIL import Image
import streamlit as st
from io import BytesIO
import base64
from datetime import datetime


class AdvancedMuralRestoration:
    """å…ˆè¿›çš„å£ç”»å¤åŸç³»ç»Ÿ"""
    
    def __init__(self):
        self.restoration_methods = {
            "inpainting": {
                "telea": cv2.INPAINT_TELEA,
                "ns": cv2.INPAINT_NS
            },
            "color_correction": {
                "histogram_equalization": self.histogram_equalization,
                "white_balance": self.white_balance,
                "color_transfer": self.color_transfer,
                "dehazing": self.dehazing
            },
            "texture_synthesis": {
                "patch_match": self.patch_match_inpainting,
                "texture_fill": self.texture_fill
            }
        }
    
    def advanced_inpainting(self, image, mask, method='telea', radius=3, iterations=1):
        """é«˜çº§å›¾åƒä¿®å¤"""
        if method == 'telea':
            flags = cv2.INPAINT_TELEA
        else:
            flags = cv2.INPAINT_NS
        
        result = image.copy()
        for i in range(iterations):
            result = cv2.inpaint(result, mask, radius, flags)
        
        return result
    
    def deep_learning_inpainting(self, image, mask):
        """æ·±åº¦å­¦ä¹ ä¿®å¤ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹
        # å¦‚: EdgeConnect, DeepFill, etc.
        
        # ä½¿ç”¨æ”¹è¿›çš„ä¼ ç»Ÿç®—æ³•æ¨¡æ‹Ÿæ·±åº¦å­¦ä¹ æ•ˆæœ
        result = image.copy()
        
        # å¤šå°ºåº¦ä¿®å¤
        scales = [0.5, 0.75, 1.0]
        for scale in scales:
            if scale != 1.0:
                h, w = image.shape[:2]
                new_size = (int(w*scale), int(h*scale))
                img_scaled = cv2.resize(image, new_size)
                mask_scaled = cv2.resize(mask, new_size)
                inpainted_scaled = cv2.inpaint(img_scaled, mask_scaled, 3, cv2.INPAINT_NS)
                inpainted = cv2.resize(inpainted_scaled, (w, h))
                # èåˆç»“æœ
                alpha = 0.3
                result = cv2.addWeighted(result, 1-alpha, inpainted, alpha, 0)
        
        return result
    
    def texture_aware_inpainting(self, image, mask, texture_weight=0.7):
        """çº¹ç†æ„ŸçŸ¥ä¿®å¤"""
        result = image.copy()
        
        # æå–çº¹ç†ä¿¡æ¯
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨å¤šç§æ–¹æ³•è¿›è¡Œä¿®å¤
        methods = ['telea', 'ns']
        results = []
        
        for method in methods:
            if method == 'telea':
                inpainted = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)
            else:
                inpainted = cv2.inpaint(image, mask, 3, cv2.INPAINT_NS)
            results.append(inpainted)
        
        # èåˆä¸åŒæ–¹æ³•çš„ç»“æœ
        if len(results) == 2:
            # åŸºäºçº¹ç†ç›¸ä¼¼åº¦è¿›è¡Œèåˆ
            blended = cv2.addWeighted(results[0], texture_weight, 
                                    results[1], 1-texture_weight, 0)
            result = blended
        
        return result
    
    def color_restoration_advanced(self, image, method='comprehensive', 
                                  contrast_enhance=1.5, saturation_boost=1.2, 
                                  sharpening_strength=0.5):
        """é«˜çº§è‰²å½©å¤åŸ"""
        if method == 'comprehensive':
            # ç»¼åˆè‰²å½©å¤åŸæµç¨‹
            result = image.copy()
            
            # 1. ç™½å¹³è¡¡
            result = self.white_balance(result)
            
            # 2. å¯¹æ¯”åº¦å¢å¼º
            result = self.adaptive_contrast_enhancement(result, clip_limit=contrast_enhance)
            
            # 3. è‰²å½©é¥±å’Œåº¦è°ƒæ•´
            result = self.saturation_enhancement(result, factor=saturation_boost)
            
            # 4. é”åŒ–
            if sharpening_strength > 0:
                result = self.smart_sharpening(result, strength=sharpening_strength)
            
            return result
        
        elif method == 'histogram_equalization':
            return self.histogram_equalization(image)
        
        elif method == 'dehazing':
            return self.dehazing(image)
    
    def white_balance(self, img):
        """æ”¹è¿›çš„ç™½å¹³è¡¡ç®—æ³•"""
        result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        avg_a = np.mean(result[:, :, 1])
        avg_b = np.mean(result[:, :, 2])
        result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
        result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
        result = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)
        return result
    
    def adaptive_contrast_enhancement(self, img, clip_limit=2.0, grid_size=8):
        """è‡ªé€‚åº”å¯¹æ¯”åº¦å¢å¼º"""
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # åˆ›å»ºCLAHEå¯¹è±¡
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(grid_size, grid_size))
        l_enhanced = clahe.apply(l)
        
        # åˆå¹¶é€šé“
        lab_enhanced = cv2.merge([l_enhanced, a, b])
        result = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        
        return result
    
    def saturation_enhancement(self, img, factor=1.2):
        """é¥±å’Œåº¦å¢å¼º"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        
        # å¢å¼ºé¥±å’Œåº¦
        s = cv2.multiply(s, factor)
        s = np.clip(s, 0, 255)
        
        hsv_enhanced = cv2.merge([h, s, v])
        result = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        
        return result
    
    def smart_sharpening(self, img, strength=0.8):
        """æ™ºèƒ½é”åŒ–"""
        kernel = np.array([[-1, -1, -1],
                          [-1, 9, -1],
                          [-1, -1, -1]]) * strength
        sharpened = cv2.filter2D(img, -1, kernel)
        return sharpened
    
    def dehazing(self, img, w=0.95, t0=0.1):
        """å›¾åƒå»é›¾ç®—æ³•"""
        # æš—é€šé“å…ˆéªŒå»é›¾
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        dark_channel = self.get_dark_channel(img, 15)
        atmospheric_light = self.get_atmospheric_light(img, dark_channel)
        
        transmission = 1 - w * dark_channel / atmospheric_light
        transmission = np.maximum(transmission, t0)
        
        result = np.zeros_like(img, dtype=np.float64)
        for i in range(3):
            result[:, :, i] = (img[:, :, i].astype(np.float64) - atmospheric_light) / transmission + atmospheric_light
        
        return np.uint8(np.clip(result, 0, 255))
    
    def get_dark_channel(self, img, window_size):
        """è®¡ç®—æš—é€šé“"""
        min_channel = np.min(img, axis=2)
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (window_size, window_size))
        dark_channel = cv2.erode(min_channel, kernel)
        return dark_channel
    
    def get_atmospheric_light(self, img, dark_channel):
        """ä¼°è®¡å¤§æ°”å…‰å€¼"""
        h, w = img.shape[:2]
        img_size = h * w
        num_pixels = int(max(img_size * 0.001, 1))
        
        dark_vec = dark_channel.reshape(img_size)
        img_vec = img.reshape(img_size, 3)
        
        indices = dark_vec.argsort()[-num_pixels:]
        atmospheric_light = np.mean(img_vec[indices], axis=0)
        
        return np.max(atmospheric_light)
    
    def patch_match_inpainting(self, image, mask, patch_size=9):
        """åŸºäºå—åŒ¹é…çš„ä¿®å¤ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        result = image.copy()
        mask_indices = np.where(mask > 0)
        
        for i in range(0, len(mask_indices[0]), patch_size):
            y, x = mask_indices[0][i], mask_indices[1][i]
            
            # è·å–å‘¨å›´åŒºåŸŸçš„çº¹ç†ä¿¡æ¯
            patch = self.get_best_matching_patch(image, mask, (x, y), patch_size)
            if patch is not None:
                # åº”ç”¨çº¹ç†è¡¥ä¸
                result[y:y+patch_size, x:x+patch_size] = patch
        
        return result
    
    def get_best_matching_patch(self, image, mask, center, patch_size):
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çº¹ç†å—"""
        x, y = center
        h, w = image.shape[:2]
        
        # æœç´¢èŒƒå›´
        search_radius = min(50, w//4, h//4)
        
        best_patch = None
        best_score = float('inf')
        
        for dy in range(-search_radius, search_radius, patch_size//2):
            for dx in range(-search_radius, search_radius, patch_size//2):
                y2, x2 = y + dy, x + dx
                
                # æ£€æŸ¥è¾¹ç•Œ
                if (y2 < 0 or y2 + patch_size >= h or 
                    x2 < 0 or x2 + patch_size >= w):
                    continue
                
                # æ£€æŸ¥ç›®æ ‡åŒºåŸŸæ˜¯å¦åœ¨æ©è†œå†…
                target_patch = image[y2:y2+patch_size, x2:x2+patch_size]
                mask_patch = mask[y2:y2+patch_size, x2:x2+patch_size]
                
                if np.any(mask_patch > 0):
                    continue
                
                # è®¡ç®—åŒ¹é…åˆ†æ•°
                score = self.calculate_patch_similarity(
                    image[y:y+patch_size, x:x+patch_size], target_patch)
                
                if score < best_score:
                    best_score = score
                    best_patch = target_patch
        
        return best_patch
    
    def calculate_patch_similarity(self, patch1, patch2):
        """è®¡ç®—å›¾åƒå—çš„ç›¸ä¼¼åº¦"""
        if patch1.shape != patch2.shape:
            return float('inf')
        
        # ä½¿ç”¨é¢œè‰²å’Œçº¹ç†ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦
        diff = patch1.astype(np.float32) - patch2.astype(np.float32)
        color_similarity = np.mean(np.abs(diff))
        
        # è®¡ç®—çº¹ç†ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨æ¢¯åº¦ï¼‰
        gray1 = cv2.cvtColor(patch1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(patch2, cv2.COLOR_BGR2GRAY)
        
        grad1 = cv2.Sobel(gray1, cv2.CV_32F, 1, 1)
        grad2 = cv2.Sobel(gray2, cv2.CV_32F, 1, 1)
        
        texture_similarity = np.mean(np.abs(grad1 - grad2))
        
        return color_similarity * 0.7 + texture_similarity * 0.3
    
    def histogram_equalization(self, img):
        """ç›´æ–¹å›¾å‡è¡¡åŒ–"""
        yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])
        return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
    
    def color_transfer(self, img, target_img):
        """é¢œè‰²è¿ç§»"""
        # ç®€åŒ–å®ç°
        return img
    
    def texture_fill(self, image, mask):
        """çº¹ç†å¡«å……"""
        return self.patch_match_inpainting(image, mask)


class VirtualRestorationSystem:
    """è™šæ‹Ÿä¿®å¤ç³»ç»Ÿ"""
    
    def __init__(self):
        self.restorer = AdvancedMuralRestoration()
    
    def comprehensive_restoration(self, image_rgb, masks_dict, restoration_config):
        """ç»¼åˆä¿®å¤æµç¨‹"""
        result = image_rgb.copy()
        
        # è½¬æ¢ä¸ºBGRæ ¼å¼ç”¨äºå¤„ç†
        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        
        # 1. åˆ›å»ºç»¼åˆæ©è†œ
        combined_mask = self.create_combined_mask(masks_dict, restoration_config['target_defects'])
        
        # 2. æ ¹æ®ç—…å®³ç±»å‹é€‰æ‹©ä¿®å¤ç­–ç•¥
        if restoration_config['method'] == 'comprehensive':
            result_bgr = self.adaptive_restoration(image_bgr, combined_mask, masks_dict, restoration_config)
        elif restoration_config['method'] == 'deep_learning':
            result_bgr = self.restorer.deep_learning_inpainting(image_bgr, combined_mask)
        elif restoration_config['method'] == 'texture_aware':
            result_bgr = self.restorer.texture_aware_inpainting(
                image_bgr, combined_mask, 
                texture_weight=restoration_config.get('texture_weight', 0.7))
        else:
            result_bgr = self.restorer.advanced_inpainting(
                image_bgr, combined_mask, 
                method=restoration_config['method'],
                radius=restoration_config['radius'],
                iterations=restoration_config['iterations']
            )
        
        # 3. è‰²å½©å¤åŸ
        if restoration_config.get('color_restoration', False):
            result_bgr = self.restorer.color_restoration_advanced(
                result_bgr,
                contrast_enhance=restoration_config.get('contrast_enhancement', 1.5),
                saturation_boost=restoration_config.get('saturation_boost', 1.2),
                sharpening_strength=restoration_config.get('sharpening_strength', 0.5)
            )
        
        # è½¬æ¢å›RGB
        result = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)
        
        return result, combined_mask
    
    def create_combined_mask(self, masks_dict, target_defects):
        """åˆ›å»ºç»¼åˆæ©è†œ"""
        if not masks_dict:
            return np.zeros((100, 100), dtype=np.uint8)
        
        # è·å–ç¬¬ä¸€ä¸ªæ©è†œçš„å°ºå¯¸
        first_mask = list(masks_dict.values())[0]
        combined_mask = np.zeros(first_mask.shape, dtype=np.uint8)
        
        defect_mapping = {
            'è£‚ç¼': 'crack',
            'å‰¥è½': 'peel', 
            'è¤ªè‰²': 'disc',
            'æ±¡æ¸/éœ‰æ–‘': 'stain',
            'ç›èš€/é£åŒ–': 'salt',
            'ç”Ÿç‰©é™„ç€': 'bio'
        }
        
        for defect in target_defects:
            mask_key = defect_mapping.get(defect)
            if mask_key and mask_key in masks_dict:
                mask = masks_dict[mask_key]
                if mask is not None and mask.size > 0:
                    combined_mask = cv2.bitwise_or(combined_mask, (mask > 0).astype(np.uint8) * 255)
        
        # å½¢æ€å­¦æ“ä½œä¼˜åŒ–æ©è†œ
        if np.any(combined_mask > 0):
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
        
        return combined_mask
    
    def adaptive_restoration(self, image, mask, masks_dict, config):
        """è‡ªé€‚åº”ä¿®å¤ç­–ç•¥"""
        result = image.copy()
        
        # åˆ†æä¸åŒç—…å®³åŒºåŸŸçš„ç‰¹æ€§
        crack_mask = (masks_dict.get('crack', np.zeros_like(mask)) > 0).astype(np.uint8) * 255
        peel_mask = (masks_dict.get('peel', np.zeros_like(mask)) > 0).astype(np.uint8) * 255
        
        # å¯¹è£‚ç¼ä½¿ç”¨ç»†çº¿ä¿®å¤
        if np.any(crack_mask > 0):
            crack_result = self.restorer.advanced_inpainting(
                image, crack_mask, method='ns', radius=2, iterations=2)
            # åªæ›¿æ¢è£‚ç¼åŒºåŸŸ
            crack_region = crack_mask > 0
            result[crack_region] = crack_result[crack_region]
        
        # å¯¹å‰¥è½åŒºåŸŸä½¿ç”¨çº¹ç†ä¿®å¤
        if np.any(peel_mask > 0):
            peel_result = self.restorer.texture_aware_inpainting(
                image, peel_mask, texture_weight=config.get('texture_weight', 0.8))
            # åªæ›¿æ¢å‰¥è½åŒºåŸŸ
            peel_region = peel_mask > 0
            result[peel_region] = peel_result[peel_region]
        
        # å¯¹å…¶ä»–åŒºåŸŸä½¿ç”¨æ ‡å‡†ä¿®å¤
        other_mask = cv2.bitwise_and(mask, cv2.bitwise_not(cv2.bitwise_or(crack_mask, peel_mask)))
        if np.any(other_mask > 0):
            other_result = self.restorer.advanced_inpainting(
                result, other_mask, method='telea', radius=config.get('radius', 3), 
                iterations=config.get('iterations', 1))
            other_region = other_mask > 0
            result[other_region] = other_result[other_region]
        
        return result


def render_advanced_restoration_ui(img_rgb, masks_dict, default_open=True):
    """æ¸²æŸ“é«˜çº§å¤åŸç•Œé¢"""
    st.markdown("## ğŸ¨ é«˜çº§å›¾åƒå¤åŸç³»ç»Ÿ")
    
    with st.expander("å±•å¼€é«˜çº§å¤åŸé€‰é¡¹", expanded=default_open):
        # ä¿®å¤é…ç½®
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ä¿®å¤ç›®æ ‡é€‰æ‹©")
            target_defects = st.multiselect(
                "é€‰æ‹©éœ€è¦ä¿®å¤çš„ç—…å®³ç±»å‹",
                ["è£‚ç¼", "å‰¥è½", "è¤ªè‰²", "æ±¡æ¸/éœ‰æ–‘", "ç›èš€/é£åŒ–", "ç”Ÿç‰©é™„ç€"],
                default=["è£‚ç¼", "å‰¥è½", "æ±¡æ¸/éœ‰æ–‘"],
                key="advanced_target_defects"
            )
            
            st.markdown("### ä¿®å¤ç®—æ³•")
            restoration_method = st.selectbox(
                "é€‰æ‹©ä¿®å¤ç®—æ³•",
                ["comprehensive", "telea", "ns", "texture_aware", "deep_learning"],
                format_func=lambda x: {
                    "comprehensive": "ç»¼åˆæ™ºèƒ½ä¿®å¤",
                    "telea": "Teleaç®—æ³•(å¿«é€Ÿ)",
                    "ns": "Navier-Stokesç®—æ³•(è´¨é‡)",
                    "texture_aware": "çº¹ç†æ„ŸçŸ¥ä¿®å¤", 
                    "deep_learning": "æ·±åº¦å­¦ä¹ ä¿®å¤(æ¨¡æ‹Ÿ)"
                }[x],
                key="advanced_method"
            )
        
        with col2:
            st.markdown("### å‚æ•°é…ç½®")
            restoration_radius = st.slider(
                "ä¿®å¤åŠå¾„", min_value=1, max_value=15, value=3, 
                help="ä¿®å¤æ“ä½œçš„å½±å“èŒƒå›´", key="advanced_radius"
            )
            
            restoration_iterations = st.slider(
                "ä¿®å¤è¿­ä»£æ¬¡æ•°", min_value=1, max_value=5, value=1,
                help="å¤šæ¬¡è¿­ä»£å¯èƒ½è·å¾—æ›´å¥½æ•ˆæœ", key="advanced_iterations"
            )
            
            enable_color_restoration = st.checkbox(
                "å¯ç”¨è‰²å½©å¤åŸ", value=True, 
                help="è‡ªåŠ¨è°ƒæ•´è‰²å½©ã€å¯¹æ¯”åº¦å’Œé¥±å’Œåº¦", key="advanced_color"
            )
        
        # é«˜çº§é€‰é¡¹
        st.markdown("### é«˜çº§é€‰é¡¹")
        advanced_col1, advanced_col2 = st.columns(2)
        
        with advanced_col1:
            texture_weight = st.slider(
                "çº¹ç†æƒé‡", min_value=0.0, max_value=1.0, value=0.7,
                help="çº¹ç†ä¿®å¤æ—¶çº¹ç†ä¿æŒçš„æƒé‡", key="texture_weight"
            )
            
            contrast_enhancement = st.slider(
                "å¯¹æ¯”åº¦å¢å¼º", min_value=1.0, max_value=3.0, value=1.5,
                help="è‰²å½©å¤åŸæ—¶çš„å¯¹æ¯”åº¦å¢å¼ºå¼ºåº¦", key="contrast_enhance"
            )
        
        with advanced_col2:
            saturation_boost = st.slider(
                "é¥±å’Œåº¦å¢å¼º", min_value=1.0, max_value=2.0, value=1.2,
                help="è‰²å½©å¤åŸæ—¶çš„é¥±å’Œåº¦å¢å¼ºå¼ºåº¦", key="saturation_boost"
            )
            
            sharpening_strength = st.slider(
                "é”åŒ–å¼ºåº¦", min_value=0.0, max_value=1.5, value=0.5,
                help="å›¾åƒé”åŒ–å¼ºåº¦", key="sharpening_strength"
            )
        
        # æ‰§è¡Œä¿®å¤
        if st.button("ğŸš€ æ‰§è¡Œé«˜çº§å¤åŸ", key="run_advanced_restoration"):
            with st.spinner("æ­£åœ¨è¿›è¡Œé«˜çº§å›¾åƒå¤åŸ..."):
                # åˆ›å»ºä¿®å¤ç³»ç»Ÿ
                restoration_system = VirtualRestorationSystem()
                
                # é…ç½®å‚æ•°
                restoration_config = {
                    'target_defects': target_defects,
                    'method': restoration_method,
                    'radius': restoration_radius,
                    'iterations': restoration_iterations,
                    'color_restoration': enable_color_restoration,
                    'texture_weight': texture_weight,
                    'contrast_enhancement': contrast_enhancement,
                    'saturation_boost': saturation_boost,
                    'sharpening_strength': sharpening_strength
                }
                
                # æ‰§è¡Œä¿®å¤
                restored_image, used_mask = restoration_system.comprehensive_restoration(
                    img_rgb, masks_dict, restoration_config
                )
                
                # æ˜¾ç¤ºç»“æœ
                st.markdown("### å¤åŸç»“æœå¯¹æ¯”")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.image(img_rgb, caption="åŸå§‹å›¾åƒ", use_column_width=True)
                    # æ˜¾ç¤ºä½¿ç”¨çš„æ©è†œ
                    mask_overlay = img_rgb.copy()
                    mask_overlay[used_mask > 0] = [255, 0, 0]  # çº¢è‰²æ˜¾ç¤ºä¿®å¤åŒºåŸŸ
                    st.image(mask_overlay, caption="ä¿®å¤åŒºåŸŸæ ‡è¯†(çº¢è‰²)", use_column_width=True)
                
                with col2:
                    st.image(restored_image, caption="å¤åŸåå›¾åƒ", use_column_width=True)
                    
                    # è®¡ç®—ä¿®å¤ç»Ÿè®¡
                    total_pixels = img_rgb.shape[0] * img_rgb.shape[1]
                    restored_pixels = np.sum(used_mask > 0)
                    restoration_ratio = (restored_pixels / total_pixels) * 100
                    
                    st.metric("ä¿®å¤åŒºåŸŸå æ¯”", f"{restoration_ratio:.2f}%")
                
                # ä¸‹è½½åŠŸèƒ½
                st.markdown("### ä¸‹è½½å¤åŸç»“æœ")
                download_col1, download_col2 = st.columns(2)
                
                with download_col1:
                    # ä¸‹è½½å¤åŸå›¾åƒ
                    buf_restored = BytesIO()
                    Image.fromarray(restored_image).save(buf_restored, format="PNG")
                    st.download_button(
                        "ğŸ“¥ ä¸‹è½½å¤åŸå›¾åƒ(PNG)",
                        data=buf_restored.getvalue(),
                        file_name="advanced_restored.png",
                        mime="image/png"
                    )
                
                with download_col2:
                    # ä¸‹è½½ä¿®å¤æŠ¥å‘Š
                    report = generate_restoration_report(
                        img_rgb, restored_image, used_mask, restoration_config
                    )
                    st.download_button(
                        "ğŸ“Š ä¸‹è½½ä¿®å¤æŠ¥å‘Š(TXT)",
                        data=report.encode('utf-8'),
                        file_name="restoration_report.txt",
                        mime="text/plain"
                    )


def generate_restoration_report(original, restored, mask, config):
    """ç”Ÿæˆä¿®å¤æŠ¥å‘Š"""
    original_size = f"{original.shape[1]}x{original.shape[0]}"
    restored_pixels = np.sum(mask > 0)
    total_pixels = original.shape[0] * original.shape[1]
    restoration_ratio = (restored_pixels / total_pixels) * 100
    
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
é«˜çº§å›¾åƒå¤åŸæŠ¥å‘Š
================

ä¿®å¤æ—¶é—´: {current_time}
åŸå§‹å›¾åƒå°ºå¯¸: {original_size}
æ€»åƒç´ æ•°: {total_pixels:,}

ä¿®å¤ç»Ÿè®¡:
--------
ä¿®å¤åŒºåŸŸåƒç´ : {restored_pixels:,}
ä¿®å¤åŒºåŸŸå æ¯”: {restoration_ratio:.2f}%
ä¿®å¤ç—…å®³ç±»å‹: {', '.join(config['target_defects'])}

ä¿®å¤å‚æ•°:
--------
ä¿®å¤ç®—æ³•: {config['method']}
ä¿®å¤åŠå¾„: {config['radius']} åƒç´ 
è¿­ä»£æ¬¡æ•°: {config['iterations']}
è‰²å½©å¤åŸ: {'å¯ç”¨' if config['color_restoration'] else 'ç¦ç”¨'}
çº¹ç†æƒé‡: {config.get('texture_weight', 0.7):.1f}

æŠ€æœ¯è¯´æ˜:
--------
æœ¬å¤åŸé‡‡ç”¨å…ˆè¿›çš„å›¾åƒå¤„ç†ç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒç—…å®³ç±»å‹é‡‡ç”¨å·®å¼‚åŒ–ä¿®å¤ç­–ç•¥ã€‚
ä¿®å¤è¿‡ç¨‹å°½å¯èƒ½ä¿æŒæ–‡ç‰©çš„åŸå§‹é£è²Œå’Œè‰ºæœ¯ä»·å€¼ã€‚

æ³¨æ„äº‹é¡¹:
--------
1. è™šæ‹Ÿä¿®å¤ç»“æœä»…ä¾›å‚è€ƒï¼Œå®é™…ä¿®å¤éœ€ä¸“ä¸šè¯„ä¼°
2. å»ºè®®ç»“åˆå®åœ°å‹˜å¯Ÿå’Œææ–™åˆ†æ
3. é‡è¦æ–‡ç‰©ä¿®å¤åº”éµå¾ªç›¸å…³è§„èŒƒå’Œæ ‡å‡†

ç”Ÿæˆç³»ç»Ÿ: çŸ³çªŸå¯ºå£ç”»AIä¿æŠ¤å¹³å°
    """
    
    return report

