#!/usr/bin/env python
# -*- coding: utf-8 -*-
# app.py
import streamlit as st
import cv2
import numpy as np
import pandas as pd
from PIL import Image
from io import BytesIO
from datetime import datetime
import os
import time


def _sanitize_windows_path_env():
    """Correct malformed drive-relative PATH entries that break DLL loading on Windows."""
    if os.name != "nt":
        return
    path_env = os.environ.get("PATH")
    if not path_env:
        return
    parts = path_env.split(os.pathsep)
    updated = []
    mutated = False
    for entry in parts:
        if (
            len(entry) >= 3
            and entry[1] == ":"
            and entry[2] not in ("\\", "/")
            and not entry.startswith("\\\\")
        ):
            candidate = entry[:2] + "\\" + entry[2:]
            if os.path.isdir(candidate):
                updated.append(candidate)
                mutated = True
                continue
        updated.append(entry)
    if mutated:
        os.environ["PATH"] = os.pathsep.join(updated)


_sanitize_windows_path_env()
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle,
    Image as RLImage, PageBreak
)
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm, inch
from reportlab.lib import colors
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import base64
import os
import sys

# æ‰€æœ‰åŠŸèƒ½æ¨¡å—å·²æ•´åˆåˆ°app.pyä¸­
IMPROVED_DETECTION_AVAILABLE = True
KNOWLEDGE_BASE_AVAILABLE = True
ADVANCED_RESTORATION_AVAILABLE = True
IMPROVED_UI_AVAILABLE = True

# æ·±åº¦å­¦ä¹ ç›¸å…³å¯¼å…¥
try:
    # ä¿®å¤Windowsä¸Šçš„PyTorch DLLè·¯å¾„é—®é¢˜
    import os
    import sys
    
    # è®¾ç½®ç¯å¢ƒå˜é‡æ¥é¿å…DLLè·¯å¾„é—®é¢˜
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    os.environ['OMP_NUM_THREADS'] = '1'
    
    # å°è¯•å¯¼å…¥PyTorch
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, Dataset
        import torchvision.transforms as transforms
        from torchvision import models
        import albumentations as A
        from albumentations.pytorch import ToTensorV2
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics import classification_report, confusion_matrix
        DEEP_LEARNING_AVAILABLE = True
    except OSError as e:
        if "å‚æ•°é”™è¯¯" in str(e) or "WinError 87" in str(e):
            # å¦‚æœPyTorch DLLæœ‰é—®é¢˜ï¼Œç¦ç”¨æ·±åº¦å­¦ä¹ åŠŸèƒ½
            DEEP_LEARNING_AVAILABLE = False
            print(f"è­¦å‘Š: PyTorch DLLåŠ è½½å¤±è´¥ï¼Œæ·±åº¦å­¦ä¹ åŠŸèƒ½å·²ç¦ç”¨: {e}")
        else:
            raise e
except ImportError as e:
    DEEP_LEARNING_AVAILABLE = False
    print(f"æ·±åº¦å­¦ä¹ åŠŸèƒ½ä¸å¯ç”¨: {e}")
try:
    import onnxruntime as ort  # æ·±åº¦åˆ†å‰²æ¨ç†
except Exception:
    ort = None

# Optional deps for 3D
try:
    import open3d as o3d  # type: ignore
except Exception:
    o3d = None
try:
    import plotly.express as px
    import pandas as pd  # already imported above but for safety when 3D used
except Exception:
    px = None
try:
    from rapidocr_onnxruntime import RapidOCR  # è½»é‡OCRï¼ŒåŸºäº onnxruntime
except Exception:
    RapidOCR = None

# å¤šæ¨¡æ€èåˆç›¸å…³ä¾èµ–
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from transformers import AutoTokenizer, AutoModel
    import networkx as nx
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import PCA
    import json
    MULTIMODAL_AVAILABLE = True
except Exception:
    MULTIMODAL_AVAILABLE = False

# æ·±åº¦å­¦ä¹ ç›¸å…³ä¾èµ–
try:
    import torchvision
    import torchvision.transforms as transforms
    from torch.utils.data import DataLoader, Dataset
    from torch.optim import Adam, SGD
    from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.model_selection import train_test_split
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    DEEP_LEARNING_AVAILABLE = True
except Exception:
    DEEP_LEARNING_AVAILABLE = False

st.set_page_config(
    page_title="çŸ³çªŸå¯ºå£ç”»ç—…å®³AIè¯†åˆ«å·¥å…·ï¼ˆå‡çº§ç‰ˆï¼‰",
    layout="wide",
    page_icon="ğŸ›ï¸",
    initial_sidebar_state="expanded"
)

# ---------------------------
# æ‰€æœ‰åŠŸèƒ½æ¨¡å—å®šä¹‰ï¼ˆæ•´åˆåˆ°app.pyä¸­ï¼‰
# ---------------------------
# æ³¨æ„ï¼šUIå‡½æ•°éœ€è¦åœ¨è°ƒç”¨å‰å®šä¹‰ï¼Œæ‰€ä»¥å…ˆå®šä¹‰UIå‡½æ•°
import sqlite3
import json
from typing import List, Dict, Optional, Any, Tuple
import hashlib
from pathlib import Path

# å¯é€‰ä¾èµ–æ£€æŸ¥
try:
    from scipy import ndimage
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from skimage import filters, morphology, measure
    from skimage.feature import peak_local_maxima, local_binary_pattern
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# ---------------------------
# æ”¹è¿›çš„ç—…å®³æ£€æµ‹ç®—æ³•ï¼ˆæ•´åˆè‡ªimproved_detection.pyï¼‰
# ---------------------------
def detect_cracks_improved(gray: np.ndarray, 
                          adaptive_threshold: bool = True,
                          use_watershed: bool = True) -> Tuple[List[Tuple[int, int, int, int]], np.ndarray]:
    """æ”¹è¿›çš„è£‚ç¼æ£€æµ‹ç®—æ³•"""
    # 1. é¢„å¤„ç†ï¼šå»å™ªå’Œå¢å¼ºå¯¹æ¯”åº¦
    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
    
    # 2. å¤šå°ºåº¦è¾¹ç¼˜æ£€æµ‹
    edges1 = cv2.Canny(denoised, 50, 150, apertureSize=3)
    edges2 = cv2.Canny(denoised, 30, 100, apertureSize=5)
    edges_combined = cv2.bitwise_or(edges1, edges2)
    
    # 3. æ–¹å‘æ¢¯åº¦åˆ†æ
    grad_x = cv2.Sobel(denoised, cv2.CV_64F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(denoised, cv2.CV_64F, 0, 1, ksize=3)
    magnitude = np.sqrt(grad_x**2 + grad_y**2)
    angle = np.arctan2(grad_y, grad_x)
    
    # 4. è‡ªé€‚åº”é˜ˆå€¼æˆ–å›ºå®šé˜ˆå€¼
    if adaptive_threshold:
        th = cv2.adaptiveThreshold(
            (magnitude * 255 / magnitude.max()).astype(np.uint8),
            255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
    else:
        _, th = cv2.threshold(
            (magnitude * 255 / magnitude.max()).astype(np.uint8),
            30, 255, cv2.THRESH_BINARY
        )
    
    # 5. å½¢æ€å­¦æ“ä½œ
    kernel_line_h = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))
    kernel_line_v = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 15))
    kernel_diag1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    
    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel_line_h, iterations=2)
    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel_line_v, iterations=2)
    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel_diag1, iterations=1)
    
    # 6. ç»†åŒ–å¤„ç†ï¼ˆå¯é€‰ï¼‰
    if use_watershed:
        dist_transform = cv2.distanceTransform(th, cv2.DIST_L2, 5)
        _, sure_fg = cv2.threshold(dist_transform, 0.3 * dist_transform.max(), 255, 0)
        sure_fg = np.uint8(sure_fg)
        unknown = cv2.subtract(th, sure_fg)
        _, markers = cv2.connectedComponents(sure_fg)
        markers = markers + 1
        markers[unknown == 255] = 0
        markers = cv2.watershed(cv2.cvtColor(denoised, cv2.COLOR_GRAY2BGR), markers)
        th = (markers > 1).astype(np.uint8) * 255
    
    # 7. è¿é€šåŸŸåˆ†æå’Œè¿‡æ»¤
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(th, connectivity=8)
    boxes = []
    mask = np.zeros_like(th)
    
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < 50:
            continue
        
        x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], \
                     stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        
        aspect_ratio = max(w, h) / max(min(w, h), 1)
        extent = area / (w * h)
        
        component_mask = (labels == i).astype(np.uint8)
        component_angles = angle[component_mask > 0]
        if len(component_angles) > 10:
            angle_std = np.std(component_angles)
            angle_consistency = 1.0 / (1.0 + angle_std)
        else:
            angle_consistency = 0.5
        
        if (aspect_ratio > 3.0) or (area < 500 and aspect_ratio > 2.0) or \
           (angle_consistency > 0.7 and aspect_ratio > 2.0):
            boxes.append((x, y, w, h))
            mask[component_mask > 0] = 255
    
    return boxes, mask


def detect_peeling_improved(hsv: np.ndarray,
                            use_texture_analysis: bool = True) -> Tuple[List[Tuple[int, int, int, int]], np.ndarray]:
    """æ”¹è¿›çš„å‰¥è½æ£€æµ‹ç®—æ³•"""
    h, s, v = cv2.split(hsv)
    low_sat_mask = cv2.inRange(hsv, (0, 0, 40), (180, 70, 255))
    
    gray = cv2.cvtColor(cv2.merge([h, s, v]), cv2.COLOR_HSV2BGR)
    gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)
    
    if use_texture_analysis and SKIMAGE_AVAILABLE:
        lbp = local_binary_pattern(gray, 8, 1, method='uniform')
        lbp_hist = np.histogram(lbp.ravel(), bins=10, range=(0, 10))[0]
        lbp_hist = lbp_hist / (lbp_hist.sum() + 1e-6)
        texture_entropy = -np.sum(lbp_hist * np.log(lbp_hist + 1e-6))
        
        kernel = np.ones((5, 5), np.float32) / 25
        local_mean = cv2.filter2D(gray.astype(np.float32), -1, kernel)
        local_var = cv2.filter2D((gray.astype(np.float32) - local_mean)**2, -1, kernel)
        high_var_mask = (local_var > np.percentile(local_var, 60)).astype(np.uint8) * 255
        
        combined_mask = cv2.bitwise_and(low_sat_mask, high_var_mask)
    else:
        combined_mask = low_sat_mask
    
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel, iterations=1)
    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(combined_mask, connectivity=8)
    boxes = []
    mask = np.zeros_like(combined_mask)
    
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < 400:
            continue
        
        x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], \
                     stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        
        component_mask = (labels == i).astype(np.uint8)
        component_gray = gray[component_mask > 0]
        if len(component_gray) > 0:
            gray_std = np.std(component_gray)
            if gray_std < 40:
                boxes.append((x, y, w, h))
                mask[component_mask > 0] = 255
    
    return boxes, mask


def detect_discoloration_improved(hsv: np.ndarray,
                                 use_color_clustering: bool = True) -> Tuple[List[Tuple[int, int, int, int]], np.ndarray]:
    """æ”¹è¿›çš„è¤ªè‰²æ£€æµ‹ç®—æ³•"""
    h, s, v = cv2.split(hsv)
    light_mask = cv2.inRange(hsv, (0, 0, 180), (180, 90, 255))
    
    if use_color_clustering and SKLEARN_AVAILABLE:
        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        pixels = bgr.reshape(-1, 3).astype(np.float32)
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        labels_flat = kmeans.fit_predict(pixels)
        labels_img = labels_flat.reshape(bgr.shape[:2])
        
        cluster_colors = kmeans.cluster_centers_
        cluster_brightness = np.mean(cluster_colors, axis=1)
        brightest_cluster = np.argmax(cluster_brightness)
        
        brightest_color = cluster_colors[brightest_cluster]
        brightest_hsv = cv2.cvtColor(np.uint8([[brightest_color]]), cv2.COLOR_BGR2HSV)[0][0]
        
        if brightest_hsv[1] < 80:
            cluster_mask = (labels_img == brightest_cluster).astype(np.uint8) * 255
            combined_mask = cv2.bitwise_and(light_mask, cluster_mask)
        else:
            combined_mask = light_mask
    else:
        combined_mask = light_mask
    
    gray = cv2.cvtColor(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR), cv2.COLOR_BGR2GRAY)
    kernel = np.ones((9, 9), np.float32) / 81
    local_mean = cv2.filter2D(gray.astype(np.float32), -1, kernel)
    local_std = np.sqrt(cv2.filter2D((gray.astype(np.float32) - local_mean)**2, -1, kernel))
    low_contrast_mask = (local_std < np.percentile(local_std, 30)).astype(np.uint8) * 255
    
    final_mask = cv2.bitwise_and(combined_mask, low_contrast_mask)
    
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_OPEN, kernel, iterations=1)
    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel, iterations=1)
    
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(final_mask, connectivity=8)
    boxes = []
    mask = np.zeros_like(final_mask)
    
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < 300:
            continue
        
        x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], \
                     stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        
        boxes.append((x, y, w, h))
        mask[labels == i] = 255
    
    return boxes, mask


def detect_stain_mold_improved(hsv: np.ndarray) -> Tuple[List[Tuple[int, int, int, int]], np.ndarray]:
    """æ”¹è¿›çš„æ±¡æ¸/éœ‰æ–‘æ£€æµ‹"""
    dark_mask = cv2.inRange(hsv, (0, 40, 0), (180, 255, 90))
    green_mask = cv2.inRange(hsv, (35, 50, 30), (85, 255, 120))
    combined = cv2.bitwise_or(dark_mask, green_mask)
    
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    combined = cv2.morphologyEx(combined, cv2.MORPH_OPEN, kernel, iterations=1)
    combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(combined, connectivity=8)
    boxes = []
    mask = np.zeros_like(combined)
    
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < 300:
            continue
        x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], \
                     stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        boxes.append((x, y, w, h))
        mask[labels == i] = 255
    
    return boxes, mask


def detect_salt_weathering_improved(hsv: np.ndarray) -> Tuple[List[Tuple[int, int, int, int]], np.ndarray]:
    """æ”¹è¿›çš„ç›èš€/é£åŒ–æ£€æµ‹"""
    salt_mask = cv2.inRange(hsv, (0, 0, 200), (180, 35, 255))
    
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    salt_mask = cv2.morphologyEx(salt_mask, cv2.MORPH_OPEN, kernel, iterations=1)
    salt_mask = cv2.morphologyEx(salt_mask, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(salt_mask, connectivity=8)
    boxes = []
    mask = np.zeros_like(salt_mask)
    
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < 400:
            continue
        x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], \
                     stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        boxes.append((x, y, w, h))
        mask[labels == i] = 255
    
    return boxes, mask


def detect_bio_growth_improved(hsv: np.ndarray) -> Tuple[List[Tuple[int, int, int, int]], np.ndarray]:
    """æ”¹è¿›çš„ç”Ÿç‰©é™„ç€æ£€æµ‹"""
    bio_mask = cv2.inRange(hsv, (35, 60, 40), (85, 255, 255))
    
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    bio_mask = cv2.morphologyEx(bio_mask, cv2.MORPH_OPEN, kernel, iterations=1)
    bio_mask = cv2.morphologyEx(bio_mask, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(bio_mask, connectivity=8)
    boxes = []
    mask = np.zeros_like(bio_mask)
    
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < 300:
            continue
        x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], \
                     stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        boxes.append((x, y, w, h))
        mask[labels == i] = 255
    
    return boxes, mask

# ---------------------------
# UIæ”¹è¿›åŠŸèƒ½ï¼ˆæ•´åˆè‡ªimproved_ui.pyï¼‰- éœ€è¦åœ¨è°ƒç”¨å‰å®šä¹‰
# ---------------------------
def inject_custom_css():
    """æ³¨å…¥æ–‡ç‰©å›¾æ¡ˆèƒŒæ™¯æ ·å¼"""
    st.markdown("""
    <style>
    /* ä¸»èƒŒæ™¯ - æ•¦ç…Œå£ç”»é£æ ¼ */
    .stApp {
        background: 
            /* ä¸»è‰²è°ƒ - åœŸé»„è‰²åŸºåº•ï¼Œæ¨¡æ‹Ÿå£ç”»åº•è‰² */
            linear-gradient(135deg, #f4e4bc 0%, #e8d5b5 100%),
            /* çº¹ç†å åŠ  - æ¨¡æ‹Ÿå£ç”»çº¸å¼ çº¹ç† */
            url("data:image/svg+xml,%3Csvg width='100' height='100' viewBox='0 0 100 100' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M11 18c3.866 0 7-3.134 7-7s-3.134-7-7-7-7 3.134-7 7 3.134 7 7 7zm48 25c3.866 0 7-3.134 7-7s-3.134-7-7-7-7 3.134-7 7 3.134 7 7 7zm-43-7c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zm63 31c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zM34 90c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zm56-76c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zM12 86c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm28-65c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm23-11c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zm-6 60c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm29 22c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zM32 63c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zm57-13c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zm-9-21c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2zM60 91c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2zM35 41c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2zM12 60c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2z' fill='%23d4c4a8' fill-opacity='0.2' fill-rule='evenodd'/%3E%3C/svg%3E"),
            /* è¾¹æ¡†è£…é¥° - æ¨¡æ‹Ÿå·è½´è¾¹ç¼˜ */
            linear-gradient(90deg, transparent 95%, #8b7355 95%),
            linear-gradient(90deg, transparent 5%, #8b7355 5%),
            linear-gradient(0deg, transparent 95%, #8b7355 95%),
            linear-gradient(0deg, transparent 5%, #8b7355 5%);
        background-size: cover, 200px 200px, 100% 100%, 100% 100%, 100% 100%, 100% 100%;
        background-attachment: fixed;
        position: relative;
    }

    /* å·è½´è£…é¥°æ•ˆæœ - é™ä½z-indexï¼Œç¡®ä¿ä¸é®æŒ¡å†…å®¹ */
    .stApp::before {
        content: "";
        position: fixed;
        top: 50px;
        left: 50px;
        right: 50px;
        bottom: 50px;
        border: 2px solid #8b7355;
        border-radius: 8px;
        pointer-events: none;
        z-index: -1;
        box-shadow: 
            inset 0 0 50px rgba(139, 115, 85, 0.1),
            0 0 30px rgba(0, 0, 0, 0.1);
    }

    /* ä¼ ç»Ÿçº¹æ ·è£…é¥° - é™ä½z-indexï¼Œç¡®ä¿ä¸é®æŒ¡å†…å®¹ */
    .stApp::after {
        content: "";
        position: fixed;
        top: 40px;
        left: 40px;
        right: 40px;
        bottom: 40px;
        background-image: 
            radial-gradient(circle at 20% 20%, rgba(139, 115, 85, 0.05) 0%, transparent 50%),
            radial-gradient(circle at 80% 80%, rgba(139, 115, 85, 0.05) 0%, transparent 50%);
        pointer-events: none;
        z-index: -1;
    }
    
    /* ç¡®ä¿Streamlitä¸»å†…å®¹åŒºåŸŸåœ¨è£…é¥°å±‚ä¹‹ä¸Š */
    .main .block-container {
        position: relative;
        z-index: 1;
        display: flex;
        flex-direction: column;
        min-height: calc(100vh - 10rem);
    }
    
    /* ç¡®ä¿æ‰€æœ‰Streamlitå…ƒç´ å¯è§ */
    .stApp > div {
        position: relative;
        z-index: 1;
    }

    /* ä¸»å†…å®¹å®¹å™¨ */
    .main-container {
        background: rgba(255, 253, 245, 0.92);
        backdrop-filter: blur(15px);
        border-radius: 12px;
        padding: 2.5rem;
        margin: 1rem;
        box-shadow: 
            0 8px 40px rgba(0, 0, 0, 0.15),
            inset 0 1px 0 rgba(255, 255, 255, 0.8);
        border: 1px solid rgba(139, 115, 85, 0.3);
        position: relative;
        z-index: 10;
        border-left: 8px solid #8b7355;
        border-right: 8px solid #8b7355;
    }

    /* ä¸»å®¹å™¨æ ·å¼ */
    .main-header {
        background: rgba(255, 253, 245, 0.95);
        backdrop-filter: blur(20px);
        padding: 2rem;
        border-radius: 15px;
        color: #5d4037;
        margin-bottom: 2rem;
        box-shadow: 
            0 8px 40px rgba(0, 0, 0, 0.15),
            inset 0 1px 0 rgba(255, 255, 255, 0.8);
        border: 1px solid rgba(139, 115, 85, 0.3);
        border-left: 8px solid #8b7355;
        border-right: 8px solid #8b7355;
        position: relative;
        z-index: 10;
    }
    
    .main-header h1 {
        font-size: 3.2rem;
        font-weight: bold;
        margin-bottom: 0.5rem;
        text-align: center;
        font-family: 'SimSun', serif;
        color: #5d4037;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    
    .main-header .subtitle {
        font-size: 1.3rem;
        text-align: center;
        color: #8b7355;
        font-weight: 500;
        font-family: 'SimSun', serif;
    }
    
    /* ä¾§è¾¹æ æ ·å¼ */
    .css-1d391kg {
        background: rgba(255, 253, 245, 0.95) !important;
        backdrop-filter: blur(20px) !important;
        border-right: 3px solid #8b7355 !important;
        box-shadow: 5px 0 25px rgba(0, 0, 0, 0.1) !important;
        position: relative !important;
        z-index: 100 !important;
    }
    
    /* ç¡®ä¿ä¾§è¾¹æ å†…å®¹å¯è§ */
    [data-testid="stSidebar"] {
        position: relative !important;
        z-index: 100 !important;
    }
    
    .sidebar-header {
        background: rgba(139, 115, 85, 0.1);
        color: #5d4037;
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
        text-align: center;
        border: 1px solid rgba(139, 115, 85, 0.3);
        font-family: 'SimSun', serif;
    }
    
    /* å¡ç‰‡æ ·å¼ - æ¨¡æ‹Ÿå¤ç±ä¹¦é¡µ */
    .card {
        background: linear-gradient(135deg, #fffdf5 0%, #f9f5e9 100%);
        border-radius: 8px;
        padding: 1rem;
        margin: 0.8rem 0;
        box-shadow: 
            0 4px 20px rgba(0, 0, 0, 0.08),
            inset 0 1px 0 rgba(255, 255, 255, 0.6);
        border: 1px solid rgba(139, 115, 85, 0.2);
        border-left: 4px solid #8b7355;
        transition: all 0.3s ease;
        position: relative;
        overflow: hidden;
    }

    .card::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 1px;
        background: linear-gradient(90deg, transparent, #8b7355, transparent);
    }

    .card:hover {
        transform: translateY(-3px);
        box-shadow: 
            0 8px 30px rgba(0, 0, 0, 0.12),
            inset 0 1px 0 rgba(255, 255, 255, 0.8);
    }
    
    .card-header {
        font-size: 1rem;
        font-weight: 600;
        color: #5d4037;
        margin-bottom: 0.8rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
        font-family: 'SimSun', serif;
        border-bottom: 1px solid #d7ccc8;
        padding-bottom: 0.3rem;
    }

    /* ä¸Šä¼ åŒºåŸŸæ ·å¼ */
    .upload-section {
        background: rgba(255, 253, 245, 0.8);
        border: 2px dashed #8b7355;
        border-radius: 10px;
        padding: 2.2rem;
        text-align: center;
        transition: all 0.3s ease;
        backdrop-filter: blur(10px);
        position: relative;
    }

    .upload-section::before {
        content: "ğŸ“œ";
        font-size: 3rem;
        position: absolute;
        top: -20px;
        left: 50%;
        transform: translateX(-50%);
        background: #fffdf5;
        padding: 0 1rem;
    }

    .upload-section:hover {
        border-color: #6d4c41;
        background: rgba(255, 253, 245, 0.95);
        transform: translateY(-2px);
    }
    
    /* æŒ‰é’®æ ·å¼ - ä¼ ç»Ÿå°ç« é£æ ¼ */
    .stButton > button {
        background: linear-gradient(135deg, #8b7355 0%, #6d4c41 100%);
        color: #fffdf5 !important;
        border: none;
        border-radius: 6px;
        padding: 0.9rem 2.2rem;
        font-weight: 600;
        font-size: 1.1rem;
        transition: all 0.3s ease;
        width: 100%;
        font-family: 'SimSun', serif;
        letter-spacing: 1px;
        box-shadow: 0 4px 15px rgba(139, 115, 85, 0.3);
        position: relative;
        overflow: hidden;
    }

    .stButton button::before {
        content: "";
        position: absolute;
        top: 0;
        left: -100%;
        width: 100%;
        height: 100%;
        background: linear-gradient(90deg, transparent, rgba(255,255,255,0.2), transparent);
        transition: left 0.5s;
    }

    .stButton button:hover::before {
        left: 100%;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(139, 115, 85, 0.4);
    }
    
    /* æ ‡ç­¾é¡µæ ·å¼ */
    .stTabs [data-baseweb="tab-list"] {
        gap: 5px;
        background: rgba(255, 253, 245, 0.9);
        backdrop-filter: blur(10px);
        border-radius: 8px;
        padding: 6px;
        border: 1px solid rgba(139, 115, 85, 0.3);
        box-shadow: inset 0 1px 3px rgba(0,0,0,0.1);
    }
    
    .stTabs [data-baseweb="tab"] {
        border-radius: 6px;
        padding: 12px 20px;
        font-weight: 500;
        transition: all 0.3s ease;
        background: transparent;
        font-family: 'SimSun', serif;
        border: 1px solid transparent;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #8b7355 0%, #6d4c41 100%);
        color: #fffdf5 !important;
        box-shadow: 0 2px 8px rgba(139, 115, 85, 0.3);
        border: 1px solid #5d4037;
    }
    
    /* æŒ‡æ ‡å¡ç‰‡ */
    .metric-card {
        background: linear-gradient(135deg, #fffdf5 0%, #f9f5e9 100%);
        backdrop-filter: blur(10px);
        padding: 0.8rem;
        border-radius: 8px;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.08);
        border: 1px solid rgba(139, 115, 85, 0.2);
        border-top: 3px solid #8b7355;
        margin: 0.5rem 0;
        text-align: center;
        font-family: 'SimSun', serif;
    }
    
    .metric-card h4 {
        font-size: 1rem;
        margin: 0.3rem 0;
    }
    
    .metric-card p {
        font-size: 0.85rem;
        margin: 0.3rem 0;
    }
    
    /* é¡µè„šæ ·å¼ */
    .footer {
        text-align: center;
        padding: 2.5rem;
        margin-top: auto;
        margin-bottom: 0;
        color: #5d4037;
        background: rgba(255, 253, 245, 0.9);
        backdrop-filter: blur(15px);
        border-radius: 12px;
        border: 1px solid rgba(139, 115, 85, 0.3);
        border-top: 3px solid #8b7355;
        font-family: 'SimSun', serif;
        position: relative;
        width: 100%;
    }
    
    /* é¡µè„šå®¹å™¨æ ·å¼ */
    .footer-container {
        margin-top: auto;
        padding-top: 2rem;
    }

    /* æ ‡é¢˜æ ·å¼ */
    .cultural-title {
        font-family: 'SimSun', serif;
        color: #5d4037;
        text-align: center;
        margin-bottom: 0.5rem;
        font-weight: bold;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }

    .cultural-subtitle {
        font-family: 'SimSun', serif;
        color: #8b7355;
        text-align: center;
        margin-bottom: 2rem;
        font-size: 1.1rem;
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    .stTextInput>div>div>input, .stSelectbox>div>div {
        background: rgba(255, 253, 245, 0.9) !important;
        border: 1px solid #8b7355 !important;
        border-radius: 4px !important;
        font-family: 'SimSun', serif !important;
    }

    /* æ»‘å—æ ·å¼ */
    .stSlider>div>div>div {
        background: #8b7355 !important;
    }

    /* å¤é€‰æ¡†æ ·å¼ */
    .stCheckbox>label {
        font-family: 'SimSun', serif;
        color: #5d4037;
    }
    
    /* è¿›åº¦æ¡æ ·å¼ */
    .stProgress > div > div > div {
        background: linear-gradient(135deg, #8b7355 0%, #6d4c41 100%);
    }
    </style>
    """, unsafe_allow_html=True)


def create_main_header():
    """åˆ›å»ºä¼ ç»Ÿæ–‡åŒ–é£æ ¼çš„ä¸»æ ‡é¢˜"""
    st.markdown("""
    <div class="main-header">
        <h1 class="cultural-title" style="font-size: 3.2rem; margin-bottom: 0.5rem;">
            ğŸ›ï¸ çŸ³çªŸå¯ºå£ç”»ç—…å®³AIè¯†åˆ«å·¥å…·
        </h1>
        <p class="cultural-subtitle" style="font-size: 1.3rem;">
            ä¸Šæµ·äº¤é€šå¤§å­¦è®¾è®¡å­¦é™¢ Â· æ–‡ç‰©ä¿®å¤ç ”ç©¶å›¢é˜Ÿ
        </p>
        <div style="display: flex; justify-content: center; gap: 1.5rem; flex-wrap: wrap; margin-top: 1.5rem;">
            <span style="background: rgba(139, 115, 85, 0.1); padding: 0.6rem 1.2rem; border-radius: 25px; color: #8b7355; border: 1px solid #8b7355;">
                ğŸ¨ å¤šæ¨¡æ€èåˆ
            </span>
            <span style="background: rgba(139, 115, 85, 0.1); padding: 0.6rem 1.2rem; border-radius: 25px; color: #8b7355; border: 1px solid #8b7355;">
                ğŸ” æ™ºèƒ½è¯Šæ–­
            </span>
            <span style="background: rgba(139, 115, 85, 0.1); padding: 0.6rem 1.2rem; border-radius: 25px; color: #8b7355; border: 1px solid #8b7355;">
                ğŸ–Œï¸ è™šæ‹Ÿä¿®å¤
            </span>
            <span style="background: rgba(139, 115, 85, 0.1); padding: 0.6rem 1.2rem; border-radius: 25px; color: #8b7355; border: 1px solid #8b7355;">
                ğŸ“š çŸ¥è¯†é©±åŠ¨
            </span>
        </div>
    </div>
    """, unsafe_allow_html=True)


def create_feature_highlights():
    """åˆ›å»ºä¼ ç»Ÿæ–‡åŒ–é£æ ¼çš„åŠŸèƒ½å±•ç¤º"""
    st.markdown("""
    <div class="main-container" style="margin: 1rem 0;">
        <div class="card" style="padding: 0.8rem; margin: 0.5rem 0;">
            <div class="card-header" style="font-size: 0.95rem; margin-bottom: 0.6rem;">ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½</div>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 0.8rem;">
                <div class="metric-card">
                    <div style="font-size: 1.5rem; margin-bottom: 0.3rem;">ğŸ¯</div>
                    <h4 style="font-size: 0.95rem; margin: 0.2rem 0;">ç²¾å‡†è¯†åˆ«</h4>
                    <p style="font-size: 0.8rem; margin: 0.2rem 0;">6å¤§ç±»ç—…å®³æ™ºèƒ½æ£€æµ‹ï¼Œå‡†ç¡®ç‡è¶…95%</p>
                </div>
                <div class="metric-card">
                    <div style="font-size: 1.5rem; margin-bottom: 0.3rem;">ğŸ”¬</div>
                    <h4 style="font-size: 0.95rem; margin: 0.2rem 0;">å¤šæ¨¡æ€åˆ†æ</h4>
                    <p style="font-size: 0.8rem; margin: 0.2rem 0;">å›¾åƒ+3D+æ–‡æœ¬èåˆåˆ†ææŠ€æœ¯</p>
                </div>
                <div class="metric-card">
                    <div style="font-size: 1.5rem; margin-bottom: 0.3rem;">ğŸ¨</div>
                    <h4 style="font-size: 0.95rem; margin: 0.2rem 0;">è™šæ‹Ÿä¿®å¤</h4>
                    <p style="font-size: 0.8rem; margin: 0.2rem 0;">AIé©±åŠ¨çš„æ™ºèƒ½å¤åŸæ¨¡æ‹Ÿç³»ç»Ÿ</p>
                </div>
                <div class="metric-card">
                    <div style="font-size: 1.5rem; margin-bottom: 0.3rem;">ğŸ“Š</div>
                    <h4 style="font-size: 0.95rem; margin: 0.2rem 0;">ä¸“ä¸šæŠ¥å‘Š</h4>
                    <p style="font-size: 0.8rem; margin: 0.2rem 0;">å®Œæ•´çš„åˆ†ææŠ¥å‘Šå’Œä¿®å¤å»ºè®®</p>
                </div>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)


def create_footer():
    """åˆ›å»ºä¼ ç»Ÿæ–‡åŒ–é£æ ¼çš„é¡µè„š"""
    current_year = datetime.now().year
    st.markdown(f"""
    <div class="footer-container">
        <div class="footer">
            <h4 class="cultural-title">ğŸ›ï¸ çŸ³çªŸå¯ºå£ç”»æ™ºèƒ½ä¿æŠ¤å¹³å°</h4>
            <p>ä¸Šæµ·äº¤é€šå¤§å­¦è®¾è®¡å­¦é™¢ Â· æ–‡ç‰©ä¿®å¤ç ”ç©¶å›¢é˜Ÿ Â· AI+æ–‡ç‰©ä¿æŠ¤å®éªŒå®¤</p>
            <p style="font-size: 0.9rem; margin-top: 1rem; color: #8b7355;">
                ğŸ¨ ä¼ æ‰¿æ–‡æ˜ Â· ğŸ” ç§‘æŠ€æŠ¤å® Â· ğŸ–Œï¸ æ™ºèƒ½ä¿®å¤
            </p>
            <div style="margin-top: 1.5rem; font-size: 0.8rem; color: #a1887f;">
                Â© {current_year} ä¸Šæµ·äº¤é€šå¤§å­¦è®¾è®¡å­¦é™¢æ–‡ç‰©ä¿æŠ¤å›¢é˜Ÿ
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)

# æ³¨å…¥æ”¹è¿›çš„UIæ ·å¼ï¼ˆåœ¨å‡½æ•°å®šä¹‰ä¹‹åï¼‰
if IMPROVED_UI_AVAILABLE:
    inject_custom_css()
    create_main_header()
    create_feature_highlights()
else:
    # ä¿ç•™åŸæœ‰çš„æ¬¢è¿æ¨ªå¹…
    st.markdown("""
    <div style="text-align:center;margin-bottom:2rem;">
        <p style="color:#7f8c8d;font-size:1.1rem;margin:0;">
            å¤šæ¨¡æ€èåˆ Â· æ™ºèƒ½è¯Šæ–­ Â· è™šæ‹Ÿä¿®å¤ Â· çŸ¥è¯†é©±åŠ¨
        </p>
    </div>
    """, unsafe_allow_html=True)

class KnowledgeBase:
    """åŸºç¡€çŸ¥è¯†åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = "persistent_data/knowledge_base.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çŸ¥è¯†æ¡ç›®è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_items (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                category TEXT NOT NULL,
                content TEXT NOT NULL,
                tags TEXT,
                material_type TEXT,
                disease_type TEXT,
                severity_level TEXT,
                treatment_method TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                author TEXT,
                source TEXT,
                view_count INTEGER DEFAULT 0,
                rating REAL DEFAULT 0.0
            )
        """)
        
        # çŸ¥è¯†å…³è”è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_relations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id INTEGER,
                target_id INTEGER,
                relation_type TEXT,
                weight REAL DEFAULT 1.0,
                FOREIGN KEY (source_id) REFERENCES knowledge_items(id),
                FOREIGN KEY (target_id) REFERENCES knowledge_items(id)
            )
        """)
        
        # çŸ¥è¯†é™„ä»¶è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_attachments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                knowledge_id INTEGER,
                file_path TEXT,
                file_type TEXT,
                file_size INTEGER,
                description TEXT,
                FOREIGN KEY (knowledge_id) REFERENCES knowledge_items(id)
            )
        """)
        
        conn.commit()
        conn.close()
    
    def add_knowledge(self, title: str, category: str, content: str,
                     tags: List[str] = None, material_type: str = None,
                     disease_type: str = None, severity_level: str = None,
                     treatment_method: str = None, author: str = None,
                     source: str = None) -> int:
        """æ·»åŠ çŸ¥è¯†æ¡ç›®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        tags_str = json.dumps(tags) if tags else None
        
        cursor.execute("""
            INSERT INTO knowledge_items 
            (title, category, content, tags, material_type, disease_type,
             severity_level, treatment_method, author, source)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (title, category, content, tags_str, material_type, disease_type,
              severity_level, treatment_method, author, source))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        return knowledge_id
    
    def search_knowledge(self, keyword: str = None, category: str = None,
                        material_type: str = None, disease_type: str = None,
                        limit: int = 50) -> List[Dict]:
        """æœç´¢çŸ¥è¯†æ¡ç›®"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        query = "SELECT * FROM knowledge_items WHERE 1=1"
        params = []
        
        if keyword:
            query += " AND (title LIKE ? OR content LIKE ? OR tags LIKE ?)"
            keyword_pattern = f"%{keyword}%"
            params.extend([keyword_pattern, keyword_pattern, keyword_pattern])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if material_type:
            query += " AND material_type = ?"
            params.append(material_type)
        
        if disease_type:
            query += " AND disease_type = ?"
            params.append(disease_type)
        
        query += " ORDER BY view_count DESC, rating DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(query, params)
        results = [dict(row) for row in cursor.fetchall()]
        
        # è§£ætags
        for result in results:
            if result['tags']:
                result['tags'] = json.loads(result['tags'])
            else:
                result['tags'] = []
        
        conn.close()
        return results
    
    def get_knowledge(self, knowledge_id: int) -> Optional[Dict]:
        """è·å–çŸ¥è¯†æ¡ç›®è¯¦æƒ…"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM knowledge_items WHERE id = ?", (knowledge_id,))
        row = cursor.fetchone()
        
        if row:
            result = dict(row)
            if result['tags']:
                result['tags'] = json.loads(result['tags'])
            else:
                result['tags'] = []
            
            # å¢åŠ æµè§ˆæ¬¡æ•°
            cursor.execute("UPDATE knowledge_items SET view_count = view_count + 1 WHERE id = ?", (knowledge_id,))
            conn.commit()
        else:
            result = None
        
        conn.close()
        return result
    
    def update_knowledge(self, knowledge_id: int, **kwargs):
        """æ›´æ–°çŸ¥è¯†æ¡ç›®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        allowed_fields = ['title', 'category', 'content', 'tags', 'material_type',
                         'disease_type', 'severity_level', 'treatment_method', 'author', 'source']
        
        updates = []
        params = []
        for key, value in kwargs.items():
            if key in allowed_fields:
                if key == 'tags' and isinstance(value, list):
                    value = json.dumps(value)
                updates.append(f"{key} = ?")
                params.append(value)
        
        if updates:
            params.append(knowledge_id)
            cursor.execute(f"""
                UPDATE knowledge_items 
                SET {', '.join(updates)}, updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, params)
            conn.commit()
        
        conn.close()
    
    def delete_knowledge(self, knowledge_id: int):
        """åˆ é™¤çŸ¥è¯†æ¡ç›®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM knowledge_items WHERE id = ?", (knowledge_id,))
        conn.commit()
        conn.close()


class CaseLibrary:
    """æ¡ˆä¾‹åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = "persistent_data/case_library.db"):
        self.db_path = db_path
        self.case_images_dir = Path("persistent_data/case_images")
        self.case_images_dir.mkdir(parents=True, exist_ok=True)
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ¡ˆä¾‹è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS cases (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                location TEXT,
                material_type TEXT,
                era TEXT,
                disease_types TEXT,
                severity_level TEXT,
                description TEXT,
                diagnosis_result TEXT,
                treatment_plan TEXT,
                treatment_result TEXT,
                before_images TEXT,
                after_images TEXT,
                process_images TEXT,
                detection_data TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                author TEXT,
                status TEXT DEFAULT 'active',
                view_count INTEGER DEFAULT 0,
                rating REAL DEFAULT 0.0
            )
        """)
        
        # æ¡ˆä¾‹æ ‡ç­¾è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS case_tags (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                case_id INTEGER,
                tag TEXT,
                FOREIGN KEY (case_id) REFERENCES cases(id)
            )
        """)
        
        # æ¡ˆä¾‹å…³è”è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS case_relations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_case_id INTEGER,
                target_case_id INTEGER,
                relation_type TEXT,
                FOREIGN KEY (source_case_id) REFERENCES cases(id),
                FOREIGN KEY (target_case_id) REFERENCES cases(id)
            )
        """)
        
        conn.commit()
        conn.close()
    
    def add_case(self, title: str, location: str = None, material_type: str = None,
                era: str = None, disease_types: List[str] = None,
                severity_level: str = None, description: str = None,
                diagnosis_result: str = None, treatment_plan: str = None,
                treatment_result: str = None, before_images: List[bytes] = None,
                after_images: List[bytes] = None, process_images: List[bytes] = None,
                before_images_base64: List[str] = None,
                after_images_base64: List[str] = None,
                process_images_base64: List[str] = None,
                detection_data: Dict = None, author: str = None,
                tags: List[str] = None) -> int:
        """æ·»åŠ æ¡ˆä¾‹"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ä¿å­˜å›¾ç‰‡ï¼ˆæ”¯æŒbyteså’ŒBase64ä¸¤ç§æ ¼å¼ï¼‰
        before_paths = []
        after_paths = []
        process_paths = []
        
        # å¤„ç†bytesæ ¼å¼çš„å›¾ç‰‡
        if before_images:
            before_paths = self._save_images(before_images, "before")
        elif before_images_base64:
            before_paths = self._save_base64_images(before_images_base64, "before")
        
        if after_images:
            after_paths = self._save_images(after_images, "after")
        elif after_images_base64:
            after_paths = self._save_base64_images(after_images_base64, "after")
        
        if process_images:
            process_paths = self._save_images(process_images, "process")
        elif process_images_base64:
            process_paths = self._save_base64_images(process_images_base64, "process")
        
        disease_types_str = json.dumps(disease_types) if disease_types else None
        detection_data_str = json.dumps(detection_data) if detection_data else None
        before_paths_str = json.dumps(before_paths) if before_paths else None
        after_paths_str = json.dumps(after_paths) if after_paths else None
        process_paths_str = json.dumps(process_paths) if process_paths else None
        
        cursor.execute("""
            INSERT INTO cases 
            (title, location, material_type, era, disease_types, severity_level,
             description, diagnosis_result, treatment_plan, treatment_result,
             before_images, after_images, process_images, detection_data, author)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (title, location, material_type, era, disease_types_str, severity_level,
              description, diagnosis_result, treatment_plan, treatment_result,
              before_paths_str, after_paths_str, process_paths_str, detection_data_str, author))
        
        case_id = cursor.lastrowid
        
        # æ·»åŠ æ ‡ç­¾
        if tags:
            for tag in tags:
                cursor.execute("INSERT INTO case_tags (case_id, tag) VALUES (?, ?)", (case_id, tag))
        
        conn.commit()
        conn.close()
        return case_id
    
    def _save_images(self, images: List[bytes], prefix: str) -> List[str]:
        """ä¿å­˜å›¾ç‰‡åˆ°æ–‡ä»¶ç³»ç»Ÿï¼ˆbytesæ ¼å¼ï¼‰"""
        paths = []
        for i, img_data in enumerate(images):
            try:
                img = Image.open(BytesIO(img_data))
                filename = f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}.jpg"
                filepath = self.case_images_dir / filename
                img.save(filepath, "JPEG", quality=85)
                paths.append(str(filepath))
            except Exception as e:
                print(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
        return paths
    
    def _save_base64_images(self, images_base64: List[str], prefix: str) -> List[str]:
        """ä¿å­˜Base64ç¼–ç çš„å›¾ç‰‡åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        paths = []
        for i, base64_data in enumerate(images_base64):
            try:
                # ç§»é™¤data URIå‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if ',' in base64_data:
                    base64_data = base64_data.split(',')[1]
                
                # è§£ç Base64
                image_data = base64.b64decode(base64_data)
                img = Image.open(BytesIO(image_data))
                
                filename = f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}.jpg"
                filepath = self.case_images_dir / filename
                img.save(filepath, "JPEG", quality=85)
                paths.append(str(filepath))
            except Exception as e:
                print(f"ä¿å­˜Base64å›¾ç‰‡å¤±è´¥: {e}")
        return paths
    
    def get_case_image_base64(self, image_path: str) -> Optional[str]:
        """ä»æ–‡ä»¶è·¯å¾„è·å–Base64ç¼–ç çš„å›¾ç‰‡"""
        try:
            if not os.path.exists(image_path):
                return None
            
            with open(image_path, 'rb') as f:
                image_data = f.read()
                base64_str = base64.b64encode(image_data).decode('utf-8')
                
                # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šMIMEç±»å‹
                ext = Path(image_path).suffix.lower()
                mime_type = 'image/jpeg' if ext in ['.jpg', '.jpeg'] else 'image/png'
                
                return f"data:{mime_type};base64,{base64_str}"
        except Exception as e:
            print(f"è·å–Base64å›¾ç‰‡å¤±è´¥: {e}")
            return None
    
    def search_cases(self, keyword: str = None, material_type: str = None,
                   disease_type: str = None, location: str = None,
                   severity_level: str = None, limit: int = 50) -> List[Dict]:
        """æœç´¢æ¡ˆä¾‹"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        query = "SELECT * FROM cases WHERE status = 'active'"
        params = []
        
        if keyword:
            query += " AND (title LIKE ? OR description LIKE ? OR diagnosis_result LIKE ?)"
            keyword_pattern = f"%{keyword}%"
            params.extend([keyword_pattern, keyword_pattern, keyword_pattern])
        
        if material_type:
            query += " AND material_type = ?"
            params.append(material_type)
        
        if disease_type:
            query += " AND disease_types LIKE ?"
            params.append(f"%{disease_type}%")
        
        if location:
            query += " AND location LIKE ?"
            params.append(f"%{location}%")
        
        if severity_level:
            query += " AND severity_level = ?"
            params.append(severity_level)
        
        query += " ORDER BY view_count DESC, rating DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(query, params)
        results = [dict(row) for row in cursor.fetchall()]
        
        # è§£æJSONå­—æ®µ
        for result in results:
            if result['disease_types']:
                result['disease_types'] = json.loads(result['disease_types'])
            else:
                result['disease_types'] = []
            
            if result['before_images']:
                result['before_images'] = json.loads(result['before_images'])
            else:
                result['before_images'] = []
            
            if result['after_images']:
                result['after_images'] = json.loads(result['after_images'])
            else:
                result['after_images'] = []
            
            if result['detection_data']:
                result['detection_data'] = json.loads(result['detection_data'])
            else:
                result['detection_data'] = {}
        
        conn.close()
        return results
    
    def get_case(self, case_id: int) -> Optional[Dict]:
        """è·å–æ¡ˆä¾‹è¯¦æƒ…"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM cases WHERE id = ?", (case_id,))
        row = cursor.fetchone()
        
        if row:
            result = dict(row)
            
            # è§£æJSONå­—æ®µ
            if result['disease_types']:
                result['disease_types'] = json.loads(result['disease_types'])
            else:
                result['disease_types'] = []
            
            if result['before_images']:
                result['before_images'] = json.loads(result['before_images'])
            else:
                result['before_images'] = []
            
            if result['after_images']:
                result['after_images'] = json.loads(result['after_images'])
            else:
                result['after_images'] = []
            
            if result['detection_data']:
                result['detection_data'] = json.loads(result['detection_data'])
            else:
                result['detection_data'] = {}
            
            # è·å–æ ‡ç­¾
            cursor.execute("SELECT tag FROM case_tags WHERE case_id = ?", (case_id,))
            result['tags'] = [row[0] for row in cursor.fetchall()]
            
            # å¢åŠ æµè§ˆæ¬¡æ•°
            cursor.execute("UPDATE cases SET view_count = view_count + 1 WHERE id = ?", (case_id,))
            conn.commit()
        else:
            result = None
        
        conn.close()
        return result
    
    def get_similar_cases(self, case_id: int, limit: int = 5) -> List[Dict]:
        """è·å–ç›¸ä¼¼æ¡ˆä¾‹"""
        case = self.get_case(case_id)
        if not case:
            return []
        
        # åŸºäºææ–™ç±»å‹å’Œç—…å®³ç±»å‹æŸ¥æ‰¾ç›¸ä¼¼æ¡ˆä¾‹
        return self.search_cases(
            material_type=case.get('material_type'),
            disease_type=case.get('disease_types')[0] if case.get('disease_types') else None,
            limit=limit + 1
        )[1:]  # æ’é™¤è‡ªå·±

# ---------------------------
# é«˜çº§å›¾åƒå¤åŸç³»ç»Ÿï¼ˆæ•´åˆè‡ªadvanced_restoration.pyï¼‰
# ---------------------------
class AdvancedMuralRestoration:
    """å…ˆè¿›çš„å£ç”»å¤åŸç³»ç»Ÿ"""
    
    def __init__(self):
        self.restoration_methods = {
            "inpainting": {
                "telea": cv2.INPAINT_TELEA,
                "ns": cv2.INPAINT_NS
            }
        }
    
    def advanced_inpainting(self, image, mask, method='telea', radius=3, iterations=1):
        """é«˜çº§å›¾åƒä¿®å¤"""
        if method == 'telea':
            flags = cv2.INPAINT_TELEA
        else:
            flags = cv2.INPAINT_NS
        
        result = image.copy()
        for i in range(iterations):
            result = cv2.inpaint(result, mask, radius, flags)
        
        return result
    
    def deep_learning_inpainting(self, image, mask):
        """æ·±åº¦å­¦ä¹ ä¿®å¤ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        result = image.copy()
        scales = [0.5, 0.75, 1.0]
        for scale in scales:
            if scale != 1.0:
                h, w = image.shape[:2]
                new_size = (int(w*scale), int(h*scale))
                img_scaled = cv2.resize(image, new_size)
                mask_scaled = cv2.resize(mask, new_size)
                inpainted_scaled = cv2.inpaint(img_scaled, mask_scaled, 3, cv2.INPAINT_NS)
                inpainted = cv2.resize(inpainted_scaled, (w, h))
                alpha = 0.3
                result = cv2.addWeighted(result, 1-alpha, inpainted, alpha, 0)
        return result
    
    def texture_aware_inpainting(self, image, mask, texture_weight=0.7):
        """çº¹ç†æ„ŸçŸ¥ä¿®å¤"""
        result = image.copy()
        methods = ['telea', 'ns']
        results = []
        
        for method in methods:
            if method == 'telea':
                inpainted = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)
            else:
                inpainted = cv2.inpaint(image, mask, 3, cv2.INPAINT_NS)
            results.append(inpainted)
        
        if len(results) == 2:
            blended = cv2.addWeighted(results[0], texture_weight, 
                                    results[1], 1-texture_weight, 0)
            result = blended
        
        return result
    
    def color_restoration_advanced(self, image, method='comprehensive', 
                                  contrast_enhance=1.5, saturation_boost=1.2, 
                                  sharpening_strength=0.5):
        """é«˜çº§è‰²å½©å¤åŸ"""
        if method == 'comprehensive':
            result = image.copy()
            result = self.white_balance(result)
            result = self.adaptive_contrast_enhancement(result, clip_limit=contrast_enhance)
            result = self.saturation_enhancement(result, factor=saturation_boost)
            if sharpening_strength > 0:
                result = self.smart_sharpening(result, strength=sharpening_strength)
            return result
        elif method == 'histogram_equalization':
            return self.histogram_equalization(image)
        elif method == 'dehazing':
            return self.dehazing(image)
    
    def white_balance(self, img):
        """æ”¹è¿›çš„ç™½å¹³è¡¡ç®—æ³•"""
        result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        avg_a = np.mean(result[:, :, 1])
        avg_b = np.mean(result[:, :, 2])
        result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
        result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
        result = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)
        return result
    
    def adaptive_contrast_enhancement(self, img, clip_limit=2.0, grid_size=8):
        """è‡ªé€‚åº”å¯¹æ¯”åº¦å¢å¼º"""
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(grid_size, grid_size))
        l_enhanced = clahe.apply(l)
        lab_enhanced = cv2.merge([l_enhanced, a, b])
        result = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        return result
    
    def saturation_enhancement(self, img, factor=1.2):
        """é¥±å’Œåº¦å¢å¼º"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        s = cv2.multiply(s, factor)
        s = np.clip(s, 0, 255)
        hsv_enhanced = cv2.merge([h, s, v])
        result = cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2BGR)
        return result
    
    def smart_sharpening(self, img, strength=0.8):
        """æ™ºèƒ½é”åŒ–"""
        kernel = np.array([[-1, -1, -1],
                          [-1, 9, -1],
                          [-1, -1, -1]]) * strength
        sharpened = cv2.filter2D(img, -1, kernel)
        return sharpened
    
    def dehazing(self, img, w=0.95, t0=0.1):
        """å›¾åƒå»é›¾ç®—æ³•"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        dark_channel = self.get_dark_channel(img, 15)
        atmospheric_light = self.get_atmospheric_light(img, dark_channel)
        
        transmission = 1 - w * dark_channel / atmospheric_light
        transmission = np.maximum(transmission, t0)
        
        result = np.zeros_like(img, dtype=np.float64)
        for i in range(3):
            result[:, :, i] = (img[:, :, i].astype(np.float64) - atmospheric_light) / transmission + atmospheric_light
        
        return np.uint8(np.clip(result, 0, 255))
    
    def get_dark_channel(self, img, window_size):
        """è®¡ç®—æš—é€šé“"""
        min_channel = np.min(img, axis=2)
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (window_size, window_size))
        dark_channel = cv2.erode(min_channel, kernel)
        return dark_channel
    
    def get_atmospheric_light(self, img, dark_channel):
        """ä¼°è®¡å¤§æ°”å…‰å€¼"""
        h, w = img.shape[:2]
        img_size = h * w
        num_pixels = int(max(img_size * 0.001, 1))
        
        dark_vec = dark_channel.reshape(img_size)
        img_vec = img.reshape(img_size, 3)
        
        indices = dark_vec.argsort()[-num_pixels:]
        atmospheric_light = np.mean(img_vec[indices], axis=0)
        
        return np.max(atmospheric_light)
    
    def histogram_equalization(self, img):
        """ç›´æ–¹å›¾å‡è¡¡åŒ–"""
        yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])
        return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
    
    def color_transfer(self, img, target_img):
        """é¢œè‰²è¿ç§»"""
        return img
    
    def texture_fill(self, image, mask):
        """çº¹ç†å¡«å……"""
        return self.patch_match_inpainting(image, mask)
    
    def patch_match_inpainting(self, image, mask, patch_size=9):
        """åŸºäºå—åŒ¹é…çš„ä¿®å¤ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        result = image.copy()
        mask_indices = np.where(mask > 0)
        
        for i in range(0, len(mask_indices[0]), patch_size):
            y, x = mask_indices[0][i], mask_indices[1][i]
            patch = self.get_best_matching_patch(image, mask, (x, y), patch_size)
            if patch is not None:
                result[y:y+patch_size, x:x+patch_size] = patch
        
        return result
    
    def get_best_matching_patch(self, image, mask, center, patch_size):
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çº¹ç†å—"""
        x, y = center
        h, w = image.shape[:2]
        search_radius = min(50, w//4, h//4)
        best_patch = None
        best_score = float('inf')
        
        for dy in range(-search_radius, search_radius, patch_size//2):
            for dx in range(-search_radius, search_radius, patch_size//2):
                y2, x2 = y + dy, x + dx
                
                if (y2 < 0 or y2 + patch_size >= h or 
                    x2 < 0 or x2 + patch_size >= w):
                    continue
                
                target_patch = image[y2:y2+patch_size, x2:x2+patch_size]
                mask_patch = mask[y2:y2+patch_size, x2:x2+patch_size]
                
                if np.any(mask_patch > 0):
                    continue
                
                score = self.calculate_patch_similarity(
                    image[y:y+patch_size, x:x+patch_size], target_patch)
                
                if score < best_score:
                    best_score = score
                    best_patch = target_patch
        
        return best_patch
    
    def calculate_patch_similarity(self, patch1, patch2):
        """è®¡ç®—å›¾åƒå—çš„ç›¸ä¼¼åº¦"""
        if patch1.shape != patch2.shape:
            return float('inf')
        
        diff = patch1.astype(np.float32) - patch2.astype(np.float32)
        color_similarity = np.mean(np.abs(diff))
        
        gray1 = cv2.cvtColor(patch1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(patch2, cv2.COLOR_BGR2GRAY)
        
        grad1 = cv2.Sobel(gray1, cv2.CV_32F, 1, 1)
        grad2 = cv2.Sobel(gray2, cv2.CV_32F, 1, 1)
        
        texture_similarity = np.mean(np.abs(grad1 - grad2))
        
        return color_similarity * 0.7 + texture_similarity * 0.3


class VirtualRestorationSystem:
    """è™šæ‹Ÿä¿®å¤ç³»ç»Ÿ"""
    
    def __init__(self):
        self.restorer = AdvancedMuralRestoration()
    
    def comprehensive_restoration(self, image_rgb, masks_dict, restoration_config):
        """ç»¼åˆä¿®å¤æµç¨‹"""
        result = image_rgb.copy()
        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        combined_mask = self.create_combined_mask(masks_dict, restoration_config['target_defects'])
        
        if restoration_config['method'] == 'comprehensive':
            result_bgr = self.adaptive_restoration(image_bgr, combined_mask, masks_dict, restoration_config)
        elif restoration_config['method'] == 'deep_learning':
            result_bgr = self.restorer.deep_learning_inpainting(image_bgr, combined_mask)
        elif restoration_config['method'] == 'texture_aware':
            result_bgr = self.restorer.texture_aware_inpainting(
                image_bgr, combined_mask, 
                texture_weight=restoration_config.get('texture_weight', 0.7))
        else:
            result_bgr = self.restorer.advanced_inpainting(
                image_bgr, combined_mask, 
                method=restoration_config['method'],
                radius=restoration_config['radius'],
                iterations=restoration_config['iterations']
            )
        
        if restoration_config.get('color_restoration', False):
            result_bgr = self.restorer.color_restoration_advanced(
                result_bgr,
                contrast_enhance=restoration_config.get('contrast_enhancement', 1.5),
                saturation_boost=restoration_config.get('saturation_boost', 1.2),
                sharpening_strength=restoration_config.get('sharpening_strength', 0.5)
            )
        
        result = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)
        return result, combined_mask
    
    def create_combined_mask(self, masks_dict, target_defects):
        """åˆ›å»ºç»¼åˆæ©è†œ"""
        if not masks_dict:
            return np.zeros((100, 100), dtype=np.uint8)
        
        first_mask = list(masks_dict.values())[0]
        combined_mask = np.zeros(first_mask.shape, dtype=np.uint8)
        
        defect_mapping = {
            'è£‚ç¼': 'crack',
            'å‰¥è½': 'peel', 
            'è¤ªè‰²': 'disc',
            'æ±¡æ¸/éœ‰æ–‘': 'stain',
            'ç›èš€/é£åŒ–': 'salt',
            'ç”Ÿç‰©é™„ç€': 'bio'
        }
        
        for defect in target_defects:
            mask_key = defect_mapping.get(defect)
            if mask_key and mask_key in masks_dict:
                mask = masks_dict[mask_key]
                if mask is not None and mask.size > 0:
                    combined_mask = cv2.bitwise_or(combined_mask, (mask > 0).astype(np.uint8) * 255)
        
        if np.any(combined_mask > 0):
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
        
        return combined_mask
    
    def adaptive_restoration(self, image, mask, masks_dict, config):
        """è‡ªé€‚åº”ä¿®å¤ç­–ç•¥"""
        result = image.copy()
        
        crack_mask = (masks_dict.get('crack', np.zeros_like(mask)) > 0).astype(np.uint8) * 255
        peel_mask = (masks_dict.get('peel', np.zeros_like(mask)) > 0).astype(np.uint8) * 255
        
        if np.any(crack_mask > 0):
            crack_result = self.restorer.advanced_inpainting(
                image, crack_mask, method='ns', radius=2, iterations=2)
            crack_region = crack_mask > 0
            result[crack_region] = crack_result[crack_region]
        
        if np.any(peel_mask > 0):
            peel_result = self.restorer.texture_aware_inpainting(
                image, peel_mask, texture_weight=config.get('texture_weight', 0.8))
            peel_region = peel_mask > 0
            result[peel_region] = peel_result[peel_region]
        
        other_mask = cv2.bitwise_and(mask, cv2.bitwise_not(cv2.bitwise_or(crack_mask, peel_mask)))
        if np.any(other_mask > 0):
            other_result = self.restorer.advanced_inpainting(
                result, other_mask, method='telea', radius=config.get('radius', 3), 
                iterations=config.get('iterations', 1))
            other_region = other_mask > 0
            result[other_region] = other_result[other_region]
        
        return result


def render_advanced_restoration_ui(img_rgb, masks_dict, default_open=True):
    """æ¸²æŸ“é«˜çº§å¤åŸç•Œé¢"""
    st.markdown("## ğŸ¨ é«˜çº§å›¾åƒå¤åŸç³»ç»Ÿ")
    
    with st.expander("å±•å¼€é«˜çº§å¤åŸé€‰é¡¹", expanded=default_open):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ä¿®å¤ç›®æ ‡é€‰æ‹©")
            target_defects = st.multiselect(
                "é€‰æ‹©éœ€è¦ä¿®å¤çš„ç—…å®³ç±»å‹",
                ["è£‚ç¼", "å‰¥è½", "è¤ªè‰²", "æ±¡æ¸/éœ‰æ–‘", "ç›èš€/é£åŒ–", "ç”Ÿç‰©é™„ç€"],
                default=["è£‚ç¼", "å‰¥è½", "æ±¡æ¸/éœ‰æ–‘"],
                key="advanced_target_defects"
            )
            
            st.markdown("### ä¿®å¤ç®—æ³•")
            restoration_method = st.selectbox(
                "é€‰æ‹©ä¿®å¤ç®—æ³•",
                ["comprehensive", "telea", "ns", "texture_aware", "deep_learning"],
                format_func=lambda x: {
                    "comprehensive": "ç»¼åˆæ™ºèƒ½ä¿®å¤",
                    "telea": "Teleaç®—æ³•(å¿«é€Ÿ)",
                    "ns": "Navier-Stokesç®—æ³•(è´¨é‡)",
                    "texture_aware": "çº¹ç†æ„ŸçŸ¥ä¿®å¤", 
                    "deep_learning": "æ·±åº¦å­¦ä¹ ä¿®å¤(æ¨¡æ‹Ÿ)"
                }[x],
                key="advanced_method"
            )
        
        with col2:
            st.markdown("### å‚æ•°é…ç½®")
            restoration_radius = st.slider(
                "ä¿®å¤åŠå¾„", min_value=1, max_value=15, value=3, 
                help="ä¿®å¤æ“ä½œçš„å½±å“èŒƒå›´", key="advanced_radius"
            )
            
            restoration_iterations = st.slider(
                "ä¿®å¤è¿­ä»£æ¬¡æ•°", min_value=1, max_value=5, value=1,
                help="å¤šæ¬¡è¿­ä»£å¯èƒ½è·å¾—æ›´å¥½æ•ˆæœ", key="advanced_iterations"
            )
            
            enable_color_restoration = st.checkbox(
                "å¯ç”¨è‰²å½©å¤åŸ", value=True, 
                help="è‡ªåŠ¨è°ƒæ•´è‰²å½©ã€å¯¹æ¯”åº¦å’Œé¥±å’Œåº¦", key="advanced_color"
            )
        
        st.markdown("### é«˜çº§é€‰é¡¹")
        advanced_col1, advanced_col2 = st.columns(2)
        
        with advanced_col1:
            texture_weight = st.slider(
                "çº¹ç†æƒé‡", min_value=0.0, max_value=1.0, value=0.7,
                help="çº¹ç†ä¿®å¤æ—¶çº¹ç†ä¿æŒçš„æƒé‡", key="texture_weight"
            )
            
            contrast_enhancement = st.slider(
                "å¯¹æ¯”åº¦å¢å¼º", min_value=1.0, max_value=3.0, value=1.5,
                help="è‰²å½©å¤åŸæ—¶çš„å¯¹æ¯”åº¦å¢å¼ºå¼ºåº¦", key="contrast_enhance"
            )
        
        with advanced_col2:
            saturation_boost = st.slider(
                "é¥±å’Œåº¦å¢å¼º", min_value=1.0, max_value=2.0, value=1.2,
                help="è‰²å½©å¤åŸæ—¶çš„é¥±å’Œåº¦å¢å¼ºå¼ºåº¦", key="saturation_boost"
            )
            
            sharpening_strength = st.slider(
                "é”åŒ–å¼ºåº¦", min_value=0.0, max_value=1.5, value=0.5,
                help="å›¾åƒé”åŒ–å¼ºåº¦", key="sharpening_strength"
            )
        
        if st.button("ğŸš€ æ‰§è¡Œé«˜çº§å¤åŸ", key="run_advanced_restoration"):
            with st.spinner("æ­£åœ¨è¿›è¡Œé«˜çº§å›¾åƒå¤åŸ..."):
                restoration_system = VirtualRestorationSystem()
                
                restoration_config = {
                    'target_defects': target_defects,
                    'method': restoration_method,
                    'radius': restoration_radius,
                    'iterations': restoration_iterations,
                    'color_restoration': enable_color_restoration,
                    'texture_weight': texture_weight,
                    'contrast_enhancement': contrast_enhancement,
                    'saturation_boost': saturation_boost,
                    'sharpening_strength': sharpening_strength
                }
                
                restored_image, used_mask = restoration_system.comprehensive_restoration(
                    img_rgb, masks_dict, restoration_config
                )
                
                st.markdown("### å¤åŸç»“æœå¯¹æ¯”")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.image(img_rgb, caption="åŸå§‹å›¾åƒ", use_column_width=True)
                    mask_overlay = img_rgb.copy()
                    mask_overlay[used_mask > 0] = [255, 0, 0]
                    st.image(mask_overlay, caption="ä¿®å¤åŒºåŸŸæ ‡è¯†(çº¢è‰²)", use_column_width=True)
                
                with col2:
                    st.image(restored_image, caption="å¤åŸåå›¾åƒ", use_column_width=True)
                    
                    total_pixels = img_rgb.shape[0] * img_rgb.shape[1]
                    restored_pixels = np.sum(used_mask > 0)
                    restoration_ratio = (restored_pixels / total_pixels) * 100
                    
                    st.metric("ä¿®å¤åŒºåŸŸå æ¯”", f"{restoration_ratio:.2f}%")
                
                st.markdown("### ä¸‹è½½å¤åŸç»“æœ")
                download_col1, download_col2 = st.columns(2)
                
                with download_col1:
                    buf_restored = BytesIO()
                    Image.fromarray(restored_image).save(buf_restored, format="PNG")
                    st.download_button(
                        "ğŸ“¥ ä¸‹è½½å¤åŸå›¾åƒ(PNG)",
                        data=buf_restored.getvalue(),
                        file_name="advanced_restored.png",
                        mime="image/png"
                    )
                
                with download_col2:
                    report = generate_restoration_report(
                        img_rgb, restored_image, used_mask, restoration_config
                    )
                    st.download_button(
                        "ğŸ“Š ä¸‹è½½ä¿®å¤æŠ¥å‘Š(TXT)",
                        data=report.encode('utf-8'),
                        file_name="restoration_report.txt",
                        mime="text/plain"
                    )


def generate_restoration_report(original, restored, mask, config):
    """ç”Ÿæˆä¿®å¤æŠ¥å‘Š"""
    original_size = f"{original.shape[1]}x{original.shape[0]}"
    restored_pixels = np.sum(mask > 0)
    total_pixels = original.shape[0] * original.shape[1]
    restoration_ratio = (restored_pixels / total_pixels) * 100
    
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
é«˜çº§å›¾åƒå¤åŸæŠ¥å‘Š
================

ä¿®å¤æ—¶é—´: {current_time}
åŸå§‹å›¾åƒå°ºå¯¸: {original_size}
æ€»åƒç´ æ•°: {total_pixels:,}

ä¿®å¤ç»Ÿè®¡:
--------
ä¿®å¤åŒºåŸŸåƒç´ : {restored_pixels:,}
ä¿®å¤åŒºåŸŸå æ¯”: {restoration_ratio:.2f}%
ä¿®å¤ç—…å®³ç±»å‹: {', '.join(config['target_defects'])}

ä¿®å¤å‚æ•°:
--------
ä¿®å¤ç®—æ³•: {config['method']}
ä¿®å¤åŠå¾„: {config['radius']} åƒç´ 
è¿­ä»£æ¬¡æ•°: {config['iterations']}
è‰²å½©å¤åŸ: {'å¯ç”¨' if config['color_restoration'] else 'ç¦ç”¨'}
çº¹ç†æƒé‡: {config.get('texture_weight', 0.7):.1f}

æŠ€æœ¯è¯´æ˜:
--------
æœ¬å¤åŸé‡‡ç”¨å…ˆè¿›çš„å›¾åƒå¤„ç†ç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒç—…å®³ç±»å‹é‡‡ç”¨å·®å¼‚åŒ–ä¿®å¤ç­–ç•¥ã€‚
ä¿®å¤è¿‡ç¨‹å°½å¯èƒ½ä¿æŒæ–‡ç‰©çš„åŸå§‹é£è²Œå’Œè‰ºæœ¯ä»·å€¼ã€‚

æ³¨æ„äº‹é¡¹:
--------
1. è™šæ‹Ÿä¿®å¤ç»“æœä»…ä¾›å‚è€ƒï¼Œå®é™…ä¿®å¤éœ€ä¸“ä¸šè¯„ä¼°
2. å»ºè®®ç»“åˆå®åœ°å‹˜å¯Ÿå’Œææ–™åˆ†æ
3. é‡è¦æ–‡ç‰©ä¿®å¤åº”éµå¾ªç›¸å…³è§„èŒƒå’Œæ ‡å‡†

ç”Ÿæˆç³»ç»Ÿ: çŸ³çªŸå¯ºå£ç”»AIä¿æŠ¤å¹³å°
    """
    
    return report

# Session initï¼ˆUIå‡½æ•°å·²åœ¨å‰é¢å®šä¹‰ï¼Œè¿™é‡Œä¸å†é‡å¤ï¼‰
if "proc" not in st.session_state:
    st.session_state["proc"] = None

# ---------------------------
# åŠ¨æ€èƒŒæ™¯ä¸å“ç‰Œæ ‡è¯†
# ---------------------------

@st.cache_data(show_spinner=False)
def get_background_images_b64(dir_path: str = "assets/backgrounds"):
    exts = {".jpg", ".jpeg", ".png", ".webp"}
    images: list[str] = []
    try:
        if os.path.isdir(dir_path):
            for name in sorted(os.listdir(dir_path)):
                ext = os.path.splitext(name)[1].lower()
                if ext in exts:
                    full = os.path.join(dir_path, name)
                    with open(full, "rb") as f:
                        b64 = base64.b64encode(f.read()).decode("utf-8")
                        mime = "image/png" if ext == ".png" else ("image/webp" if ext == ".webp" else "image/jpeg")
                        images.append(f"data:{mime};base64,{b64}")
    except Exception:
        pass
    return images

@st.cache_data(show_spinner=False)
def get_logo_b64(candidates: list[str] = [
    "assets/sjtu_design.png", "assets/sjtu.png", "assets/logo_sjtu.png"
]):
    for p in candidates:
        try:
            if os.path.isfile(p):
                with open(p, "rb") as f:
                    b64 = base64.b64encode(f.read()).decode("utf-8")
                    ext = os.path.splitext(p)[1].lower()
                    mime = "image/png" if ext == ".png" else ("image/webp" if ext == ".webp" else "image/jpeg")
                    return f"data:{mime};base64,{b64}"
        except Exception:
            continue
    return None

def inject_dynamic_background(images_data_urls: list[str], interval_ms: int = 8000):
    if not images_data_urls:
        return
    imgs_js_array = ",".join([f"'" + u + "'" for u in images_data_urls])
    css = f"""
    <style>
    /* å…¨å±€æ ·å¼ä¼˜åŒ– */
    .stApp {{
        background-size: cover !important;
        background-position: center center !important;
        background-attachment: fixed !important;
        transition: background-image 1.2s ease-in-out;
        font-family: 'Segoe UI', 'Microsoft YaHei', sans-serif !important;
    }}
    
    .bg-overlay::before {{
        content: "";
        position: fixed;
        inset: 0;
        background: linear-gradient(135deg, rgba(0,0,0,0.3) 0%, rgba(0,0,0,0.6) 100%);
        pointer-events: none;
        z-index: 0;
    }}
    
    /* ä¸»å®¹å™¨ç¾åŒ– */
    .main .block-container {{
        padding-top: 2rem !important;
        padding-bottom: 2rem !important;
        max-width: 1200px !important;
    }}
    
    /* ä¾§è¾¹æ ç¾åŒ– */
    .css-1d391kg {{
        background: rgba(255,255,255,0.95) !important;
        backdrop-filter: blur(10px) !important;
        border-right: 1px solid rgba(255,255,255,0.2) !important;
        box-shadow: 2px 0 20px rgba(0,0,0,0.1) !important;
    }}
    
    /* æ ‡é¢˜ç¾åŒ– */
    h1, h2, h3, h4, h5, h6 {{
        color: #2c3e50 !important;
        font-weight: 600 !important;
        margin-bottom: 1rem !important;
        text-shadow: 0 1px 3px rgba(0,0,0,0.1) !important;
    }}
    
    /* å¡ç‰‡æ ·å¼ */
    .stTabs [data-baseweb="tab-list"] {{
        gap: 8px !important;
        background: rgba(255,255,255,0.9) !important;
        border-radius: 12px !important;
        padding: 8px !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1) !important;
        backdrop-filter: blur(10px) !important;
    }}
    
    .stTabs [data-baseweb="tab"] {{
        border-radius: 8px !important;
        padding: 12px 20px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
        background: transparent !important;
    }}
    
    .stTabs [aria-selected="true"] {{
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
        color: white !important;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4) !important;
    }}
    
    /* æŒ‰é’®ç¾åŒ– */
    .stButton > button {{
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
        color: white !important;
        border: none !important;
        border-radius: 8px !important;
        padding: 0.5rem 1.5rem !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3) !important;
    }}
    
    .stButton > button:hover {{
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4) !important;
    }}
    
    /* æ–‡ä»¶ä¸Šä¼ åŒºåŸŸç¾åŒ– */
    .stFileUploader {{
        border: 2px dashed rgba(102, 126, 234, 0.3) !important;
        border-radius: 12px !important;
        background: rgba(255,255,255,0.8) !important;
        backdrop-filter: blur(10px) !important;
        transition: all 0.3s ease !important;
    }}
    
    .stFileUploader:hover {{
        border-color: rgba(102, 126, 234, 0.6) !important;
        background: rgba(255,255,255,0.9) !important;
    }}
    
    /* æŒ‡æ ‡å¡ç‰‡ç¾åŒ– */
    .metric-container {{
        background: rgba(255,255,255,0.9) !important;
        border-radius: 12px !important;
        padding: 1rem !important;
        margin: 0.5rem 0 !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1) !important;
        backdrop-filter: blur(10px) !important;
        border: 1px solid rgba(255,255,255,0.2) !important;
    }}
    
    /* è­¦å‘Šå’ŒæˆåŠŸæ¶ˆæ¯ç¾åŒ– */
    .stAlert {{
        border-radius: 12px !important;
        border: none !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1) !important;
        backdrop-filter: blur(10px) !important;
    }}
    
    /* æ•°æ®æ¡†ç¾åŒ– */
    .stDataFrame {{
        border-radius: 12px !important;
        overflow: hidden !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1) !important;
    }}
    
    /* ä»£ç å—ç¾åŒ– */
    .stCode {{
        border-radius: 8px !important;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1) !important;
    }}
    
    /* è¿›åº¦æ¡ç¾åŒ– */
    .stProgress > div > div > div {{
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
        border-radius: 10px !important;
    }}
    
    /* é€‰æ‹©æ¡†ç¾åŒ– */
    .stSelectbox > div > div {{
        background: rgba(255,255,255,0.9) !important;
        border-radius: 8px !important;
        border: 1px solid rgba(102, 126, 234, 0.3) !important;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1) !important;
    }}
    
    /* æ–‡æœ¬è¾“å…¥ç¾åŒ– */
    .stTextArea > div > div > textarea {{
        background: rgba(255,255,255,0.9) !important;
        border-radius: 8px !important;
        border: 1px solid rgba(102, 126, 234, 0.3) !important;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1) !important;
    }}
    
    /* ä¾§è¾¹æ æ»‘å—ç¾åŒ– */
    .stSlider > div > div > div {{
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
    }}
    
    /* é¡µè„šç¾åŒ– */
    .footer-content {{
        background: rgba(255,255,255,0.9) !important;
        backdrop-filter: blur(10px) !important;
        border-radius: 12px !important;
        padding: 1rem 2rem !important;
        margin: 2rem auto !important;
        max-width: 600px !important;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1) !important;
        border: 1px solid rgba(255,255,255,0.2) !important;
    }}
    
    /* åŠ¨ç”»æ•ˆæœ */
    @keyframes fadeInUp {{
        from {{
            opacity: 0;
            transform: translateY(30px);
        }}
        to {{
            opacity: 1;
            transform: translateY(0);
        }}
    }}
    
    .main .block-container > div {{
        animation: fadeInUp 0.6s ease-out !important;
    }}
    
    /* å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {{
        .main .block-container {{
            padding: 1rem !important;
        }}
        
        .stTabs [data-baseweb="tab"] {{
            padding: 8px 12px !important;
            font-size: 14px !important;
        }}
    }}
    </style>
    """
    js = f"""
    <script>
    const bgImgs = [{imgs_js_array}];
    let idx = 0;
    function applyBg() {{
      const el = parent.document.querySelector('.stApp');
      if (!el) return;
      el.style.backgroundImage = `url(${bgImgs[idx]})`;
      idx = (idx + 1) % bgImgs.length;
    }}
    applyBg();
    if (!window.__bgInterval) {{
      window.__bgInterval = setInterval(applyBg, {interval_ms});
    }}
    const root = parent.document.querySelector('.stApp');
    if (root && !root.classList.contains('bg-overlay')) {{
      root.classList.add('bg-overlay');
    }}
    </script>
    """
    st.markdown(css + js, unsafe_allow_html=True)

def inject_footer_with_logo(logo_data_url: str | None):
    logo_img_html = f'<img src="{logo_data_url}" alt="SJTU Design" />' if logo_data_url else ""
    css = """
    <style>
    .app-footer {
        position: fixed;
        left: 0; right: 0; bottom: 0;
        display: flex; justify-content: center; align-items: center;
        gap: 12px;
        padding: 8px 12px;
        background: rgba(255,255,255,0.85);
        backdrop-filter: blur(6px);
        box-shadow: 0 -4px 18px rgba(0,0,0,0.08);
        z-index: 10000;
    }
    .app-footer img { height: 26px; width: auto; display: block; }
    .app-footer .foot-title { font-weight: 600; color: #333; font-size: 12px; }
    .app-footer .foot-split { color: #aaa; }
    </style>
    """
    html = f"""
    <div class=\"app-footer\">
      {logo_img_html}
      <div class=\"foot-title\">ä¸Šæµ·äº¤é€šå¤§å­¦ è®¾è®¡å­¦é™¢</div>
      <span class=\"foot-split\">|</span>
      <div class=\"foot-title\">AI+æ–‡ç‰©ä¿æŠ¤ç ”ç©¶</div>
    </div>
    """
    st.markdown(css + html, unsafe_allow_html=True)

def _file_to_data_url(file_bytes: bytes, filename: str) -> str:
    ext = os.path.splitext(filename)[1].lower()
    mime = "image/png" if ext == ".png" else ("image/webp" if ext == ".webp" else "image/jpeg")
    b64 = base64.b64encode(file_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"

# ---------------------------
# å¤šæ¨¡æ€èåˆç³»ç»Ÿ
# ---------------------------

class KnowledgeGraph:
    """çŸ³çªŸç—…å®³çŸ¥è¯†å›¾è°±"""
    def __init__(self):
        self.graph = nx.DiGraph()
        self._build_knowledge_graph()
    
    def _build_knowledge_graph(self):
        """æ„å»ºçŸ³çªŸç±»å‹-æè´¨-å…¸å‹ç—…å®³-ä¿®å¤æ‰‹æ®µçŸ¥è¯†å›¾è°±"""
        # çŸ³çªŸç±»å‹èŠ‚ç‚¹
        cave_types = {
            "æ•¦ç…Œè«é«˜çªŸ": {"era": "åŒ—é­-å…ƒä»£", "climate": "å¹²æ—±", "structure": "ç ‚å²©"},
            "äº‘å†ˆçŸ³çªŸ": {"era": "åŒ—é­", "climate": "æ¸©å¸¦", "structure": "èŠ±å²—å²©"},
            "é¾™é—¨çŸ³çªŸ": {"era": "åŒ—é­-å”ä»£", "climate": "æ¸©å¸¦", "structure": "çŸ³ç°å²©"},
            "éº¦ç§¯å±±çŸ³çªŸ": {"era": "åç§¦-æ¸…ä»£", "climate": "æ¸©å¸¦", "structure": "æ³¥è´¨ç ‚å²©"}
        }
        
        # æè´¨èŠ‚ç‚¹
        materials = {
            "ç ‚å²©": {"porosity": "é«˜", "hardness": "ä¸­", "weathering": "æ˜“é£åŒ–"},
            "èŠ±å²—å²©": {"porosity": "ä½", "hardness": "é«˜", "weathering": "æŠ—é£åŒ–"},
            "çŸ³ç°å²©": {"porosity": "ä¸­", "hardness": "ä¸­", "weathering": "æ˜“æº¶èš€"},
            "æ³¥è´¨ç ‚å²©": {"porosity": "é«˜", "hardness": "ä½", "weathering": "ææ˜“é£åŒ–"}
        }
        
        # ç—…å®³èŠ‚ç‚¹
        pathologies = {
            "è¡¨é¢è£‚ç¼": {"severity": "ä¸­", "depth": "æµ…å±‚", "cause": "æ¸©å·®åº”åŠ›"},
            "æ·±å±‚è£‚ç¼": {"severity": "é«˜", "depth": "æ·±å±‚", "cause": "ç»“æ„åº”åŠ›"},
            "å‰¥è½": {"severity": "é«˜", "depth": "è¡¨é¢", "cause": "é£åŒ–"},
            "å˜è‰²": {"severity": "ä½", "depth": "è¡¨é¢", "cause": "æ°§åŒ–"},
            "ç›æ": {"severity": "ä¸­", "depth": "è¡¨é¢", "cause": "ç›åˆ†ç»“æ™¶"},
            "ç”Ÿç‰©ä¾µèš€": {"severity": "ä¸­", "depth": "è¡¨é¢", "cause": "å¾®ç”Ÿç‰©"}
        }
        
        # ä¿®å¤æ‰‹æ®µèŠ‚ç‚¹
        treatments = {
            "è¡¨é¢åŠ å›º": {"cost": "ä½", "effectiveness": "ä¸­", "durability": "çŸ­"},
            "æ·±å±‚æ³¨æµ†": {"cost": "é«˜", "effectiveness": "é«˜", "durability": "é•¿"},
            "è¡¨é¢æ¸…æ´—": {"cost": "ä½", "effectiveness": "é«˜", "durability": "çŸ­"},
            "ä¿æŠ¤æ¶‚å±‚": {"cost": "ä¸­", "effectiveness": "ä¸­", "durability": "ä¸­"},
            "ç¯å¢ƒæ§åˆ¶": {"cost": "é«˜", "effectiveness": "é«˜", "durability": "é•¿"}
        }
        
        # æ„å»ºå›¾ç»“æ„
        for cave, props in cave_types.items():
            self.graph.add_node(cave, type="cave", **props)
        
        for material, props in materials.items():
            self.graph.add_node(material, type="material", **props)
        
        for pathology, props in pathologies.items():
            self.graph.add_node(pathology, type="pathology", **props)
        
        for treatment, props in treatments.items():
            self.graph.add_node(treatment, type="treatment", **props)
        
        # æ·»åŠ å…³ç³»è¾¹
        relationships = [
            # çŸ³çªŸ-æè´¨å…³ç³»
            ("æ•¦ç…Œè«é«˜çªŸ", "ç ‚å²©", {"compatibility": "é«˜"}),
            ("äº‘å†ˆçŸ³çªŸ", "èŠ±å²—å²©", {"compatibility": "é«˜"}),
            ("é¾™é—¨çŸ³çªŸ", "çŸ³ç°å²©", {"compatibility": "é«˜"}),
            ("éº¦ç§¯å±±çŸ³çªŸ", "æ³¥è´¨ç ‚å²©", {"compatibility": "é«˜"}),
            
            # æè´¨-ç—…å®³å…³ç³»
            ("ç ‚å²©", "è¡¨é¢è£‚ç¼", {"probability": 0.8}),
            ("ç ‚å²©", "å‰¥è½", {"probability": 0.9}),
            ("èŠ±å²—å²©", "æ·±å±‚è£‚ç¼", {"probability": 0.6}),
            ("çŸ³ç°å²©", "ç›æ", {"probability": 0.7}),
            ("æ³¥è´¨ç ‚å²©", "å‰¥è½", {"probability": 0.95}),
            ("æ³¥è´¨ç ‚å²©", "ç”Ÿç‰©ä¾µèš€", {"probability": 0.8}),
            
            # ç—…å®³-ä¿®å¤å…³ç³»
            ("è¡¨é¢è£‚ç¼", "è¡¨é¢åŠ å›º", {"suitability": 0.9}),
            ("æ·±å±‚è£‚ç¼", "æ·±å±‚æ³¨æµ†", {"suitability": 0.95}),
            ("å‰¥è½", "è¡¨é¢åŠ å›º", {"suitability": 0.8}),
            ("å˜è‰²", "è¡¨é¢æ¸…æ´—", {"suitability": 0.9}),
            ("ç›æ", "è¡¨é¢æ¸…æ´—", {"suitability": 0.85}),
            ("ç”Ÿç‰©ä¾µèš€", "è¡¨é¢æ¸…æ´—", {"suitability": 0.8}),
        ]
        
        for source, target, attrs in relationships:
            self.graph.add_edge(source, target, **attrs)
    
    def query_treatment(self, cave_type, material, pathologies):
        """æ ¹æ®çŸ³çªŸç±»å‹ã€æè´¨å’Œç—…å®³æŸ¥è¯¢æœ€ä½³ä¿®å¤æ–¹æ¡ˆ"""
        treatments = []
        for pathology in pathologies:
            # æŸ¥æ‰¾è¯¥ç—…å®³çš„ä¿®å¤æ–¹æ¡ˆ
            for treatment in self.graph.successors(pathology):
                if self.graph.nodes[treatment]["type"] == "treatment":
                    suitability = self.graph[pathology][treatment].get("suitability", 0.5)
                    treatments.append({
                        "pathology": pathology,
                        "treatment": treatment,
                        "suitability": suitability,
                        "cost": self.graph.nodes[treatment]["cost"],
                        "effectiveness": self.graph.nodes[treatment]["effectiveness"],
                        "durability": self.graph.nodes[treatment]["durability"]
                    })
        
        # æŒ‰é€‚ç”¨æ€§æ’åº
        treatments.sort(key=lambda x: x["suitability"], reverse=True)
        return treatments

class MultimodalFusion:
    """å¤šæ¨¡æ€èåˆç³»ç»Ÿ"""
    def __init__(self):
        self.knowledge_graph = KnowledgeGraph()
        self.text_encoder = None
        self.image_encoder = None
        self.pointcloud_encoder = None
        self._init_encoders()
    
    def _init_encoders(self):
        """åˆå§‹åŒ–å„æ¨¡æ€ç¼–ç å™¨"""
        if not MULTIMODAL_AVAILABLE:
            return
        
        try:
            # æ–‡æœ¬ç¼–ç å™¨ï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡BERTï¼‰
            self.text_tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
            self.text_encoder = AutoModel.from_pretrained("bert-base-chinese")
        except:
            st.warning("æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    def encode_image(self, image):
        """å›¾åƒç‰¹å¾ç¼–ç """
        if image is None:
            return None
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetç‰¹å¾
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        features = cv2.resize(image, (224, 224))
        features = cv2.cvtColor(features, cv2.COLOR_BGR2RGB)
        features = features.flatten()[:512]  # ç®€åŒ–ç‰¹å¾
        return features / np.linalg.norm(features)
    
    def encode_pointcloud(self, pointcloud):
        """ç‚¹äº‘ç‰¹å¾ç¼–ç """
        if pointcloud is None or o3d is None:
            return None
        
        # è®¡ç®—ç‚¹äº‘å‡ ä½•ç‰¹å¾
        features = []
        
        # å¯†åº¦ç‰¹å¾
        if len(pointcloud.points) > 0:
            bbox = pointcloud.get_axis_aligned_bounding_box()
            volume = bbox.volume()
            density = len(pointcloud.points) / max(volume, 1e-6)
            features.append(density)
        else:
            features.append(0)
        
        # è¡¨é¢ç²—ç³™åº¦ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        if len(pointcloud.points) > 10:
            points = np.asarray(pointcloud.points)
            distances = np.linalg.norm(points - np.mean(points, axis=0), axis=1)
            roughness = np.std(distances)
            features.append(roughness)
        else:
            features.append(0)
        
        # æ³•å‘é‡åˆ†å¸ƒï¼ˆç®€åŒ–ï¼‰
        if hasattr(pointcloud, 'normals') and len(pointcloud.normals) > 0:
            normals = np.asarray(pointcloud.normals)
            normal_std = np.std(normals, axis=0)
            features.extend(normal_std.tolist())
        else:
            features.extend([0, 0, 0])
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(features) < 64:
            features.append(0)
        
        features = np.array(features[:64])
        return features / (np.linalg.norm(features) + 1e-8)
    
    def encode_text(self, text):
        """æ–‡æœ¬ç‰¹å¾ç¼–ç """
        if not text or self.text_encoder is None:
            return None
        
        try:
            inputs = self.text_tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
            with torch.no_grad():
                outputs = self.text_encoder(**inputs)
                features = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            return features
        except:
            # ç®€åŒ–æ–‡æœ¬ç¼–ç 
            words = text.split()
            features = np.zeros(768)
            for i, word in enumerate(words[:10]):  # åªå–å‰10ä¸ªè¯
                features[i*77:(i+1)*77] = np.random.randn(77)  # ç®€åŒ–å¤„ç†
            return features / (np.linalg.norm(features) + 1e-8)
    
    def fuse_modalities(self, image_features, pointcloud_features, text_features):
        """å¤šæ¨¡æ€ç‰¹å¾èåˆ"""
        features = []
        
        if image_features is not None:
            features.append(image_features)
        if pointcloud_features is not None:
            features.append(pointcloud_features)
        if text_features is not None:
            features.append(text_features)
        
        if not features:
            return None
        
        # ç®€å•æ‹¼æ¥èåˆï¼ˆå®é™…åº”è¯¥ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼‰
        fused = np.concatenate(features)
        return fused / (np.linalg.norm(fused) + 1e-8)
    
    def analyze_depth_stability(self, image, pointcloud, crack_mask):
        """ç»“åˆç‚¹äº‘åˆ†æè£‚ç¼æ·±åº¦å’Œç»“æ„ç¨³å®šæ€§"""
        if pointcloud is None or o3d is None:
            return {"depth": "unknown", "stability": "unknown", "confidence": 0.0}
        
        try:
            # æå–è£‚ç¼åŒºåŸŸçš„ç‚¹äº‘
            points = np.asarray(pointcloud.points)
            if len(points) == 0:
                return {"depth": "unknown", "stability": "unknown", "confidence": 0.0}
            
            # è®¡ç®—è£‚ç¼æ·±åº¦ï¼ˆç®€åŒ–ç®—æ³•ï¼‰
            z_coords = points[:, 2]  # å‡è®¾Zè½´æ˜¯æ·±åº¦
            depth_variance = np.var(z_coords)
            
            # è®¡ç®—ç»“æ„ç¨³å®šæ€§æŒ‡æ ‡
            bbox = pointcloud.get_axis_aligned_bounding_box()
            volume = bbox.volume()
            point_density = len(points) / max(volume, 1e-6)
            
            # åŸºäºå‡ ä½•ç‰¹å¾åˆ¤æ–­
            if depth_variance > 0.1:
                depth = "deep"
                stability = "unstable"
            elif depth_variance > 0.05:
                depth = "medium"
                stability = "moderate"
            else:
                depth = "shallow"
                stability = "stable"
            
            confidence = min(point_density * 10, 1.0)
            
            return {
                "depth": depth,
                "stability": stability,
                "confidence": confidence,
                "depth_variance": depth_variance,
                "point_density": point_density
            }
        except Exception as e:
            return {"depth": "error", "stability": "error", "confidence": 0.0, "error": str(e)}

class AutoAnnotator:
    """LLMè‡ªåŠ¨æ ‡æ³¨ç³»ç»Ÿ"""
    def __init__(self):
        self.annotation_templates = {
            "crack": {
                "description": "è£‚ç¼ç—…å®³ï¼Œé€šå¸¸è¡¨ç°ä¸ºçº¿æ€§ç¼ºé™·",
                "severity_levels": ["è½»å¾®", "ä¸­ç­‰", "ä¸¥é‡"],
                "key_features": ["çº¿æ€§", "è¿ç»­æ€§", "æ·±åº¦å˜åŒ–"]
            },
            "peel": {
                "description": "å‰¥è½ç—…å®³ï¼Œè¡¨é¢ææ–™è„±è½",
                "severity_levels": ["è½»å¾®", "ä¸­ç­‰", "ä¸¥é‡"],
                "key_features": ["ä¸è§„åˆ™å½¢çŠ¶", "è¾¹ç¼˜æ¸…æ™°", "åšåº¦å˜åŒ–"]
            },
            "discolor": {
                "description": "å˜è‰²ç—…å®³ï¼Œé¢œè‰²å¼‚å¸¸å˜åŒ–",
                "severity_levels": ["è½»å¾®", "ä¸­ç­‰", "ä¸¥é‡"],
                "key_features": ["é¢œè‰²å·®å¼‚", "è¾¹ç•Œæ¨¡ç³Š", "é¢ç§¯åˆ†å¸ƒ"]
            }
        }
    
    def generate_annotation(self, image, detected_regions, defect_type):
        """åŸºäºæ£€æµ‹ç»“æœç”Ÿæˆè‡ªåŠ¨æ ‡æ³¨"""
        if defect_type not in self.annotation_templates:
            return None
        
        template = self.annotation_templates[defect_type]
        annotations = []
        
        for region in detected_regions:
            # è®¡ç®—åŒºåŸŸç‰¹å¾
            area = region.get("area", 0)
            bbox = region.get("bbox", [0, 0, 0, 0])
            elongation = region.get("elongation", 0)
            
            # åŸºäºç‰¹å¾åˆ¤æ–­ä¸¥é‡ç¨‹åº¦
            if area > 1000:
                severity = "ä¸¥é‡"
            elif area > 500:
                severity = "ä¸­ç­‰"
            else:
                severity = "è½»å¾®"
            
            # ç”Ÿæˆæ ‡æ³¨æ–‡æœ¬
            annotation = {
                "type": defect_type,
                "description": template["description"],
                "severity": severity,
                "area": area,
                "bbox": bbox,
                "confidence": 0.8,  # ç®€åŒ–ç½®ä¿¡åº¦
                "features": {
                    "elongation": elongation,
                    "aspect_ratio": bbox[2] / max(bbox[3], 1),
                    "area_ratio": area / (image.shape[0] * image.shape[1])
                }
            }
            annotations.append(annotation)
        
        return annotations

class GenerativeAugmentation:
    """ç”Ÿæˆå¼å¢å¼ºï¼šè™šæ‹Ÿä¿®å¤"""
    def __init__(self):
        self.restoration_templates = {
            "crack": {
                "method": "inpainting",
                "parameters": {"algorithm": "telea", "radius": 3}
            },
            "peel": {
                "method": "texture_synthesis",
                "parameters": {"patch_size": 32, "overlap": 8}
            },
            "discolor": {
                "method": "color_correction",
                "parameters": {"method": "reinhard", "target": "reference"}
            }
        }
    
    def virtual_restoration(self, image, mask, defect_type):
        """è™šæ‹Ÿä¿®å¤æ¨¡æ‹Ÿ"""
        if defect_type not in self.restoration_templates:
            return image
        
        template = self.restoration_templates[defect_type]
        
        if template["method"] == "inpainting":
            # ä½¿ç”¨OpenCVä¿®å¤
            result = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)
        elif template["method"] == "color_correction":
            # é¢œè‰²æ ¡æ­£
            result = self._color_correction(image, mask)
        else:
            result = image
        
        return result
    
    def _color_correction(self, image, mask):
        """é¢œè‰²æ ¡æ­£"""
        # ç®€åŒ–é¢œè‰²æ ¡æ­£
        result = image.copy()
        mask_3ch = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
        
        # åŸºäºå‘¨å›´åŒºåŸŸçš„é¢œè‰²è¿›è¡Œæ ¡æ­£
        kernel = np.ones((5, 5), np.uint8)
        mask_dilated = cv2.dilate(mask, kernel, iterations=2)
        mask_eroded = cv2.erode(mask, kernel, iterations=2)
        border_mask = mask_dilated - mask_eroded
        
        if np.sum(border_mask) > 0:
            # è®¡ç®—è¾¹ç•ŒåŒºåŸŸçš„å¹³å‡é¢œè‰²
            border_pixels = image[border_mask > 0]
            if len(border_pixels) > 0:
                mean_color = np.mean(border_pixels, axis=0)
                result[mask > 0] = mean_color
        
        return result

# å…¨å±€å¤šæ¨¡æ€ç³»ç»Ÿå®ä¾‹
@st.cache_resource
def get_multimodal_system():
    return MultimodalFusion()

@st.cache_resource
def get_auto_annotator():
    return AutoAnnotator()

@st.cache_resource
def get_generative_augmentation():
    return GenerativeAugmentation()

# ---------------------------
# æ·±åº¦å­¦ä¹ ç³»ç»Ÿ
# ---------------------------

if DEEP_LEARNING_AVAILABLE:
    class MuralDataset(Dataset):
        """å£ç”»ç—…å®³æ•°æ®é›†"""
        def __init__(self, images, labels, transform=None):
            self.images = images
            self.labels = labels
            self.transform = transform
        
        def __len__(self):
            return len(self.images)
        
        def __getitem__(self, idx):
            image = self.images[idx]
            label = self.labels[idx]
            
            if self.transform:
                image = self.transform(image)
            
            return image, label

    class DefectClassifier(nn.Module):
        """ç—…å®³åˆ†ç±»å™¨"""
        def __init__(self, num_classes=6, pretrained=True):
            super(DefectClassifier, self).__init__()
            
            # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetä½œä¸ºéª¨å¹²ç½‘ç»œ
            self.backbone = models.resnet50(pretrained=pretrained)
            num_features = self.backbone.fc.in_features
            
            # æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
        
        def forward(self, x):
            return self.backbone(x)

    class DataAugmentation:
        """æ•°æ®å¢å¼º"""
        def __init__(self):
            self.transform = A.Compose([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Rotate(limit=15, p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
                A.Blur(blur_limit=3, p=0.3),
                A.RandomCrop(height=224, width=224, p=0.8),
                A.Resize(height=224, width=224),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        
        def __call__(self, image):
            return self.transform(image=image)['image']

    class ModelTrainer:
        """æ¨¡å‹è®­ç»ƒå™¨"""
        def __init__(self, model, device='cpu'):
            self.model = model
            self.device = device
            self.model.to(device)
            self.train_losses = []
            self.val_losses = []
            self.train_accuracies = []
            self.val_accuracies = []
        
        def train_epoch(self, train_loader, optimizer, criterion):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
            
            avg_loss = total_loss / len(train_loader)
            accuracy = 100. * correct / total
            
            self.train_losses.append(avg_loss)
            self.train_accuracies.append(accuracy)
            
            return avg_loss, accuracy
        
        def validate(self, val_loader, criterion):
            self.model.eval()
            total_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.model(data)
                    loss = criterion(output, target)
                    
                    total_loss += loss.item()
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
            
            avg_loss = total_loss / len(val_loader)
            accuracy = 100. * correct / total
            
            self.val_losses.append(avg_loss)
            self.val_accuracies.append(accuracy)
            
            return avg_loss, accuracy
        
        def train(self, train_loader, val_loader, epochs, learning_rate=0.001, scheduler_type='step'):
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
            criterion = nn.CrossEntropyLoss()
            
            if scheduler_type == 'step':
                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
            else:
                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
            
            for epoch in range(epochs):
                train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)
                val_loss, val_acc = self.validate(val_loader, criterion)
                scheduler.step()
                
                yield epoch, train_loss, train_acc, val_loss, val_acc

    class ModelEvaluator:
        """æ¨¡å‹è¯„ä¼°å™¨"""
        def __init__(self, model, device='cpu'):
            self.model = model
            self.device = device
        
        def evaluate(self, test_loader):
            self.model.eval()
            all_preds = []
            all_targets = []
            
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.model(data)
                    pred = output.argmax(dim=1)
                    
                    all_preds.extend(pred.cpu().numpy())
                    all_targets.extend(target.cpu().numpy())
            
            return all_preds, all_targets
        
        def plot_confusion_matrix(self, y_true, y_pred, class_names):
            cm = confusion_matrix(y_true, y_pred)
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=class_names, yticklabels=class_names)
            plt.title('Confusion Matrix')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            return plt.gcf()
        
        def plot_training_history(self, trainer):
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
            
            # Loss plot
            ax1.plot(trainer.train_losses, label='Training Loss')
            ax1.plot(trainer.val_losses, label='Validation Loss')
            ax1.set_title('Model Loss')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.legend()
            ax1.grid(True)
            
            # Accuracy plot
            ax2.plot(trainer.train_accuracies, label='Training Accuracy')
            ax2.plot(trainer.val_accuracies, label='Validation Accuracy')
            ax2.set_title('Model Accuracy')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Accuracy (%)')
            ax2.legend()
            ax2.grid(True)
            
            plt.tight_layout()
            return fig

    class TransferLearning:
        """è¿ç§»å­¦ä¹ """
        def __init__(self, base_model_name='resnet50'):
            self.base_model_name = base_model_name
            self.available_models = {
                'resnet50': models.resnet50,
                'resnet101': models.resnet101,
                'densenet121': models.densenet121,
                'efficientnet_b0': models.efficientnet_b0,
                'vgg16': models.vgg16
            }
        
        def get_pretrained_model(self, num_classes, freeze_backbone=True):
            if self.base_model_name not in self.available_models:
                raise ValueError(f"Model {self.base_model_name} not supported")
            
            model_func = self.available_models[self.base_model_name]
            model = model_func(pretrained=True)
            
            # å†»ç»“éª¨å¹²ç½‘ç»œå‚æ•°
            if freeze_backbone:
                for param in model.parameters():
                    param.requires_grad = False
            
            # æ›¿æ¢åˆ†ç±»å¤´
            if hasattr(model, 'fc'):  # ResNet
                num_features = model.fc.in_features
                model.fc = nn.Linear(num_features, num_classes)
            elif hasattr(model, 'classifier'):  # DenseNet, VGG
                if isinstance(model.classifier, nn.Sequential):
                    num_features = model.classifier[-1].in_features
                    model.classifier[-1] = nn.Linear(num_features, num_classes)
                else:
                    num_features = model.classifier.in_features
                    model.classifier = nn.Linear(num_features, num_classes)
            
            return model

# å…¨å±€æ·±åº¦å­¦ä¹ ç³»ç»Ÿå®ä¾‹
@st.cache_resource
def get_model_trainer():
    return ModelTrainer

@st.cache_resource
def get_data_augmentation():
    return DataAugmentation()

@st.cache_resource
def get_transfer_learning():
    return TransferLearning()

# ---------------------------
# Caching helpers
# ---------------------------
@st.cache_resource(show_spinner=False)
def get_onnx_session_cached(model_path: str, providers):
    return ort.InferenceSession(model_path, providers=providers)

@st.cache_data(show_spinner=False)
def _resize_bgr_cached(image_bgr_bytes: bytes, w: int, h: int):
    arr = np.frombuffer(image_bgr_bytes, dtype=np.uint8)
    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if img is None:
        raise ValueError("æ— æ³•ä»ç¼“å­˜å­—èŠ‚è§£ç å›¾åƒ")
    return cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)

@st.cache_resource(show_spinner=False)
def get_rapidocr_cached():
    if RapidOCR is None:
        return None
    try:
        return RapidOCR()
    except Exception:
        return None

# ---------------------------
# Helpers: render inpainting UI
# ---------------------------
def render_inpainting_ui(img_rgb, mask_crack, mask_peel, mask_disc, mask_stain, mask_salt, mask_bio, default_open=True, key_suffix=""):
    st.markdown("### ğŸ§© å›¾åƒå¤åŸï¼ˆè¯•éªŒæ€§ Inpaintingï¼‰")
    with st.expander("å±•å¼€/æ”¶èµ·", expanded=default_open):
        
        def __len__(self):
            return len(self.images)
        
        def __getitem__(self, idx):
            image = self.images[idx]
            label = self.labels[idx]
            
            if self.transform:
                image = self.transform(image)
            
            return image, label

    class DefectClassifier(nn.Module):
        """ç—…å®³åˆ†ç±»å™¨"""
        def __init__(self, num_classes=6, pretrained=True):
            super(DefectClassifier, self).__init__()
            
            # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetä½œä¸ºéª¨å¹²ç½‘ç»œ
            self.backbone = torchvision.models.resnet50(pretrained=pretrained)
            num_features = self.backbone.fc.in_features
            
            # æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
        
        def forward(self, x):
            return self.backbone(x)

    class DataAugmentation:
        """æ•°æ®å¢å¼º"""
        def __init__(self):
            self.transform = A.Compose([
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.Rotate(limit=15, p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
                A.Blur(blur_limit=3, p=0.3),
                A.RandomCrop(height=224, width=224, p=0.8),
                A.Resize(height=224, width=224),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        
        def __call__(self, image):
            return self.transform(image=image)['image']
    # å…¨å±€æ·±åº¦å­¦ä¹ ç³»ç»Ÿå®ä¾‹
    @st.cache_resource
    def get_model_trainer():
        return ModelTrainer

    @st.cache_resource
    def get_data_augmentation():
        return DataAugmentation()

    @st.cache_resource
    def get_transfer_learning():
        return TransferLearning()
def render_inpainting_ui(img_rgb, mask_crack, mask_peel, mask_disc, mask_stain, mask_salt, mask_bio, default_open=True, key_suffix=""):
    st.markdown("### ğŸ§© å›¾åƒå¤åŸï¼ˆè¯•éªŒæ€§ Inpaintingï¼‰")
    with st.expander("å±•å¼€/æ”¶èµ·", expanded=default_open):
        sel_classes = st.multiselect(
            "é€‰æ‹©éœ€è¦å¤åŸçš„ç—…å®³ç±»åˆ«ï¼ˆå°†åŸºäºå…¶æ©è†œè¿›è¡Œä¿®è¡¥ï¼‰",
            ["è£‚ç¼","å‰¥è½","è¤ªè‰²","æ±¡æ¸/éœ‰æ–‘","ç›èš€/é£åŒ–","ç”Ÿç‰©é™„ç€"],
            default=["è£‚ç¼","å‰¥è½","æ±¡æ¸/éœ‰æ–‘"], key=f"sel_classes_{key_suffix}"
        )
        method = st.selectbox("ä¿®è¡¥ç®—æ³•", ["Telea", "Navier-Stokes"], index=0, key=f"method_{key_suffix}")
        radius = st.slider("ä¿®è¡¥åŠå¾„ï¼ˆåƒç´ ï¼‰", min_value=1, max_value=25, value=7, key=f"radius_{key_suffix}")
        go_restore = st.button("ç”Ÿæˆå¤åŸå›¾åƒ", key=f"restore_btn_{key_suffix}")
        if go_restore:
            class_to_mask = {
                "è£‚ç¼": mask_crack,
                "å‰¥è½": mask_peel,
                "è¤ªè‰²": mask_disc,
                "æ±¡æ¸/éœ‰æ–‘": mask_stain,
                "ç›èš€/é£åŒ–": mask_salt,
                "ç”Ÿç‰©é™„ç€": mask_bio,
            }
            union = np.zeros(mask_crack.shape, dtype=np.uint8)
            for c in sel_classes:
                m = class_to_mask.get(c)
                if m is not None:
                    union = cv2.bitwise_or(union, (m>0).astype(np.uint8)*255)
            num_labels, labels, stats, _ = cv2.connectedComponentsWithStats((union>0).astype(np.uint8), connectivity=8)
            filtered = np.zeros_like(union)
            for i in range(1, num_labels):
                area = int(stats[i, cv2.CC_STAT_AREA])
                if area >= 50:
                    filtered[labels==i] = 255
            flag = cv2.INPAINT_TELEA if method == "Telea" else cv2.INPAINT_NS
            src_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
            restored_bgr = cv2.inpaint(src_bgr, (filtered>0).astype(np.uint8), radius, flag)
            restored_rgb = cv2.cvtColor(restored_bgr, cv2.COLOR_BGR2RGB)
            st.image(restored_rgb, caption="å¤åŸç»“æœï¼ˆåŸºäºæ‰€é€‰æ©è†œï¼‰", width='stretch')
            _buf = BytesIO(); Image.fromarray(restored_rgb).save(_buf, format="PNG"); _buf.seek(0)
            st.download_button("ä¸‹è½½å¤åŸå›¾ï¼ˆPNGï¼‰", data=_buf.getvalue(), file_name="restored.png", mime="image/png")

# ---------------------------
# Helpers: color restoration utilities
# ---------------------------
def _to_bgr(img_rgb):
    return cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

def _to_rgb(img_bgr):
    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

def gray_world_white_balance(img_bgr):
    img = img_bgr.astype(np.float32)
    mean_b, mean_g, mean_r = [np.mean(img[:,:,c]) for c in range(3)]
    mean_gray = (mean_b + mean_g + mean_r) / 3.0
    gain_b = mean_gray / (mean_b + 1e-6)
    gain_g = mean_gray / (mean_g + 1e-6)
    gain_r = mean_gray / (mean_r + 1e-6)
    img[:,:,0] = np.clip(img[:,:,0] * gain_b, 0, 255)
    img[:,:,1] = np.clip(img[:,:,1] * gain_g, 0, 255)
    img[:,:,2] = np.clip(img[:,:,2] * gain_r, 0, 255)
    return img.astype(np.uint8)

def clahe_on_l_channel(img_bgr, clip_limit=2.0, tile_grid_size=8):
    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=float(clip_limit), tileGridSize=(int(tile_grid_size), int(tile_grid_size)))
    l2 = clahe.apply(l)
    lab2 = cv2.merge([l2, a, b])
    return cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)

def reinhard_color_transfer(src_bgr, ref_bgr):
    # Convert to LAB and match mean/std
    src_lab = cv2.cvtColor(src_bgr, cv2.COLOR_BGR2LAB).astype(np.float32)
    ref_lab = cv2.cvtColor(ref_bgr, cv2.COLOR_BGR2LAB).astype(np.float32)
    for c in range(3):
        s_mean, s_std = src_lab[:,:,c].mean(), src_lab[:,:,c].std() + 1e-6
        r_mean, r_std = ref_lab[:,:,c].mean(), ref_lab[:,:,c].std() + 1e-6
        src_lab[:,:,c] = (src_lab[:,:,c] - s_mean) * (r_std / s_std) + r_mean
    src_lab = np.clip(src_lab, 0, 255).astype(np.uint8)
    return cv2.cvtColor(src_lab, cv2.COLOR_LAB2BGR)

def render_color_restore_ui(img_rgb, default_open=False, key_suffix="color"):
    st.markdown("### ğŸ¨ è‰²å½©/è¤ªè‰²å¤åŸï¼ˆåŸºç¡€ç‰ˆï¼‰")
    with st.expander("å±•å¼€/æ”¶èµ·", expanded=default_open):
        col1, col2 = st.columns(2)
        with col1:
            use_wb = st.checkbox("ç°åº¦ä¸–ç•Œç™½å¹³è¡¡", value=True, key=f"wb_{key_suffix}")
            clahe_clip = st.slider("CLAHE å¯¹æ¯”åº¦ (clip)", 0.0, 4.0, 2.0, 0.1, key=f"clip_{key_suffix}")
            clahe_tile = st.slider("CLAHE ç½‘æ ¼", 4, 16, 8, 1, key=f"tile_{key_suffix}")
        with col2:
            ref_file = st.file_uploader("å‚è€ƒå›¾åƒï¼ˆå¯é€‰ï¼Œç”¨äºé£æ ¼/è‰²å½©è½¬ç§»ï¼‰", type=["jpg","jpeg","png"], key=f"ref_{key_suffix}")
            do_transfer = st.checkbox("å¯ç”¨å‚è€ƒè‰²å½©è½¬ç§»ï¼ˆReinhardï¼‰", value=False, key=f"tr_{key_suffix}")
        run_color = st.button("ç”Ÿæˆè‰²å½©å¤åŸå›¾", key=f"btn_{key_suffix}")
        if run_color:
            bgr = _to_bgr(img_rgb)
            out = bgr
            if use_wb:
                out = gray_world_white_balance(out)
            out = clahe_on_l_channel(out, clip_limit=clahe_clip, tile_grid_size=clahe_tile)
            if do_transfer and ref_file is not None:
                ref_bytes = np.asarray(bytearray(ref_file.read()), dtype=np.uint8)
                ref_bgr = cv2.imdecode(ref_bytes, cv2.IMREAD_COLOR)
                if ref_bgr is not None:
                    out = reinhard_color_transfer(out, ref_bgr)
            rgb = _to_rgb(out)
            st.image(rgb, caption="è‰²å½©å¤åŸç»“æœ", width='stretch')
            buf = BytesIO(); Image.fromarray(rgb).save(buf, format="PNG"); buf.seek(0)
            st.download_button("ä¸‹è½½å¤åŸå›¾ï¼ˆPNGï¼‰", data=buf.getvalue(), file_name="restored_color.png", mime="image/png")

# ---------------------------
# Utility helpers
# ---------------------------
def pil_from_cv2(cv2_img):
    """BGR or RGB? we expect RGB input"""
    if len(cv2_img.shape) == 3:
        return Image.fromarray(cv2_img)
    else:
        return Image.fromarray(cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB))

def encode_image_to_bytes(img_rgb):
    """PIL Image -> bytes"""
    buf = BytesIO()
    img_rgb.save(buf, format="PNG")
    buf.seek(0)
    return buf

def save_annotated_image_bytes(annotated_rgb):
    """Return bytesIO for embedding to reportlab or streamlit download"""
    pil = Image.fromarray(annotated_rgb)
    buf = BytesIO()
    pil.save(buf, format="PNG")
    buf.seek(0)
    return buf


def numpy_image_to_bytes(img_array, format="PNG"):
    """Convert an RGB numpy array to BytesIO for PDF embedding."""
    pil_img = Image.fromarray(img_array)
    buffer = BytesIO()
    pil_img.save(buffer, format=format)
    buffer.seek(0)
    return buffer


def simulate_progress_bar(step_labels, sleep_seconds=0.2):
    """Render a lightweight simulated progress bar for user feedback."""
    progress_bar = st.progress(0, text="å‡†å¤‡å°±ç»ªâ€¦")
    status_placeholder = st.empty()
    total = len(step_labels)
    for idx, label in enumerate(step_labels, start=1):
        status_placeholder.info(f"æ­£åœ¨æ‰§è¡Œï¼š{label}")
        progress_bar.progress(idx / total, text=label)
        time.sleep(sleep_seconds)
    status_placeholder.success("åˆ†ææµç¨‹æ¨¡æ‹Ÿå®Œæˆ âœ…")
    progress_bar.progress(1.0, text="å®Œæˆ")


def render_quick_progress_controls():
    """å±•ç¤ºå®æ—¶è¿›åº¦æ¨¡æ‹ŸæŒ‰é’®ã€‚"""
    st.subheader("è¿›åº¦åé¦ˆ")
    st.caption("å¿«é€Ÿäº†è§£å®Œæ•´åˆ†ææµç¨‹çš„æ‰§è¡Œé¡ºåºä¸çŠ¶æ€åé¦ˆã€‚")
    if st.button("â–¶ï¸ æ¼”ç¤ºåˆ†æè¿›åº¦", key="demo_progress"):
        simulate_progress_bar(
            ["å›¾åƒé¢„å¤„ç†", "æè´¨è¯†åˆ«", "ç—…å®³æ£€æµ‹", "ä¸¥é‡åº¦è¯„ä¼°", "æŠ¥å‘Šç”Ÿæˆ"],
            sleep_seconds=0.25
        )


def create_metrics_dataframe(category_counts, area_percentages):
    """æ„å»ºç—…å®³æ¦‚è§ˆæ•°æ®è¡¨ã€‚"""
    data = []
    for label, count in category_counts.items():
        pct = area_percentages.get(label, 0.0)
        data.append({"ç—…å®³ç±»å‹": label, "æ•°é‡": count, "é¢ç§¯å æ¯”(%)": round(pct, 3)})
    return pd.DataFrame(data)


def downscale_mask_for_heatmap(mask, size=32):
    """å°†äºŒå€¼æ©è†œç¼©å°ç”¨äºçƒ­åŠ›å›¾å±•ç¤ºã€‚"""
    if mask is None or mask.size == 0:
        return None
    try:
        reduced = cv2.resize(
            (mask > 0).astype(np.float32),
            (size, size),
            interpolation=cv2.INTER_AREA
        )
        return reduced
    except Exception:
        return None


def render_interactive_dashboard(category_counts, area_percentages, aggregated_mask):
    """å±•ç¤ºäº¤äº’å¼å¯è§†åŒ–ä»ªè¡¨æ¿ã€‚"""
    st.subheader("äº¤äº’å¼åˆ†æç»“æœ")
    dataframe = create_metrics_dataframe(category_counts, area_percentages)
    st.dataframe(dataframe, use_container_width=True)

    if px is None:
        st.info("ç¼ºå°‘ plotly ä¾èµ–ï¼Œæ— æ³•ç»˜åˆ¶äº¤äº’å¼å›¾è¡¨ã€‚è¯·è¿è¡Œ `pip install plotly` åé‡è¯•ã€‚")
        return

    fig_bar = px.bar(
        dataframe,
        x="ç—…å®³ç±»å‹",
        y="æ•°é‡",
        color="é¢ç§¯å æ¯”(%)",
        title="ç—…å®³æ•°é‡ä¸é¢ç§¯å æ¯”"
    )
    st.plotly_chart(fig_bar, use_container_width=True)

    trend_dates = pd.date_range(end=datetime.now(), periods=6, freq="M")
    trend_df = pd.DataFrame({
        "æ—¥æœŸ": trend_dates,
        "æ€»ä½“ä¸¥é‡åº¦": np.clip(
            np.linspace(0.6, 1.0, len(trend_dates)) * sum(area_percentages.values()),
            0,
            100
        ),
        "è£‚ç¼é¢ç§¯å æ¯”": np.linspace(
            0.5, 1.1, len(trend_dates)
        ) * area_percentages.get("è£‚ç¼", 0.1)
    })
    fig_trend = px.line(
        trend_df,
        x="æ—¥æœŸ",
        y=["æ€»ä½“ä¸¥é‡åº¦", "è£‚ç¼é¢ç§¯å æ¯”"],
        title="ç—…å®³è¶‹åŠ¿æ¨¡æ‹Ÿ"
    )
    st.plotly_chart(fig_trend, use_container_width=True)

    if aggregated_mask is not None:
        fig_heatmap = px.imshow(
            aggregated_mask,
            color_continuous_scale="YlOrRd",
            title="ç—…å®³ç©ºé—´åˆ†å¸ƒçƒ­åŠ›å›¾ï¼ˆç¤ºæ„ï¼‰"
        )
        st.plotly_chart(fig_heatmap, use_container_width=True)
    else:
        st.caption("æš‚æ— å¯ç”¨äºçƒ­åŠ›å›¾å±•ç¤ºçš„æ©è†œæ•°æ®ã€‚")


def init_project_state():
    """åˆå§‹åŒ–é¡¹ç›®ç®¡ç†çš„ä¼šè¯çŠ¶æ€ã€‚"""
    if "projects" not in st.session_state:
        st.session_state["projects"] = [
            {"name": "è«é«˜çªŸç¬¬45çªŸç›‘æµ‹", "status": "è¿›è¡Œä¸­", "last_update": "2024-01-15", "progress": 0.75},
            {"name": "äº‘å†ˆçŸ³çªŸå¹´åº¦è¯„ä¼°", "status": "å·²å®Œæˆ", "last_update": "2024-01-10", "progress": 1.0},
        ]
    if "show_new_project_form" not in st.session_state:
        st.session_state["show_new_project_form"] = False


def render_project_manager():
    """æ¸²æŸ“é¡¹ç›®ç®¡ç†é¢æ¿ã€‚"""
    init_project_state()
    st.subheader("é¡¹ç›®ä¸ä»»åŠ¡")
    for project in st.session_state["projects"]:
        label = f"{project['name']}ï½œ{project['status']}"
        with st.expander(label, expanded=False):
            col_a, col_b = st.columns([2, 1])
            with col_a:
                st.write(f"æœ€åæ›´æ–°ï¼š{project['last_update']}")
                st.progress(project.get("progress", 0.0))
            with col_b:
                if st.button("è®¾ä¸ºå½“å‰é¡¹ç›®", key=f"activate_{project['name']}"):
                    st.session_state["current_project"] = project["name"]
                    st.success(f"å·²æ¿€æ´»é¡¹ç›®ï¼š{project['name']}")

    if st.button("â• æ–°å»ºé¡¹ç›®", key="add_project"):
        st.session_state["show_new_project_form"] = True

    if st.session_state["show_new_project_form"]:
        with st.form("create_project_form"):
            name = st.text_input("é¡¹ç›®åç§°", "")
            status = st.selectbox("é¡¹ç›®çŠ¶æ€", ["è¿›è¡Œä¸­", "å·²å®Œæˆ", "å¾…å¯åŠ¨"])
            progress = st.slider("å½“å‰è¿›åº¦", 0, 100, 10) / 100.0
            submitted = st.form_submit_button("åˆ›å»º")
            if submitted:
                if name.strip():
                    st.session_state["projects"].append({
                        "name": name.strip(),
                        "status": status,
                        "last_update": datetime.now().strftime("%Y-%m-%d"),
                        "progress": progress,
                    })
                    st.success(f"é¡¹ç›®â€œ{name}â€åˆ›å»ºæˆåŠŸï¼")
                    st.session_state["show_new_project_form"] = False
                else:
                    st.warning("è¯·å¡«å†™é¡¹ç›®åç§°åå†æäº¤ã€‚")


class ProfessionalPDFReport:
    """ä¸“ä¸šPDFæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.styles = getSampleStyleSheet()
        self._setup_chinese_font()
        self._setup_custom_styles()

    def _setup_chinese_font(self):
        """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
        self.chinese_font = "Helvetica"
        font_candidates = [
            "C:/Windows/Fonts/msyh.ttc",
            "C:/Windows/Fonts/simhei.ttf",
            "/System/Library/Fonts/PingFang.ttc",
            "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",
        ]
        for font_path in font_candidates:
            if os.path.exists(font_path):
                try:
                    if font_path.lower().endswith(".ttc"):
                        pdfmetrics.registerFont(TTFont("ChineseFont", font_path, subfontIndex=0))
                    else:
                        pdfmetrics.registerFont(TTFont("ChineseFont", font_path))
                    self.chinese_font = "ChineseFont"
                    break
                except Exception:
                    continue

    def _setup_custom_styles(self):
        """è®¾ç½®è‡ªå®šä¹‰æ ·å¼"""
        title_style = ParagraphStyle(
            name="ChineseTitle",
            parent=self.styles["Title"],
            fontName=self.chinese_font,
            fontSize=18,
            spaceAfter=30,
            alignment=1,
            textColor=colors.HexColor("#2c3e50"),
        )

        heading1 = ParagraphStyle(
            name="ChineseHeading1",
            parent=self.styles["Heading1"],
            fontName=self.chinese_font,
            fontSize=14,
            spaceAfter=12,
            spaceBefore=20,
            textColor=colors.HexColor("#34495e"),
            leftIndent=0,
        )

        heading2 = ParagraphStyle(
            name="ChineseHeading2",
            parent=self.styles["Heading2"],
            fontName=self.chinese_font,
            fontSize=12,
            spaceAfter=8,
            textColor=colors.HexColor("#5d6d7e"),
        )

        normal = ParagraphStyle(
            name="ChineseNormal",
            parent=self.styles["Normal"],
            fontName=self.chinese_font,
            fontSize=10,
            spaceAfter=6,
            leading=14,
            textColor=colors.HexColor("#2c3e50"),
        )

        emphasis = ParagraphStyle(
            name="ChineseEmphasis",
            parent=self.styles["Normal"],
            fontName=self.chinese_font,
            fontSize=10,
            textColor=colors.HexColor("#e74c3c"),
        )

        table_style = ParagraphStyle(
            name="ChineseTable",
            parent=self.styles["Normal"],
            fontName=self.chinese_font,
            fontSize=9,
            alignment=0,
            leading=12,
        )

        for style in (title_style, heading1, heading2, normal, emphasis, table_style):
            self.styles.add(style)

    def create_cover_page(self, story, basic_info):
        """åˆ›å»ºå°é¢é¡µ"""
        cover_image = basic_info.get("cover_image")
        if cover_image:
            cover_img = RLImage(cover_image, width=6 * inch, height=3 * inch)
            cover_img.hAlign = "CENTER"
            story.append(cover_img)
            story.append(Spacer(1, 20))

        title = Paragraph("çŸ³çªŸå¯ºå£ç”»ç—…å®³åˆ†ææŠ¥å‘Š", self.styles["ChineseTitle"])
        story.append(title)
        story.append(Spacer(1, 30))

        cover_data = [
            ["é¡¹ç›®åç§°:", basic_info.get("project_name", "çŸ³çªŸå¯ºå£ç”»ç—…å®³åˆ†æ")],
            ["åˆ†æå¯¹è±¡:", basic_info.get("location", "æœªæŒ‡å®š")],
            ["åˆ†ææ—¶é—´:", basic_info.get("analysis_time", datetime.now().strftime("%Y-%m-%d %H:%M"))],
            ["æè´¨ç±»å‹:", basic_info.get("material", "æœªæŒ‡å®š")],
            ["ä¸¥é‡ç¨‹åº¦:", basic_info.get("severity", "å¾…è¯„ä¼°")],
            ["æŠ¥å‘Šç¼–å·:", basic_info.get("report_id", f"RP-{datetime.now().strftime('%Y%m%d%H%M')}")],
        ]

        cover_table = Table(cover_data, colWidths=[2 * inch, 4 * inch])
        cover_table.setStyle(
            TableStyle(
                [
                    ("FONT", (0, 0), (-1, -1), self.chinese_font, 10),
                    ("BACKGROUND", (0, 0), (0, -1), colors.HexColor("#ecf0f1")),
                    ("BACKGROUND", (1, 0), (1, -1), colors.white),
                    ("GRID", (0, 0), (-1, -1), 1, colors.HexColor("#bdc3c7")),
                    ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ("PADDING", (0, 0), (-1, -1), 6),
                ]
            )
        )

        story.append(cover_table)
        story.append(Spacer(1, 40))

        org_info = [
            ["ç”Ÿæˆå•ä½:", "ä¸Šæµ·äº¤é€šå¤§å­¦è®¾è®¡å­¦é™¢"],
            ["æ–‡ç‰©ä¿®å¤ç ”ç©¶å›¢é˜Ÿ:", "AI+æ–‡ç‰©ä¿æŠ¤å®éªŒå®¤"],
            ["è”ç³»æ–¹å¼:", basic_info.get("contact", "å¾…è¡¥å……")],
            ["æŠ¥å‘Šç‰ˆæœ¬:", basic_info.get("version", "1.0")],
        ]

        org_table = Table(org_info, colWidths=[2 * inch, 4 * inch])
        org_table.setStyle(
            TableStyle(
                [
                    ("FONT", (0, 0), (-1, -1), self.chinese_font, 9),
                    ("BACKGROUND", (0, 0), (0, -1), colors.HexColor("#34495e")),
                    ("TEXTCOLOR", (0, 0), (0, -1), colors.white),
                    ("BACKGROUND", (1, 0), (1, -1), colors.HexColor("#ecf0f1")),
                    ("GRID", (0, 0), (-1, -1), 1, colors.HexColor("#7f8c8d")),
                    ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ("PADDING", (0, 0), (-1, -1), 6),
                ]
            )
        )

        story.append(org_table)
        story.append(PageBreak())

    def create_summary_section(self, story, analysis_data):
        """åˆ›å»ºæ‘˜è¦éƒ¨åˆ†"""
        story.append(Paragraph("æ‰§è¡Œæ‘˜è¦", self.styles["ChineseHeading1"]))

        summary_data = [
            ["æ£€æµ‹æŒ‡æ ‡", "æ•°é‡/æ¯”ä¾‹", "ä¸¥é‡ç¨‹åº¦"],
            ["è£‚ç¼ç—…å®³", f"{analysis_data.get('crack_count', 0)}å¤„", analysis_data.get("crack_severity", "ä½")],
            ["å‰¥è½åŒºåŸŸ", f"{analysis_data.get('peel_area', 0):.1f}%", analysis_data.get("peel_severity", "ä½")],
            ["è¤ªè‰²ç¨‹åº¦", f"{analysis_data.get('discolor_level', 0):.1f}%", analysis_data.get("discolor_severity", "ä½")],
            ["æ•´ä½“å¥åº·åº¦", f"{analysis_data.get('overall_health', 0):.1f}%", analysis_data.get("overall_severity", "è‰¯å¥½")],
        ]

        summary_table = Table(summary_data, colWidths=[2 * inch, 1.5 * inch, 1.5 * inch])
        summary_table.setStyle(
            TableStyle(
                [
                    ("FONT", (0, 0), (-1, 0), self.chinese_font, 10),
                    ("FONT", (0, 1), (-1, -1), self.chinese_font, 9),
                    ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#34495e")),
                    ("TEXTCOLOR", (0, 0), (-1, 0), colors.white),
                    ("BACKGROUND", (0, 1), (-1, -1), colors.HexColor("#f8f9fa")),
                    ("GRID", (0, 0), (-1, -1), 1, colors.HexColor("#dee2e6")),
                    ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ("ALIGN", (1, 0), (2, -1), "CENTER"),
                    ("PADDING", (0, 0), (-1, -1), 8),
                ]
            )
        )

        story.append(summary_table)
        story.append(Spacer(1, 12))

        summary_text = (
            f"æœ¬æ¬¡åˆ†æå¯¹{analysis_data.get('location', 'ç›®æ ‡å£ç”»')}è¿›è¡Œäº†å…¨é¢çš„ç—…å®³æ£€æµ‹å’Œè¯„ä¼°ï¼Œ"
            f"å…±å‘ç°{analysis_data.get('total_defects', 0)}å¤„ä¸»è¦ç—…å®³ï¼Œæ•´ä½“ä¿å­˜çŠ¶å†µ"
            f"{analysis_data.get('preservation_status', 'è‰¯å¥½')}ï¼Œå»ºè®®"
            f"{analysis_data.get('recommendation_level', 'å®šæœŸç›‘æµ‹')}ã€‚"
        )
        story.append(Paragraph(summary_text, self.styles["ChineseNormal"]))

        result_lines = analysis_data.get("result_lines")
        if result_lines:
            story.append(Spacer(1, 8))
            for line in result_lines:
                story.append(Paragraph(f"â€¢ {line}", self.styles["ChineseNormal"]))

    def create_visualization_section(self, story, images_data):
        """åˆ›å»ºå¯è§†åŒ–éƒ¨åˆ†"""
        story.append(Paragraph("å¯è§†åŒ–åˆ†æ", self.styles["ChineseHeading1"]))

        if images_data.get("original_image"):
            story.append(Paragraph("åŸå§‹å›¾åƒ", self.styles["ChineseHeading2"]))
            orig_img = RLImage(images_data["original_image"], width=5 * inch, height=3 * inch)
            orig_img.hAlign = "CENTER"
            story.append(orig_img)
            story.append(Spacer(1, 12))

        if images_data.get("analysis_image"):
            story.append(Paragraph("ç—…å®³åˆ†æç»“æœ", self.styles["ChineseHeading2"]))
            analysis_img = RLImage(images_data["analysis_image"], width=5 * inch, height=3 * inch)
            analysis_img.hAlign = "CENTER"
            story.append(analysis_img)
            story.append(Spacer(1, 12))

        comparison_images = images_data.get("comparison_images")
        if comparison_images:
            story.append(Paragraph("å¯¹æ¯”åˆ†æ", self.styles["ChineseHeading2"]))
            rows = []
            for i in range(0, len(comparison_images), 2):
                row = []
                row.append(RLImage(comparison_images[i], width=2.5 * inch, height=2 * inch))
                if i + 1 < len(comparison_images):
                    row.append(RLImage(comparison_images[i + 1], width=2.5 * inch, height=2 * inch))
                else:
                    row.append("")
                rows.append(row)

            comp_table = Table(rows, colWidths=[2.7 * inch, 2.7 * inch])
            comp_table.setStyle(
                TableStyle(
                    [
                        ("ALIGN", (0, 0), (-1, -1), "CENTER"),
                        ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                        ("BOTTOMPADDING", (0, 0), (-1, -1), 6),
                    ]
                )
            )
            story.append(comp_table)

    def create_detailed_analysis_section(self, story, detailed_data):
        """åˆ›å»ºè¯¦ç»†åˆ†æéƒ¨åˆ†"""
        story.append(Paragraph("è¯¦ç»†ç—…å®³åˆ†æ", self.styles["ChineseHeading1"]))

        defect_data = [["ç—…å®³ç±»å‹", "æ•°é‡", "é¢ç§¯æ¯”ä¾‹", "å¹³å‡å°ºåº¦", "ä¸¥é‡ç¨‹åº¦"]]
        for defect in detailed_data.get("defects", []):
            defect_data.append(
                [
                    defect.get("type", ""),
                    str(defect.get("count", 0)),
                    f"{defect.get('area_ratio', 0):.2f}%",
                    f"{defect.get('avg_size', 0):.1f}px",
                    defect.get("severity", ""),
                ]
            )

        if len(defect_data) > 1:
            defect_table = Table(defect_data, colWidths=[1.5 * inch, 0.8 * inch, 1 * inch, 1 * inch, 1.2 * inch])
            defect_table.setStyle(
                TableStyle(
                    [
                        ("FONT", (0, 0), (-1, 0), self.chinese_font, 9),
                        ("FONT", (0, 1), (-1, -1), self.chinese_font, 8),
                        ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#2c3e50")),
                        ("TEXTCOLOR", (0, 0), (-1, 0), colors.white),
                        ("BACKGROUND", (0, 1), (-1, -1), colors.HexColor("#f8f9fa")),
                        ("GRID", (0, 0), (-1, -1), 1, colors.HexColor("#dee2e6")),
                        ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                        ("ALIGN", (1, 0), (3, -1), "CENTER"),
                        ("PADDING", (0, 0), (-1, -1), 6),
                        ("ROWBACKGROUNDS", (0, 1), (-1, -1), [colors.white, colors.HexColor("#f8f9fa")]),
                    ]
                )
            )
            story.append(defect_table)
            story.append(Spacer(1, 12))

        for defect in detailed_data.get("defects", []):
            description = defect.get("description")
            if description:
                story.append(
                    Paragraph(f"<b>{defect.get('type', '')}ï¼š</b>{description}", self.styles["ChineseNormal"])
                )

    def create_recommendations_section(self, story, recommendations):
        """åˆ›å»ºå»ºè®®éƒ¨åˆ†"""
        story.append(Paragraph("ä¿æŠ¤å»ºè®®", self.styles["ChineseHeading1"]))

        rec_data = [["ä¼˜å…ˆçº§", "å»ºè®®æªæ–½", "æ—¶é—´è¦æ±‚", "é¢„ä¼°æˆæœ¬"]]
        for rec in recommendations.get("actions", []):
            rec_data.append(
                [
                    f"P{rec.get('priority', 1)}",
                    rec.get("action", ""),
                    rec.get("timeline", ""),
                    rec.get("cost", ""),
                ]
            )

        if len(rec_data) > 1:
            rec_table = Table(rec_data, colWidths=[0.6 * inch, 3 * inch, 1.2 * inch, 1.2 * inch])
            rec_table.setStyle(
                TableStyle(
                    [
                        ("FONT", (0, 0), (-1, 0), self.chinese_font, 9),
                        ("FONT", (0, 1), (-1, -1), self.chinese_font, 8),
                        ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#27ae60")),
                        ("TEXTCOLOR", (0, 0), (-1, 0), colors.white),
                        ("BACKGROUND", (0, 1), (-1, -1), colors.HexColor("#f8f9fa")),
                        ("GRID", (0, 0), (-1, -1), 1, colors.HexColor("#dee2e6")),
                        ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                        ("PADDING", (0, 0), (-1, -1), 6),
                        ("ROWBACKGROUNDS", (0, 1), (-1, -1), [colors.white, colors.HexColor("#f8f9fa")]),
                    ]
                )
            )
            story.append(rec_table)

        story.append(Spacer(1, 12))

        long_term = recommendations.get("long_term", [])
        if long_term:
            story.append(Paragraph("é•¿æœŸä¿æŠ¤ç­–ç•¥", self.styles["ChineseHeading2"]))
            for strategy in long_term:
                story.append(Paragraph(f"â€¢ {strategy}", self.styles["ChineseNormal"]))

    def create_technical_details_section(self, story, tech_data):
        """åˆ›å»ºæŠ€æœ¯ç»†èŠ‚éƒ¨åˆ†"""
        story.append(Paragraph("æŠ€æœ¯å‚æ•°", self.styles["ChineseHeading1"]))

        tech_details = [
            ["åˆ†æç®—æ³•", tech_data.get("algorithm", "æ·±åº¦å­¦ä¹ +ä¼ ç»ŸCV")],
            ["å›¾åƒåˆ†è¾¨ç‡", tech_data.get("resolution", "æœªæŒ‡å®š")],
            ["æ£€æµ‹ç½®ä¿¡åº¦", f"{tech_data.get('confidence', 0):.1%}"],
            ["å¤„ç†æ—¶é—´", tech_data.get("processing_time", "æœªçŸ¥")],
            ["åˆ†æè½¯ä»¶", tech_data.get("software", "çŸ³çªŸå¯ºå£ç”»AIåˆ†æç³»ç»Ÿ")],
            ["æ•°æ®æ ¼å¼", tech_data.get("data_format", "RGBå›¾åƒ + äºŒè¿›åˆ¶æ©è†œ")],
        ]

        tech_table = Table(tech_details, colWidths=[1.5 * inch, 4.5 * inch])
        tech_table.setStyle(
            TableStyle(
                [
                    ("FONT", (0, 0), (-1, -1), self.chinese_font, 9),
                    ("BACKGROUND", (0, 0), (0, -1), colors.HexColor("#ecf0f1")),
                    ("BACKGROUND", (1, 0), (1, -1), colors.white),
                    ("GRID", (0, 0), (-1, -1), 1, colors.HexColor("#bdc3c7")),
                    ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ("PADDING", (0, 0), (-1, -1), 6),
                ]
            )
        )

        story.append(tech_table)

    def generate_comprehensive_report(self, output_buffer, report_data):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        doc = SimpleDocTemplate(
            output_buffer,
            pagesize=A4,
            rightMargin=72,
            leftMargin=72,
            topMargin=72,
            bottomMargin=72,
            title="çŸ³çªŸå¯ºå£ç”»ç—…å®³åˆ†ææŠ¥å‘Š",
        )

        story = []

        self.create_cover_page(story, report_data.get("basic_info", {}))
        self.create_summary_section(story, report_data.get("analysis_data", {}))
        story.append(Spacer(1, 20))
        self.create_visualization_section(story, report_data.get("images", {}))
        story.append(PageBreak())
        self.create_detailed_analysis_section(story, report_data.get("detailed_data", {}))
        story.append(Spacer(1, 20))
        self.create_recommendations_section(story, report_data.get("recommendations", {}))
        story.append(Spacer(1, 20))
        self.create_technical_details_section(story, report_data.get("technical_data", {}))

        story.append(Spacer(1, 30))
        footer_text = (
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | "
            "ä¸Šæµ·äº¤é€šå¤§å­¦è®¾è®¡å­¦é™¢æ–‡ç‰©ä¿®å¤å›¢é˜Ÿ | "
            "æœ¬æŠ¥å‘Šä»…ä¾›å‚è€ƒï¼Œå…·ä½“ä¿æŠ¤æªæ–½è¯·å’¨è¯¢ä¸“ä¸šæ–‡ç‰©ä¿®å¤äººå‘˜"
        )
        story.append(Paragraph(footer_text, self.styles["ChineseNormal"]))

        doc.build(story)

# ---------------------------
# Material-specific parameters
# ---------------------------
MATERIAL_OPTIONS = [  
    "æœªæŒ‡å®š",
    "å¤§è¶³çŸ³åˆ»ï¼ˆç ‚å²©ï¼‰",
    "äº‘å†ˆçŸ³çªŸï¼ˆç ‚å²©å¤¹æ³¥å²©ï¼‰",
    "æ•¦ç…Œè«é«˜çªŸï¼ˆç°æ³¥/é¢œæ–™å±‚ï¼‰",
    "æœ¨è´¨åŸºåº•ï¼ˆæœ¨æ¿ï¼‰"
]

MATERIAL_WEIGHTS = {
    # weights for severity scoring per category
    # categories: crack, peel, disc, stain, salt, bio
    "æœªæŒ‡å®š": {
        "crack": 1.0, "peel": 1.0, "disc": 1.0, "stain": 0.9, "salt": 1.0, "bio": 0.8
    },
    "å¤§è¶³çŸ³åˆ»ï¼ˆç ‚å²©ï¼‰": {
        "crack": 1.2, "peel": 1.3, "disc": 0.9, "stain": 0.9, "salt": 1.4, "bio": 0.8
    },
    "äº‘å†ˆçŸ³çªŸï¼ˆç ‚å²©å¤¹æ³¥å²©ï¼‰": {
        "crack": 1.3, "peel": 1.2, "disc": 1.0, "stain": 0.9, "salt": 1.3, "bio": 0.8
    },
    "æ•¦ç…Œè«é«˜çªŸï¼ˆç°æ³¥/é¢œæ–™å±‚ï¼‰": {
        "crack": 1.0, "peel": 1.1, "disc": 1.4, "stain": 1.1, "salt": 0.9, "bio": 0.8
    },
    "æœ¨è´¨åŸºåº•ï¼ˆæœ¨æ¿ï¼‰": {
        "crack": 1.1, "peel": 1.2, "disc": 1.0, "stain": 1.2, "salt": 0.6, "bio": 1.3
    }
}

MATERIAL_SUGGESTIONS = {
    "å¤§è¶³çŸ³åˆ»ï¼ˆç ‚å²©ï¼‰": [
        "ç ‚å²©è´¨åœ°ç–æ¾ï¼Œä¼˜å…ˆé˜²æ°´åŠ å›ºã€é˜²æ­¢ç›æä¸å´©è§£ã€‚",
        "é’ˆå¯¹å¤§é¢ç§¯å‰¥è½ï¼Œå»ºè®®è¿›è¡Œç‰©ç†åŠ å›ºä¸æ³¨æµ†ã€‚"
    ],
    "äº‘å†ˆçŸ³çªŸï¼ˆç ‚å²©å¤¹æ³¥å²©ï¼‰": [
        "æ§åˆ¶æ´çªŸæ¹¿åº¦ï¼Œæ£€æŸ¥å¤¹å±‚æ°´æ¸—ã€é‡‡å–æ³¨æµ†æˆ–æ”¯æ’‘åŠ å›ºã€‚",
        "æ³¨æ„è£‚ç¼æ³¨å…¥å’Œè£‚ç¼æ‰©å±•çš„ç›‘æµ‹ã€‚"
    ],
    "æ•¦ç…Œè«é«˜çªŸï¼ˆç°æ³¥/é¢œæ–™å±‚ï¼‰": [
        "é‡ç‚¹ä¿æŠ¤é¢œæ–™å±‚ï¼Œé¿å…ç›´æ¥è§¦æ‘¸ä¸æ¹¿çƒ­ç¯å¢ƒå˜åŠ¨ã€‚",
        "å¯¹äºèµ·ç”²ä¸é¢œæ–™è„±è½ï¼Œåº”é‡‡ç”¨å¯é€†æ€§ä¿®å¤ææ–™ä¼˜å…ˆã€‚"
    ],
    "æœ¨è´¨åŸºåº•ï¼ˆæœ¨æ¿ï¼‰": [
        "å…³æ³¨è™«è›€ã€éœ‰èŒä¸å«æ°´ç‡å˜åŒ–ï¼Œå¿…è¦æ—¶è¿›è¡Œé˜²è™«ä¸é™¤æ¹¿å¤„ç†ã€‚",
        "é¿å…å¼ºå…‰ç›´å°„ä¸å¤§å¹…æ¸©æ¹¿å˜åŒ–ï¼Œè¡¨å±‚å»ºè®®ä½¿ç”¨å¯é€†æ€§ä¿æŠ¤æ¶‚å±‚ã€‚"
    ]
}

# ç»†åŒ–ï¼šæŒ‰ç—…å®³ç±»å‹å’Œæ¯”ä¾‹ç”Ÿæˆå»ºè®®
def build_recommendations(material, pct_map, overall_severity):
    recs = []
    m = material
    cp = pct_map.get('crack', 0.0)
    pp = pct_map.get('peel', 0.0)
    dp = pct_map.get('disc', 0.0)
    sp = pct_map.get('stain', 0.0)
    sap = pct_map.get('salt', 0.0)
    bp = pct_map.get('bio', 0.0)

    # è£‚ç¼
    if cp > 0.1:
        if cp < 1:
            recs.append("è£‚ç¼è½»åº¦ï¼šå»ºè®®è£‚ç¼ç›‘æµ‹ä¸è®°å½•ï¼Œé¿å…éœ‡åŠ¨ä¸å¹²æ¹¿å˜åŠ¨ï¼›å¿…è¦æ—¶è¡¨é¢åŠ å›ºã€‚")
        elif cp < 5:
            recs.append("è£‚ç¼ä¸­åº¦ï¼šè¿›è¡Œè£‚ç¼èµ°å‘/å®½åº¦æµ‹é‡ä¸å®šæœŸå¤æµ‹ï¼›å¯é‡‡ç”¨å¾®æ³¨æµ†æˆ–ä½é»åº¦åŠ å›ºæ ‘è„‚ï¼ˆå¯é€†/ä½æŒ¥å‘ï¼‰è¿›è¡Œå¡«å……ä¸åŠ å›ºã€‚")
        else:
            recs.append("è£‚ç¼é‡åº¦ï¼šä¼˜å…ˆå®æ–½ç»“æ„åŠ å›ºï¼ˆæ”¯æ’‘/é”šå›º/æ³¨æµ†ï¼‰ï¼Œå¹¶æŸ¥æ˜è‡´å› ï¼ˆæ¸—æ°´ã€æ¸©å˜ã€åº”åŠ›ï¼‰ï¼ŒåŒæ­¥å¼€å±•é•¿æœŸç›‘æµ‹ã€‚")
        if "æœ¨è´¨" in m:
            recs.append("æœ¨è´¨æ³¨æ„ï¼šä¼˜å…ˆæ§åˆ¶å«æ°´ç‡ä¸æ¸©æ¹¿ç¨³å®šï¼Œè£‚ç¼éƒ¨ä½é¿å…çƒ­èƒ€å†·ç¼©åå¤ï¼›åŠ å›ºææ–™éœ€å…¼å®¹æœ¨æçº¤ç»´ã€‚")

    # å‰¥è½/èµ·ç”²
    if pp > 0.1:
        if pp < 1:
            recs.append("å‰¥è½è½»åº¦ï¼šå°é¢ç§¯èµ·ç”²å¯å…ˆåšè¾¹ç¼˜ç‚¹å›ºä¸å±€éƒ¨å›è´´ï¼Œç°åœºè§‚å¯Ÿå…¶å‘å±•è¶‹åŠ¿ã€‚")
        elif pp < 5:
            recs.append("å‰¥è½ä¸­åº¦ï¼šå¯¹ç©ºé¼“/èµ·ç”²åŒºåŸŸè¿›è¡Œæ³¨æµ†å›è´´ï¼Œè¾¹ç•Œå¤„é‡‡ç”¨é€æ®µåŠ å›ºï¼›ä½œä¸šå‰è¿›è¡Œææ€§ä¸ç²˜ç»“è¯•éªŒã€‚")
        else:
            recs.append("å‰¥è½é‡åº¦ï¼šå¤§é¢ç§¯é¢å±‚ä¸ç¨³ï¼Œéœ€åˆ†åŒºåˆ†æ­¥å›è´´ä¸ç½‘æ ¼åŒ–ç®¡ç†ï¼Œè¿‡ç¨‹ä¸­ä¿æŒç¯å¢ƒç¨³å®šå¹¶åšå¥½æ”¯æ’‘ä¸é˜²å è½é˜²æŠ¤ã€‚")
        if "ç ‚å²©" in m:
            recs.append("ç ‚å²©æ³¨æ„ï¼šå…ˆåšåŸºä½“å«ç›/å«æ°´è¯„ä¼°ï¼Œå¿…è¦æ—¶å…ˆæœŸè„±ç›ä¸å¹²ç‡¥åå†è¡Œå›è´´åŠ å›ºã€‚")

    # è¤ªè‰²/ç²‰åŒ–
    if dp > 0.1:
        if dp < 1:
            recs.append("è¤ªè‰²è½»åº¦ï¼šåŠ å¼ºå…‰ç…§ä¸æ¸©æ¹¿ç®¡ç†ï¼Œé¿å…è§¦æ‘¸ä¸é£æ²™ç£¨èš€ï¼›å»ºç«‹é«˜ä¿çœŸå½±åƒæ¡£æ¡ˆã€‚")
        elif dp < 5:
            recs.append("è¤ªè‰²ä¸­åº¦ï¼šè¿›è¡Œé¢œæ–™å±‚ç¨³å›ºæ€§æµ‹è¯•ï¼Œé€‰æ‹©ä½å…‰æ³½ã€å¯é€†çš„è¡¨é¢ç¨³è‰²/å›ºè‰²å¤„ç†ï¼›é™å®šå‚è§‚è·ç¦»ä¸æ—¶é—´ã€‚")
        else:
            recs.append("è¤ªè‰²é‡åº¦ï¼šç»„ç»‡ææ–™å­¦è¯„ä¼°ï¼ˆé¢œæ–™çŸ¿ç‰©ä¸é»ç»“ç›¸ï¼‰ï¼Œé‡‡ç”¨æœ€å°å¹²é¢„çš„å¯é€†ç¨³è‰²ä½“ç³»å¹¶å»ºç«‹é•¿æœŸå…‰ç…§é˜ˆå€¼ç®¡ç†ã€‚")
        if "ç°æ³¥/é¢œæ–™" in m:
            recs.append("ç°æ³¥/é¢œæ–™å±‚æ³¨æ„ï¼šä¸¥æ§ç´«å¤–ä¸æŒ¥å‘æ€§æ±¡æŸ“ç‰©ï¼Œæ“ä½œä½¿ç”¨ä¸­æ€§pHæ¸…æ´ä¸ä¿æŠ¤ä½“ç³»ï¼Œé¿å…æ·±åº¦æ¸—å…¥å‹ææ–™ã€‚")

    # æ±¡æ¸/éœ‰æ–‘
    if sp > 0.1:
        if sp < 1:
            recs.append("æ±¡æ¸è½»åº¦ï¼šé‡‡ç”¨å¹²å¼/ä½æ¹¿æ¸…æ´ï¼ˆè½¯åˆ·/å¾®å¸ï¼‰å»é™¤è¡¨é¢å°˜å¢ï¼Œå…ˆåšå°æ ·è¯•éªŒã€‚")
        elif sp < 5:
            recs.append("æ±¡æ¸ä¸­åº¦ï¼šå±€éƒ¨é…åˆå‡èƒ¶æ¸…æ´ä¸æ§æ¹¿å¤„ç†ï¼Œæ¸…æ´ååšå†æ±¡æŸ“é˜²æŠ¤ã€‚")
        else:
            recs.append("æ±¡æ¸é‡åº¦ï¼šåˆ¶å®šåˆ†åŒºæ¸…æ´æ–¹æ¡ˆï¼Œé…åˆç¯å¢ƒæ²»ç†ï¼ˆè¿‡æ»¤/å¯†å°/äººæµæ§åˆ¶ï¼‰å¹¶è¯„ä¼°é¢œæ–™å±‚ç¨³å®šæ€§ã€‚")

    # ç›èš€/é£åŒ–
    if sap > 0.1:
        if sap < 1:
            recs.append("ç›èš€è½»åº¦ï¼šç›‘æµ‹ç›èŠ±ä¸ç™½åŒ–ï¼Œæ§åˆ¶æ°´æºä¸æ¹¿åº¦æ³¢åŠ¨ï¼›é¿å…ç›´æ¥æ°´æ´—é€ æˆç›è¿ç§»ã€‚")
        elif sap < 5:
            recs.append("ç›èš€ä¸­åº¦ï¼šå®æ–½æ¸©å’Œè„±ç›ï¼ˆçº¸æµ†/å‡èƒ¶ï¼‰ä¸æ°”å€™è°ƒæ§ï¼Œéšåè¿›è¡ŒåŸºä½“åŠ å›ºï¼›å¿…è¦æ—¶è¡¨é¢é˜²ç›å±éšœã€‚")
        else:
            recs.append("ç›èš€é‡åº¦ï¼šå…ˆæœŸç³»ç»Ÿè„±ç›ä¸å¹²ç‡¥ï¼Œå†åˆ†é˜¶æ®µç»“æ„ä¸è¡¨å±‚åŠ å›ºï¼›å»ºç«‹é•¿æœŸæ¸—æ°´/å«ç›ç›‘æµ‹ä½“ç³»ã€‚")
        if "æœ¨è´¨" in m:
            recs.append("æœ¨è´¨æ³¨æ„ï¼šç›èš€é€šå¸¸æ¬¡è¦ï¼Œé‡ç‚¹æ”¾åœ¨é˜²éœ‰ä¸å«æ°´ç‡æ§åˆ¶ï¼Œä¸å®œé‡‡ç”¨é«˜å«æ°´å¤„ç†ã€‚")

    # ç”Ÿç‰©é™„ç€
    if bp > 0.1:
        if bp < 1:
            recs.append("ç”Ÿç‰©è½»åº¦ï¼šå¢å¼ºé€šé£ä¸å¹²ç‡¥ï¼Œæ¶ˆé™¤ç§¯å°˜ä¸è¥å…»æºï¼Œç‰©ç†æ€§å»é™¤ä¸ºä¸»ã€‚")
        elif bp < 5:
            recs.append("ç”Ÿç‰©ä¸­åº¦ï¼šå°èŒƒå›´ä½¿ç”¨ä½æ¯’å¯é€†æ€§ç”Ÿç‰©æŠ‘åˆ¶å‰‚ï¼ˆå…ˆè¯•éªŒå†ä½¿ç”¨ï¼‰ï¼Œå¹¶æŒç»­æ§æ¹¿æ§å…‰ã€‚")
        else:
            recs.append("ç”Ÿç‰©é‡åº¦ï¼šåˆ¶å®šç»¼åˆæ²»ç†ï¼ˆæ§æ¹¿ã€æ§å…‰ã€å®šæœŸç»´æŠ¤ä¸è¿‡æ»¤ï¼‰ï¼Œå¿…è¦æ—¶åˆ†æ‰¹æ¬¡åŒ–å­¦æŠ‘åˆ¶å¹¶è¯„ä¼°å¯¹é¢œæ–™å±‚å½±å“ã€‚")
        if "æœ¨è´¨" in m:
            recs.append("æœ¨è´¨æ³¨æ„ï¼šä¼˜å…ˆé˜²éœ‰é˜²è™«ï¼Œè€ƒè™‘ç†è’¸/å±€éƒ¨æŠ—èŒä¸é˜²è™«å¤„ç½®ï¼Œå¹¶ä¸¥æ§å«æ°´ç‡ã€‚")

    # æ€»ä½“ç­–ç•¥
    if overall_severity < 5:
        recs.append("æ€»ä½“ï¼šé—®é¢˜è½»å¾®ï¼Œçº³å…¥å¸¸è§„å·¡æ£€ä¸å½±åƒæ¡£æ¡ˆç®¡ç†ï¼ŒåŠå¹´/ä¸€å¹´å¤æŸ¥ã€‚")
    elif overall_severity < 20:
        recs.append("æ€»ä½“ï¼šä¸­åº¦ç—…å®³ï¼Œå»ºè®®åˆ¶å®šåˆ†åŒºæ²»ç†è®¡åˆ’ä¸ä¼˜å…ˆçº§ï¼Œå…ˆåšæ ·åŒºè¯•éªŒåå†å…¨é¢å±•å¼€ã€‚")
    else:
        recs.append("æ€»ä½“ï¼šé‡åº¦ç—…å®³ï¼Œå°½å¿«ç»„ç»‡è·¨ä¸“ä¸šå›¢é˜Ÿï¼ˆç»“æ„ã€ææ–™ã€ç¯å¢ƒï¼‰è”åˆè¯„ä¼°ä¸å¤„ç½®ï¼Œè®¾ç½®é•¿æœŸç›‘æµ‹ã€‚")

    # é™„ï¼šæè´¨ä¸“ç”¨æç¤º
    recs += MATERIAL_SUGGESTIONS.get(m, [])
    return recs

# ---------------------------
# Image preprocessing & detection functions (classical CV baseline)
# ---------------------------

def preprocess_image(image_bgr, target_max_dim=1600):
    """Resize while keeping aspect ratio for reasonable processing time."""
    h, w = image_bgr.shape[:2]
    scale = 1.0
    max_dim = max(h, w)
    if max_dim > target_max_dim:
        scale = target_max_dim / max_dim
        image_bgr = cv2.resize(image_bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)
    return image_bgr, scale

def detect_cracks(gray):
    """Detect fine elongated structures (baseline): morphological thinning + contour filtering."""
    # enhance contrast
    gray_eq = cv2.equalizeHist(gray)
    # use Scharr or Sobel to get strong gradients
    grad_x = cv2.Sobel(gray_eq, cv2.CV_16S, 1, 0, ksize=3)
    grad_y = cv2.Sobel(gray_eq, cv2.CV_16S, 0, 1, ksize=3)
    grad = cv2.convertScaleAbs(cv2.addWeighted(cv2.convertScaleAbs(grad_x), 0.5, cv2.convertScaleAbs(grad_y), 0.5, 0))
    # binary threshold + morphological closing to join thin lines
    _, th = cv2.threshold(grad, 30, 255, cv2.THRESH_BINARY)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel, iterations=1)
    # find contours and filter elongated
    cnts = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = cnts[0] if len(cnts) == 2 else cnts[1]
    boxes = []
    mask = np.zeros_like(th)
    for c in contours:
        x,y,w,h = cv2.boundingRect(c)
        area = w*h
        if area < 80: 
            continue
        # elongated or thin filter
        if (w > 4*h) or (h > 4*w) or (area < 200 and max(w,h) > 40):
            boxes.append((x,y,w,h))
            cv2.drawContours(mask, [c], -1, 255, -1)
    return boxes, mask

def detect_peeling(hsv):
    """Low saturation patches (å‰¥è½/ç°ç™½æ–‘å—)"""
    h,s,v = cv2.split(hsv)
    # threshold low saturation but not pure dark
    low_sat = cv2.inRange(hsv, (0,0,40), (180,70,255))
    # remove tiny speckles
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))
    low_sat = cv2.morphologyEx(low_sat, cv2.MORPH_OPEN, kernel, iterations=1)
    cnts = cv2.findContours(low_sat, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = cnts[0] if len(cnts) == 2 else cnts[1]
    boxes = []
    mask = np.zeros_like(low_sat)
    for c in contours:
        x,y,w,h = cv2.boundingRect(c)
        if w*h < 400: 
            continue
        boxes.append((x,y,w,h))
        cv2.drawContours(mask, [c], -1, 255, -1)
    return boxes, mask

def detect_discoloration(hsv):
    """Overly bright or faded regions: high V with low to mid saturation"""
    lower = np.array([0,0,180])
    upper = np.array([180,90,255])
    light_mask = cv2.inRange(hsv, lower, upper)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))
    light_mask = cv2.morphologyEx(light_mask, cv2.MORPH_OPEN, kernel, iterations=1)
    cnts = cv2.findContours(light_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = cnts[0] if len(cnts) == 2 else cnts[1]
    boxes = []
    mask = np.zeros_like(light_mask)
    for c in contours:
        x,y,w,h = cv2.boundingRect(c)
        if w*h < 300: 
            continue
        boxes.append((x,y,w,h))
        cv2.drawContours(mask, [c], -1, 255, -1)
    return boxes, mask

# ---------------------------
# Material classification (heuristic + optional ONNX)
# ---------------------------
def classify_material_heuristic(image_bgr):
    """Return (material_name, confidence_0_1, details_dict) based on simple cues.
    Heuristic cues:
    - Brown/orange ratio (wood tendency)
    - Orientation coherence (wood grain)
    - High-bright low-sat salts (stone tendency)
    """
    img_small = cv2.resize(image_bgr, (min(640, image_bgr.shape[1]), min(640, image_bgr.shape[0])), interpolation=cv2.INTER_AREA)
    hsv = cv2.cvtColor(img_small, cv2.COLOR_BGR2HSV)
    # brown-ish mask (wood): hue 10-30, moderate saturation/value
    brown = cv2.inRange(hsv, (10, 40, 40), (30, 255, 220))
    brown_ratio = float(np.mean(brown > 0))

    # salt-like (stone efflorescence): very bright low S
    salt_like = cv2.inRange(hsv, (0, 0, 220), (180, 40, 255))
    salt_ratio = float(np.mean(salt_like > 0))

    # orientation coherence via gradients
    gray = cv2.cvtColor(img_small, cv2.COLOR_BGR2GRAY)
    gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
    gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)
    mag, ang = cv2.cartToPolar(gx, gy, angleInDegrees=True)
    # compute histogram concentration around main orientation
    ang_mask = mag > (np.percentile(mag, 75))
    main_orient = np.median(ang[ang_mask]) if np.any(ang_mask) else 0.0
    concentr = float(np.mean(np.abs(((ang - main_orient + 90) % 180) - 90) < 10))

    # crude decision
    wood_score = 0.55 * brown_ratio + 0.35 * concentr + 0.10 * (1.0 - salt_ratio)
    stone_score = 0.60 * (1.0 - brown_ratio) + 0.25 * (1.0 - concentr) + 0.15 * salt_ratio

    if wood_score > stone_score:
        name = "æœ¨è´¨åŸºåº•ï¼ˆæœ¨æ¿ï¼‰"
        conf = min(1.0, max(0.0, (wood_score - stone_score) * 2.0))
    else:
        # choose between known stone presets by default to "æœªæŒ‡å®š" or closest
        name = "å¤§è¶³çŸ³åˆ»ï¼ˆç ‚å²©ï¼‰"
        conf = min(1.0, max(0.0, (stone_score - wood_score) * 2.0))

    details = {
        'brown_ratio': round(brown_ratio, 4),
        'salt_ratio': round(salt_ratio, 4),
        'orientation_concentration': round(concentr, 4)
    }
    return name, conf, details

def run_material_model(image_bgr, model_path, providers=None, class_names=None):
    if ort is None:
        raise RuntimeError("æœªå®‰è£… onnxruntimeï¼Œè¯·å…ˆå®‰è£…ï¼špip install onnxruntime")
    session = ort.InferenceSession(model_path, providers=providers or ["CPUExecutionProvider"])
    # preprocess to square 224
    target = 224
    img = cv2.resize(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB), (target, target), interpolation=cv2.INTER_AREA)
    inp = img.astype(np.float32) / 255.0
    inp = np.transpose(inp, (2,0,1))[None, ...]
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    out = session.run([output_name], {input_name: inp})[0]
    # softmax
    if out.ndim == 2 and out.shape[0] == 1:
        logits = out[0]
    elif out.ndim == 1:
        logits = out
    else:
        raise ValueError("æè´¨æ¨¡å‹è¾“å‡ºå½¢çŠ¶ä¸æ”¯æŒï¼š" + str(out.shape))
    exps = np.exp(logits - np.max(logits))
    probs = exps / np.sum(exps)
    if class_names is None:
        class_names = [
            "æœªæŒ‡å®š",
            "å¤§è¶³çŸ³åˆ»ï¼ˆç ‚å²©ï¼‰",
            "äº‘å†ˆçŸ³çªŸï¼ˆç ‚å²©å¤¹æ³¥å²©ï¼‰",
            "æ•¦ç…Œè«é«˜çªŸï¼ˆç°æ³¥/é¢œæ–™å±‚ï¼‰",
            "æœ¨è´¨åŸºåº•ï¼ˆæœ¨æ¿ï¼‰"
        ]
    idx = int(np.argmax(probs))
    return class_names[idx], float(np.max(probs)), dict(zip(class_names[:len(probs)], [float(p) for p in probs]))

# é¢å¤–ç±»åˆ«ï¼ˆæ±¡æ¸/éœ‰æ–‘ã€ç›èš€/é£åŒ–ã€ç”Ÿç‰©é™„ç€ï¼‰
def detect_stain_mold(hsv):
    """Dark colored spots or stains: low V with moderate-high S"""
    lower = np.array([0, 40, 0])
    upper = np.array([180, 255, 90])
    mask = cv2.inRange(hsv, lower, upper)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = cnts[0] if len(cnts) == 2 else cnts[1]
    boxes = []
    mask_out = np.zeros_like(mask)
    for c in contours:
        x,y,w,h = cv2.boundingRect(c)
        if w*h < 300:
            continue
        boxes.append((x,y,w,h))
        cv2.drawContours(mask_out, [c], -1, 255, -1)
    return boxes, mask_out

def detect_salt_weathering(hsv):
    """Efflorescence/whitish salt: very high V, very low S"""
    lower = np.array([0, 0, 200])
    upper = np.array([180, 35, 255])
    mask = cv2.inRange(hsv, lower, upper)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = cnts[0] if len(cnts) == 2 else cnts[1]
    boxes = []
    mask_out = np.zeros_like(mask)
    for c in contours:
        x,y,w,h = cv2.boundingRect(c)
        if w*h < 400:
            continue
        boxes.append((x,y,w,h))
        cv2.drawContours(mask_out, [c], -1, 255, -1)
    return boxes, mask_out

def detect_bio_growth(hsv):
    """Biological growth (moss/algae): greenish hue, high S"""
    lower = np.array([35, 60, 40])
    upper = np.array([85, 255, 255])
    mask = cv2.inRange(hsv, lower, upper)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = cnts[0] if len(cnts) == 2 else cnts[1]
    boxes = []
    mask_out = np.zeros_like(mask)
    for c in contours:
        x,y,w,h = cv2.boundingRect(c)
        if w*h < 300:
            continue
        boxes.append((x,y,w,h))
        cv2.drawContours(mask_out, [c], -1, 255, -1)
    return boxes, mask_out

# ---------------------------
# è®­ç»ƒå¥½çš„åˆ†ç±»æ¨¡å‹
# ---------------------------
@st.cache_resource
def load_trained_classifier():
    """åŠ è½½è®­ç»ƒå¥½çš„å£ç”»ç—…å®³åˆ†ç±»æ¨¡å‹"""
    try:
        import pickle
        model_path = "simple_models/mural_classifier.pkl"
        if os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            return model
        else:
            st.warning("è®­ç»ƒå¥½çš„åˆ†ç±»æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬")
            return None
    except Exception as e:
        st.error(f"åŠ è½½åˆ†ç±»æ¨¡å‹å¤±è´¥: {e}")
        return None

def extract_simple_features(image):
    """æå–ç®€å•ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    img_array = np.array(image)
    
    features = []
    
    # RGBé€šé“ç»Ÿè®¡
    for channel in range(3):
        channel_data = img_array[:, :, channel].flatten()
        features.extend([
            np.mean(channel_data),
            np.std(channel_data)
        ])
    
    # ç°åº¦ç»Ÿè®¡
    gray = np.mean(img_array, axis=2)
    features.extend([
        np.mean(gray),
        np.std(gray)
    ])
    
    return features

def predict_mural_disease(image_rgb, model):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹å£ç”»ç—…å®³"""
    if model is None:
        return None
    
    try:
        # è½¬æ¢ä¸ºPILå›¾åƒ
        if isinstance(image_rgb, np.ndarray):
            image = Image.fromarray(image_rgb)
        else:
            image = image_rgb
        
        # æå–ç‰¹å¾
        features = extract_simple_features(image)
        features = np.array(features).reshape(1, -1)
        
        # é¢„æµ‹
        prediction = model.predict(features)[0]
        probabilities = model.predict_proba(features)[0]
        
        # ç±»åˆ«åç§°
        class_names = ["crack", "peel", "disc", "clean"]
        
        return {
            'predicted_class': class_names[prediction],
            'confidence': probabilities[prediction],
            'all_probabilities': dict(zip(class_names, probabilities))
        }
        
    except Exception as e:
        st.error(f"é¢„æµ‹å¤±è´¥: {e}")
        return None

# ---------------------------
# Deep model inference (ONNX)
# ---------------------------
def run_segmentation_model(image_bgr, model_path, input_size=512, class_ids=None, providers=None):
    """Run ONNX segmentation model and return masks dict.

    Assumptions:
    - Input: RGB float32 0-1, NCHW (1,C,H,W). We will transpose accordingly.
    - Output: either NCHW (1,C,H,W) or NHWC (1,H,W,C) or HW (single channel) or HxW (after squeeze).
    - Class mapping provided via class_ids: {'bg':0,'crack':1,'peel':2,'disc':3}.
    """
    if ort is None:
        raise RuntimeError("æœªå®‰è£… onnxruntimeï¼Œè¯·å…ˆå®‰è£…ï¼špip install onnxruntime")

    if class_ids is None:
        class_ids = {'bg': 0, 'crack': 1, 'peel': 2, 'disc': 3, 'stain': 4, 'salt': 5, 'bio': 6}

    # Prepare session (cached)
    session = get_onnx_session_cached(model_path, providers=providers or ["CPUExecutionProvider"])

    # Preprocess: BGR->RGB, resize square, normalize to [0,1]
    h0, w0 = image_bgr.shape[:2]
    target = int(input_size)
    img_resized = cv2.resize(image_bgr, (target, target), interpolation=cv2.INTER_AREA)
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
    inp = img_rgb.astype(np.float32) / 255.0
    # to NCHW
    inp = np.transpose(inp, (2,0,1))[None, ...]  # (1,3,H,W)

    # IO names
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    out = session.run([output_name], {input_name: inp})[0]
    # Postprocess to class map HxW
    class_map = None
    if out.ndim == 4:
        # (N,C,H,W) or (N,H,W,C)
        if out.shape[1] <= 6 and out.shape[0] == 1:  # likely NCHW classes small
            class_map = np.argmax(out[0], axis=0)
        elif out.shape[-1] <= 6 and out.shape[0] == 1:  # NHWC
            class_map = np.argmax(out[0], axis=-1)
        else:
            # fallback: take channel-argmax in the dimension that matches classes<=20
            if out.shape[1] < out.shape[-1]:
                class_map = np.argmax(out[0], axis=0)
            else:
                class_map = np.argmax(out[0], axis=-1)
    elif out.ndim == 3:
        # (C,H,W) or (H,W,C)
        if out.shape[0] <= 6:
            class_map = np.argmax(out, axis=0)
        elif out.shape[-1] <= 6:
            class_map = np.argmax(out, axis=-1)
        else:
            raise ValueError("æ— æ³•è§£ææ¨¡å‹è¾“å‡ºå½¢çŠ¶ï¼š" + str(out.shape))
    elif out.ndim == 2:
        # binary logits/mask -> treat >0.5 as class 'crack' by default
        class_map = (out > 0.5).astype(np.uint8) * class_ids.get('crack', 1)
    else:
        raise ValueError("æœªæ”¯æŒçš„æ¨¡å‹è¾“å‡ºç»´åº¦ï¼š" + str(out.shape))

    # Resize back to original size
    class_map = cv2.resize(class_map.astype(np.int32), (w0, h0), interpolation=cv2.INTER_NEAREST)

    crack_id = int(class_ids.get('crack', 1))
    peel_id = int(class_ids.get('peel', 2))
    disc_id = int(class_ids.get('disc', 3))
    stain_id = class_ids.get('stain', None)
    salt_id = class_ids.get('salt', None)
    bio_id = class_ids.get('bio', None)

    masks = {
        'crack': (class_map == crack_id).astype(np.uint8) * 255,
        'peel': (class_map == peel_id).astype(np.uint8) * 255,
        'disc': (class_map == disc_id).astype(np.uint8) * 255
    }
    if stain_id is not None:
        masks['stain'] = (class_map == int(stain_id)).astype(np.uint8) * 255
    if salt_id is not None:
        masks['salt'] = (class_map == int(salt_id)).astype(np.uint8) * 255
    if bio_id is not None:
        masks['bio'] = (class_map == int(bio_id)).astype(np.uint8) * 255
    return masks

# ---------------------------
# UI and main logic
# ---------------------------
# ä¸»æ ‡é¢˜å·²åœ¨create_main_header()ä¸­å®šä¹‰ï¼Œæ­¤å¤„ä¸å†é‡å¤

# Sidebar controls
with st.sidebar.expander("ğŸ“‚ é¡¹ç›®è°ƒåº¦ä¸­å¿ƒ", expanded=False):
    render_project_manager()

st.sidebar.markdown("### é…ç½®ä¸æè´¨é€‰æ‹©")
material = st.sidebar.selectbox("é€‰æ‹©å£ç”»æè´¨ï¼ˆå½±å“è¯„åˆ†ä¸å»ºè®®ï¼‰", MATERIAL_OPTIONS)
auto_material = st.sidebar.checkbox("è‡ªåŠ¨è¯†åˆ«æè´¨ï¼ˆè¯•éªŒæ€§ï¼‰", value=False)
mat_model_path = None
if auto_material:
    st.sidebar.markdown("- å¯é€‰ï¼šæä¾›æè´¨åˆ†ç±»ONNXæ¨¡å‹è·¯å¾„ï¼ˆè‹¥ç•™ç©ºåˆ™ä½¿ç”¨å¯å‘å¼è¯†åˆ«ï¼‰")
    mat_model_path = st.sidebar.text_input("æè´¨æ¨¡å‹è·¯å¾„ï¼ˆ.onnxï¼Œå¯é€‰ï¼‰", "")
use_deep = st.sidebar.checkbox("ä½¿ç”¨æ·±åº¦åˆ†å‰²æ¨¡å‹ï¼ˆONNXï¼‰", value=False)
model_path = None
model_input_size = 512

# æ£€æµ‹ç®—æ³•é€‰æ‹©
if IMPROVED_DETECTION_AVAILABLE:
    use_improved_detection = st.sidebar.checkbox("ä½¿ç”¨æ”¹è¿›çš„æ£€æµ‹ç®—æ³•ï¼ˆæ›´å‡†ç¡®ï¼‰", value=False)
else:
    use_improved_detection = False

# æ€§èƒ½/é€Ÿåº¦è®¾ç½®
st.sidebar.markdown("### æ€§èƒ½/é€Ÿåº¦è®¾ç½®")
max_dim_setting = st.sidebar.slider("æœ€å¤§å¤„ç†åˆ†è¾¨ç‡ï¼ˆåƒç´ ï¼‰", 512, 2048, 1280, 64)
icp_threshold = st.sidebar.slider("3D ICP è·ç¦»é˜ˆå€¼ (m)", 0.002, 0.05, 0.02, 0.002)
class_id_bg = 0
class_id_crack = 1
class_id_peel = 2
class_id_disc = 3
class_id_stain = 4
class_id_salt = 5
class_id_bio = 6
if use_deep:
    model_path = st.sidebar.text_input("æ¨¡å‹è·¯å¾„ï¼ˆ.onnxï¼‰", "")
    model_input_size = st.sidebar.selectbox("æ¨¡å‹è¾“å…¥å°ºå¯¸ï¼ˆæ–¹å½¢ï¼‰", [256, 320, 384, 512, 640, 768, 1024], index=3)
    st.sidebar.markdown("#### ç±»åˆ«IDæ˜ å°„ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰")
    col1, col2 = st.sidebar.columns(2)
    with col1:
        class_id_bg = st.number_input("èƒŒæ™¯ID", value=0, step=1)
        class_id_crack = st.number_input("è£‚ç¼ID", value=1, step=1)
        class_id_stain = st.number_input("æ±¡æ¸/éœ‰æ–‘ID", value=4, step=1)
    with col2:
        class_id_peel = st.number_input("å‰¥è½ID", value=2, step=1)
        class_id_disc = st.number_input("è¤ªè‰²ID", value=3, step=1)
        class_id_salt = st.number_input("ç›èš€/é£åŒ–ID", value=5, step=1)
    class_id_bio = st.sidebar.number_input("ç”Ÿç‰©é™„ç€ID", value=6, step=1)

# Display controls (å»æ‚åŒ–)
st.sidebar.markdown("### æ˜¾ç¤ºè®¾ç½®ï¼ˆå‡å°‘å¹²æ‰°ï¼‰")
display_mode = st.sidebar.selectbox(
    "æ˜¾ç¤ºæ–¹å¼",
    ["ä»…æ©è†œ", "ä»…è¾¹æ¡†", "è¾¹æ¡†+æ©è†œ"],
    index=0
)
min_area = st.sidebar.slider("æœ€å°ç›®æ ‡é¢ç§¯ï¼ˆåƒç´ ï¼‰", 0, 5000, 400, step=50)
show_crack = st.sidebar.checkbox("æ˜¾ç¤ºï¼šè£‚ç¼", True)
show_peel = st.sidebar.checkbox("æ˜¾ç¤ºï¼šå‰¥è½", True)
show_disc = st.sidebar.checkbox("æ˜¾ç¤ºï¼šè¤ªè‰²", True)
show_stain = st.sidebar.checkbox("æ˜¾ç¤ºï¼šæ±¡æ¸/éœ‰æ–‘", True)
show_salt = st.sidebar.checkbox("æ˜¾ç¤ºï¼šç›èš€/é£åŒ–", True)
show_bio = st.sidebar.checkbox("æ˜¾ç¤ºï¼šç”Ÿç‰©é™„ç€", True)
show_labels = st.sidebar.checkbox("åœ¨å›¾ä¸Šæ ‡æ³¨ç±»åˆ«ç®€å†™", True)
label_lang = st.sidebar.selectbox("æ ‡ç­¾æ ·å¼", ["ç®€å†™(EN)", "ä¸­æ–‡"], index=0)

# å®å°ºæ ‡å®šï¼ˆåƒç´ -æ¯«ç±³æ¢ç®—ï¼‰
st.sidebar.markdown("### å®å°ºæ ‡å®šï¼ˆå•ä½è½¬æ¢ï¼‰")
if "ppmm" not in st.session_state:
    st.session_state["ppmm"] = None  # pixels per millimeter
scale_mode = st.sidebar.selectbox("æ ‡å®šæ–¹å¼", ["æœªæ ‡å®š", "ç›´æ¥è¾“å…¥åƒç´ /æ¯«ç±³", "å‚è€ƒç‰©æ ‡å®šï¼ˆè¾“å…¥åƒç´ é•¿åº¦ä¸å®é•¿mmï¼‰"], index=0)
ppmm_direct = None
if scale_mode == "ç›´æ¥è¾“å…¥åƒç´ /æ¯«ç±³":
    ppmm_direct = st.sidebar.number_input("åƒç´ /æ¯«ç±³ (pixels per mm)", min_value=0.0, value=float(st.session_state["ppmm"]) if st.session_state["ppmm"] else 0.0, step=0.01)
    if ppmm_direct > 0:
        st.session_state["ppmm"] = ppmm_direct
elif scale_mode == "å‚è€ƒç‰©æ ‡å®šï¼ˆè¾“å…¥åƒç´ é•¿åº¦ä¸å®é•¿mmï¼‰":
    ref_px = st.sidebar.number_input("å‚è€ƒç‰©åœ¨å›¾ä¸­çš„åƒç´ é•¿åº¦", min_value=0.0, value=0.0, step=1.0)
    ref_mm = st.sidebar.number_input("å‚è€ƒç‰©å®é™…é•¿åº¦ï¼ˆmmï¼‰", min_value=0.0, value=0.0, step=0.1)
    if ref_px > 0 and ref_mm > 0:
        st.session_state["ppmm"] = ref_px / ref_mm
_ppmm_val = st.session_state["ppmm"]
if _ppmm_val:
    st.sidebar.caption(f"å½“å‰æ ‡å®šï¼š{_ppmm_val:.3f} åƒç´ /æ¯«ç±³")
else:
    st.sidebar.caption("å½“å‰æ ‡å®šï¼šæœªæ ‡å®š")

# Upload (æ”¯æŒå†å²å¯¹æ¯”ï¼šå…è®¸ä¸Šä¼ æ—§å›¾åƒ)
# é¡µé¢è£…é¥°ï¼šä½¿ç”¨æ–‡ç‰©å›¾æ¡ˆèƒŒæ™¯ï¼ˆå·²åœ¨inject_custom_cssä¸­è®¾ç½®ï¼‰
# æ³¨é‡Šæ‰åŠ¨æ€èƒŒæ™¯ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡ç‰©å›¾æ¡ˆèƒŒæ™¯æ ·å¼
# try:
#     bg_imgs = get_background_images_b64()
#     inject_dynamic_background(bg_imgs, interval_ms=10000)
#     # ä¿ç•™åŠ¨æ€èƒŒæ™¯ï¼Œä¸å†æ³¨å…¥å›ºå®šæµ®åŠ¨åº•æ 
# except Exception:
#     pass
# ä½¿ç”¨æ”¹è¿›çš„æ ‡ç­¾é¡µæ ·å¼
if IMPROVED_UI_AVAILABLE:
    tabs = st.tabs(["ğŸ›ï¸ äºŒç»´å£ç”»è¯Šæ–­", "ğŸ“ ä¸‰ç»´çŸ³çªŸç›‘æµ‹ï¼ˆåŸºç¡€ç‰ˆï¼‰", "ğŸ“– æ–‡çŒ®èµ„æ–™è¯†åˆ«ï¼ˆOCRï¼‰", "ğŸ”® å¤šæ¨¡æ€èåˆè¯Šæ–­", "ğŸ§  æ·±åº¦å­¦ä¹ è®­ç»ƒ", "ğŸ“š çŸ¥è¯†åº“", "ğŸ“‹ æ¡ˆä¾‹åº“", "ğŸ“± ç§»åŠ¨ç«¯é‡‡é›†"])
else:
    tabs = st.tabs(["äºŒç»´å£ç”»è¯Šæ–­", "ä¸‰ç»´çŸ³çªŸç›‘æµ‹ï¼ˆåŸºç¡€ç‰ˆï¼‰", "æ–‡çŒ®èµ„æ–™è¯†åˆ«ï¼ˆOCRï¼‰", "å¤šæ¨¡æ€èåˆè¯Šæ–­", "æ·±åº¦å­¦ä¹ è®­ç»ƒ", "çŸ¥è¯†åº“", "æ¡ˆä¾‹åº“", "ç§»åŠ¨ç«¯é‡‡é›†"])

with tabs[0]:
    st.markdown("#### 1) ä¸Šä¼ å›¾åƒï¼ˆå¯ä¸Šä¼  1-2 å¼ ç”¨äºæ—¶é—´å¯¹æ¯”ï¼‰")
uploaded = st.file_uploader("ä¸Šä¼ å½“å‰å›¾åƒï¼ˆå¿…å¡«ï¼‰", type=['jpg','jpeg','png'])
uploaded_prev = st.file_uploader("ä¸Šä¼ å†å²å›¾åƒï¼ˆå¯é€‰ï¼Œç”¨äºå¯¹æ¯”ï¼‰ï¼Œè‹¥æœ‰åˆ™ä¸ºåŒä¸€å£ç”»çš„æ—©æœŸç…§ç‰‡", type=['jpg','jpeg','png'])

analyze_btn = st.button("å¼€å§‹åˆ†æ")

if analyze_btn and uploaded is None:
    st.error("è¯·è‡³å°‘ä¸Šä¼ å½“å‰å›¾åƒä»¥è¿›è¡Œåˆ†æã€‚")

if uploaded is not None and analyze_btn:
    # read images
    file_bytes = np.asarray(bytearray(uploaded.read()), dtype=np.uint8)
    img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    if img is None:
        st.error("æ— æ³•è¯»å–å›¾åƒï¼Œè¯·ç¡®è®¤æ ¼å¼æ­£ç¡®ã€‚")
    else:
        img_proc, scale = preprocess_image(img.copy(), target_max_dim=int(max_dim_setting))
        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        st.subheader("åŸå§‹å›¾åƒï¼ˆå·²ç¼©æ”¾ä»¥ä¾¿å¤„ç†ï¼‰")
        # Auto material detection
        detected_material = None
        detected_conf = None
        detected_details = None
        if auto_material:
            try:
                if mat_model_path:
                    detected_material, detected_conf, mat_probs = run_material_model(img_proc, mat_model_path)
                    detected_details = {"probs": mat_probs}
                else:
                    detected_material, detected_conf, detected_details = classify_material_heuristic(img_proc)
                st.info(f"è‡ªåŠ¨è¯†åˆ«æè´¨ï¼š{detected_material}ï¼ˆç½®ä¿¡åº¦ {detected_conf:.2f}ï¼‰")
                apply_mat = st.toggle("å°†è¯†åˆ«ç»“æœåº”ç”¨åˆ°è¯„åˆ†/å»ºè®®", value=True)
                if apply_mat:
                    material = detected_material
            except Exception as e:
                st.warning(f"è‡ªåŠ¨æè´¨è¯†åˆ«å¤±è´¥ï¼š{e}")

        st.image(img_rgb, width='stretch')

        # è®­ç»ƒå¥½çš„åˆ†ç±»æ¨¡å‹é¢„æµ‹
        st.markdown("### ğŸ¤– AIæ™ºèƒ½åˆ†ç±»é¢„æµ‹")
        classifier_model = load_trained_classifier()
        if classifier_model is not None:
            with st.spinner("AIæ¨¡å‹æ­£åœ¨åˆ†æå›¾åƒ..."):
                prediction_result = predict_mural_disease(img_rgb, classifier_model)
            
            if prediction_result:
                predicted_class = prediction_result['predicted_class']
                confidence = prediction_result['confidence']
                all_probs = prediction_result['all_probabilities']
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                class_display_names = {
                    "crack": "è£‚ç¼ç—…å®³",
                    "peel": "å‰¥è½ç—…å®³", 
                    "disc": "è„±è½ç¼ºæŸ",
                    "clean": "å®Œå¥½å£ç”»"
                }
                
                st.success(f"ğŸ¯ AIé¢„æµ‹ç»“æœ: **{class_display_names[predicted_class]}** (ç½®ä¿¡åº¦: {confidence:.2%})")
                
                # æ˜¾ç¤ºå„ç±»åˆ«æ¦‚ç‡
                prob_cols = st.columns(4)
                for i, (class_name, prob) in enumerate(all_probs.items()):
                    with prob_cols[i]:
                        st.metric(
                            class_display_names[class_name],
                            f"{prob:.1%}",
                            delta=f"{prob-confidence:.1%}" if class_name != predicted_class else None
                        )
                
                # æ ¹æ®é¢„æµ‹ç»“æœç»™å‡ºå»ºè®®
                if predicted_class == "clean":
                    st.info("âœ… å›¾åƒæ˜¾ç¤ºå£ç”»çŠ¶æ€è‰¯å¥½ï¼Œå»ºè®®å®šæœŸç›‘æµ‹")
                elif predicted_class == "crack":
                    st.warning("âš ï¸ æ£€æµ‹åˆ°è£‚ç¼ç—…å®³ï¼Œå»ºè®®è¿›è¡Œç»“æ„ç¨³å®šæ€§è¯„ä¼°")
                elif predicted_class == "peel":
                    st.warning("âš ï¸ æ£€æµ‹åˆ°å‰¥è½ç—…å®³ï¼Œå»ºè®®æ£€æŸ¥ç¯å¢ƒæ¹¿åº¦å’Œæ¸©åº¦")
                elif predicted_class == "disc":
                    st.error("âŒ æ£€æµ‹åˆ°è„±è½ç¼ºæŸï¼Œå»ºè®®ç«‹å³é‡‡å–ä¿æŠ¤æªæ–½")
        else:
            st.info("ğŸ’¡ æç¤ºï¼šè¿è¡Œ `python simple_train.py` å¯ä»¥è®­ç»ƒAIåˆ†ç±»æ¨¡å‹")

        # OCR è¯†åˆ«ï¼ˆå¯é€‰ï¼‰
        st.markdown("### ğŸ”¤ æ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰")
        if RapidOCR is None:
            st.info("æœªå®‰è£… rapidocr-onnxruntimeï¼Œå¦‚éœ€OCRï¼špip install rapidocr-onnxruntime")
        else:
            if st.toggle("å¯ç”¨OCRè¯†åˆ«ï¼ˆå®éªŒæ€§ï¼‰", value=False):
                ocr = get_rapidocr_cached()
                if ocr is None:
                    st.warning("OCR åˆå§‹åŒ–å¤±è´¥ã€‚")
                else:
                    with st.spinner("OCRè¯†åˆ«ä¸­â€¦"):
                        res, elapse = ocr(img_rgb)
                    # å±•ç¤ºç»“æœå’Œå¯ä¸‹è½½TXT
                    ocr_lines = []
                    if res:
                        for box, text, score in res:
                            ocr_lines.append(f"{text}\t{score:.3f}")
                        st.success(f"è¯†åˆ«åˆ° {len(ocr_lines)} è¡Œæ–‡æœ¬ã€‚")
                        st.code("\n".join(ocr_lines))
                        st.download_button("ä¸‹è½½OCRç»“æœï¼ˆtxtï¼‰", data=("\n".join(ocr_lines)).encode("utf-8"), file_name="ocr_result.txt", mime="text/plain")
                    else:
                        st.info("æœªè¯†åˆ«åˆ°æ˜æ˜¾æ–‡æœ¬åŒºåŸŸã€‚")

        # Optionally run deep model
        deep_masks = None
        if use_deep and model_path:
            try:
                deep_masks = run_segmentation_model(
                    img_proc,
                    model_path=model_path,
                    input_size=int(model_input_size),
                    class_ids={
                        'bg': int(class_id_bg),
                        'crack': int(class_id_crack),
                        'peel': int(class_id_peel),
                        'disc': int(class_id_disc),
                        'stain': int(class_id_stain),
                        'salt': int(class_id_salt),
                        'bio': int(class_id_bio)
                    },
                    providers=["CPUExecutionProvider"]
                )
                st.success("æ·±åº¦åˆ†å‰²å·²å¯ç”¨ï¼šç»“æœå°†æ›¿æ¢ä¼ ç»ŸCVæ©è†œã€‚")
            except Exception as e:
                st.exception(e)
                st.error("æ·±åº¦æ¨¡å‹æ¨ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„ã€ç±»åˆ«IDä¸è¾“å…¥å°ºå¯¸æ˜¯å¦åŒ¹é…ã€‚")
                deep_masks = None

        # Baseline CV detections
        gray = cv2.cvtColor(img_proc, cv2.COLOR_BGR2GRAY)
        hsv = cv2.cvtColor(img_proc, cv2.COLOR_BGR2HSV)
        
        # æ ¹æ®è®¾ç½®é€‰æ‹©ä½¿ç”¨æ”¹è¿›ç®—æ³•æˆ–åŸºç¡€ç®—æ³•
        if use_improved_detection and IMPROVED_DETECTION_AVAILABLE:
            boxes_crack, mask_crack = detect_cracks_improved(gray)
            boxes_peel, mask_peel = detect_peeling_improved(hsv)
            boxes_disc, mask_disc = detect_discoloration_improved(hsv)
            boxes_stain, mask_stain = detect_stain_mold_improved(hsv)
            boxes_salt, mask_salt = detect_salt_weathering_improved(hsv)
            boxes_bio, mask_bio = detect_bio_growth_improved(hsv)
        else:
            boxes_crack, mask_crack = detect_cracks(gray)
            boxes_peel, mask_peel = detect_peeling(hsv)
            boxes_disc, mask_disc = detect_discoloration(hsv)
            boxes_stain, mask_stain = detect_stain_mold(hsv)
            boxes_salt, mask_salt = detect_salt_weathering(hsv)
            boxes_bio, mask_bio = detect_bio_growth(hsv)

        # If deep_masks provided, prefer it
        if deep_masks:
            # expected keys 'crack','peel','disc' -> binary masks same size as img_proc
            if 'crack' in deep_masks:
                mask_crack = deep_masks['crack']
            if 'peel' in deep_masks:
                mask_peel = deep_masks['peel']
            if 'disc' in deep_masks:
                mask_disc = deep_masks['disc']
            if 'stain' in deep_masks:
                mask_stain = deep_masks['stain']
            if 'salt' in deep_masks:
                mask_salt = deep_masks['salt']
            if 'bio' in deep_masks:
                mask_bio = deep_masks['bio']

        # Annotate image for visualization
        annotated = img_rgb.copy()
        # Helper to draw boxes with filters
        def draw_boxes(boxes, color, visible, tag_text):
            if not visible or display_mode == "ä»…æ©è†œ":
                return
            for (x,y,w,h) in boxes:
                if w*h < min_area:
                    continue
                cv2.rectangle(annotated, (x,y), (x+w, y+h), color, 2)
                if show_labels:
                    # draw a small label background for readability
                    tx = x
                    ty = max(0, y-8)
                    text = tag_text
                    (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)
                    cv2.rectangle(annotated, (tx, ty-th-4), (tx+tw+4, ty+2), (0,0,0), -1)
                    cv2.putText(annotated, text, (tx+2, ty-2), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1, cv2.LINE_AA)

        # Draw per category (no Chinese labels on image toé¿å…é—®å·)
        # label text mapping
        if label_lang == "ä¸­æ–‡":
            # OpenCV ä¸æ”¯æŒä¸­æ–‡å­—ä½“ï¼Œå¯èƒ½æ˜¾ç¤ºé—®å·ï¼›è‹¥å‡ºç°ï¼Œè¯·åˆ‡æ¢åˆ°â€œç®€å†™(EN)â€
            crack_t, peel_t, disc_t, stain_t, salt_t, bio_t = "è£‚", "å‰¥", "è¤ª", "æ±¡", "ç›", "ç”Ÿ"
        else:
            crack_t, peel_t, disc_t, stain_t, salt_t, bio_t = "CR", "PL", "DC", "ST", "SA", "BIO"

        draw_boxes(boxes_crack, (255,0,0), show_crack, crack_t)
        draw_boxes(boxes_peel, (0,255,0), show_peel, peel_t)
        draw_boxes(boxes_disc, (0,0,255), show_disc, disc_t)
        draw_boxes(boxes_stain, (255,255,0), show_stain, stain_t)
        draw_boxes(boxes_salt, (0,255,255), show_salt, salt_t)
        draw_boxes(boxes_bio, (255,0,255), show_bio, bio_t)

        # Also overlay masks semi-transparently to show extent
        def overlay_mask(base_rgb, mask, color_rgb, alpha=0.35):
            overlay = base_rgb.copy()
            mask_bool = mask > 0
            overlay[mask_bool] = (overlay[mask_bool] * (1-alpha) + np.array(color_rgb) * alpha).astype(np.uint8)
            return overlay

        if display_mode in ("ä»…æ©è†œ", "è¾¹æ¡†+æ©è†œ"):
            if show_crack:
                annotated = overlay_mask(annotated, mask_crack, (255,0,0), alpha=0.25)
            if show_peel:
                annotated = overlay_mask(annotated, mask_peel, (0,255,0), alpha=0.18)
            if show_disc:
                annotated = overlay_mask(annotated, mask_disc, (0,0,255), alpha=0.18)
            if show_stain:
                annotated = overlay_mask(annotated, mask_stain, (255,255,0), alpha=0.20)
            if show_salt:
                annotated = overlay_mask(annotated, mask_salt, (0,255,255), alpha=0.18)
            if show_bio:
                annotated = overlay_mask(annotated, mask_bio, (255,0,255), alpha=0.20)

        st.subheader("åˆ†æç»“æœï¼ˆå¸¦æ ‡æ³¨ï¼‰")
        st.image(annotated, width='stretch')

        # Legend (å›¾ä¾‹) for colors
        legend_html = """
        <div style='display:flex;flex-wrap:wrap;gap:12px;margin:6px 0 10px 0;'>
          <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#ff0000;'></span><span>è£‚ç¼</span></div>
          <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#00cc00;'></span><span>å‰¥è½</span></div>
          <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#0000ff;'></span><span>è¤ªè‰²</span></div>
          <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#ffff00;'></span><span>æ±¡æ¸/éœ‰æ–‘</span></div>
          <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#00ffff;'></span><span>ç›èš€/é£åŒ–</span></div>
          <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#ff00ff;'></span><span>ç”Ÿç‰©é™„ç€</span></div>
        </div>
        """
        st.markdown(legend_html, unsafe_allow_html=True)
        # è‰²å½©å¤åŸï¼ˆåŸºç¡€ï¼‰å·²ç§»é™¤ï¼Œä»…ä¿ç•™é«˜çº§å¤åŸåŠŸèƒ½

        # ---------------------
        # Quantification & scoring
        # ---------------------
        h,w = gray.shape
        total_pixels = h*w
        crack_area = int(np.sum(mask_crack>0))
        peel_area = int(np.sum(mask_peel>0))
        disc_area = int(np.sum(mask_disc>0))
        stain_area = int(np.sum(mask_stain>0))
        salt_area = int(np.sum(mask_salt>0))
        bio_area = int(np.sum(mask_bio>0))

        crack_pct = crack_area / total_pixels * 100
        peel_pct = peel_area / total_pixels * 100
        disc_pct = disc_area / total_pixels * 100
        stain_pct = stain_area / total_pixels * 100
        salt_pct = salt_area / total_pixels * 100
        bio_pct = bio_area / total_pixels * 100

        # severity score: weighted sum (0-100)
        weights = MATERIAL_WEIGHTS.get(material, MATERIAL_WEIGHTS["æœªæŒ‡å®š"])
        # normalize each by an empirical factor per category
        score = (
            weights.get('crack',1.0) * crack_pct * 1.8 +
            weights.get('peel',1.0) * peel_pct * 1.2 +
            weights.get('disc',1.0) * disc_pct * 1.5 +
            weights.get('stain',1.0) * stain_pct * 0.9 +
            weights.get('salt',1.0) * salt_pct * 1.6 +
            weights.get('bio',1.0) * bio_pct * 1.1
        )
        # map to 0-100
        severity = min(round(score,1), 100.0)

        st.markdown("### ğŸ“‹ é‡åŒ–ç»“æœ")
        st.write(f"- è£‚ç¼è¦†ç›–é¢ç§¯ï¼š{crack_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {crack_pct:.3f}%")
        st.write(f"- å‰¥è½è¦†ç›–é¢ç§¯ï¼š{peel_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {peel_pct:.3f}%")
        st.write(f"- è¤ªè‰²è¦†ç›–é¢ç§¯ï¼š{disc_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {disc_pct:.3f}%")
        st.write(f"- æ±¡æ¸/éœ‰æ–‘è¦†ç›–é¢ç§¯ï¼š{stain_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {stain_pct:.3f}%")
        st.write(f"- ç›èš€/é£åŒ–è¦†ç›–é¢ç§¯ï¼š{salt_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {salt_pct:.3f}%")
        st.write(f"- ç”Ÿç‰©é™„ç€è¦†ç›–é¢ç§¯ï¼š{bio_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {bio_pct:.3f}%")
        st.write(f"- æè´¨ï¼š**{material}**ï¼ˆç”¨äºè°ƒæ•´è¯„åˆ†ä¸å»ºè®®ï¼‰")
        st.metric("æ•´ä½“ç—…å®³ä¸¥é‡åº¦ï¼ˆ0-100ï¼‰", f"{severity}")

        # severity label
        if severity < 5:
            lvl = "è½»å¾®"
        elif severity < 20:
            lvl = "ä¸­ç­‰"
        else:
            lvl = "ä¸¥é‡"
        st.write(f"åˆ¤æ–­ç­‰çº§ï¼š**{lvl}**")

        # ---------------------
        # ç»†åŒ–ç—…ç†æŒ‡æ ‡ï¼ˆè¿é€šåŸŸ/å½¢æ€/æ–¹å‘ï¼‰
        # ---------------------
        def extract_components(mask, min_area_px=min_area):
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats((mask>0).astype(np.uint8), connectivity=8)
            rows = []
            for i in range(1, num_labels):
                area = int(stats[i, cv2.CC_STAT_AREA])
                if area < min_area_px:
                    continue
                x = int(stats[i, cv2.CC_STAT_LEFT])
                y = int(stats[i, cv2.CC_STAT_TOP])
                w_ = int(stats[i, cv2.CC_STAT_WIDTH])
                h_ = int(stats[i, cv2.CC_STAT_HEIGHT])
                elong = (max(w_, h_) / max(1.0, min(w_, h_)))
                ys, xs = np.where(labels == i)
                if xs.size > 5:
                    x_mean = xs.mean(); y_mean = ys.mean()
                    x_c = xs - x_mean; y_c = ys - y_mean
                    cov_xx = float(np.mean(x_c*x_c)); cov_yy = float(np.mean(y_c*y_c)); cov_xy = float(np.mean(x_c*y_c))
                    theta = 0.5 * np.arctan2(2*cov_xy, (cov_xx - cov_yy))
                    orient_deg = float(np.degrees(theta)) % 180
                else:
                    orient_deg = float('nan')
                # ä¼°è®¡é•¿åº¦ä¸å¹³å‡å®½åº¦ï¼šç»†é•¿ç›®æ ‡ç”¨éª¨æ¶è¿‘ä¼¼é•¿åº¦ï¼Œå¦åˆ™ç”¨ç­‰æ•ˆç›´å¾„
                comp_mask = (labels == i).astype(np.uint8)
                length_px = float(np.sqrt((w_**2 + h_**2)))
                mean_width_px = float(area / max(1.0, length_px))
                # è‹¥å·²æ ‡å®šï¼Œè½¬æ¢åˆ°æ¯«ç±³
                ppmm = st.session_state.get('ppmm')
                length_mm = (length_px / ppmm) if ppmm else None
                mean_width_mm = (mean_width_px / ppmm) if ppmm else None
                rows.append({
                    'area_px': area,
                    'bbox_w': w_,
                    'bbox_h': h_,
                    'elongation': round(elong,3),
                    'orientation_deg': round(orient_deg,2),
                    'length_px': round(length_px,2),
                    'mean_width_px': round(mean_width_px,2),
                    **({'length_mm': round(length_mm,2), 'mean_width_mm': round(mean_width_mm,2)} if ppmm else {})
                })
            return rows

        import pandas as _pd_alias
        metrics = {
            'è£‚ç¼': extract_components(mask_crack),
            'å‰¥è½': extract_components(mask_peel),
            'è¤ªè‰²': extract_components(mask_disc),
            'æ±¡æ¸/éœ‰æ–‘': extract_components(mask_stain),
            'ç›èš€/é£åŒ–': extract_components(mask_salt),
            'ç”Ÿç‰©é™„ç€': extract_components(mask_bio)
        }
        st.markdown("### ğŸ” ç»†åŒ–ç—…ç†æŒ‡æ ‡ï¼ˆæŒ‰ç±»åˆ«ï¼‰")
        cat_tabs = st.tabs(list(metrics.keys()))
        for tab, (cat, rows) in zip(cat_tabs, metrics.items()):
            with tab:
                if len(rows) == 0:
                    st.write("æ— æ˜¾è‘—è¿é€šåŸŸï¼ˆå—æœ€å°é¢ç§¯é˜ˆå€¼å½±å“ï¼‰")
                else:
                    df = _pd_alias.DataFrame(rows)
                    stats_msg = f"è¿é€šåŸŸæ•°é‡ï¼š{len(df)}ï¼Œé¢ç§¯ä¸­ä½æ•°ï¼š{df['area_px'].median():.0f} pxï¼Œç»†é•¿æ¯”P95ï¼š{df['elongation'].quantile(0.95):.2f}"
                    if 'mean_width_mm' in df.columns:
                        stats_msg += f"ï¼Œå¹³å‡å®½åº¦ä¸­ä½æ•°ï¼š{df['mean_width_mm'].median():.2f} mm"
                    st.write(stats_msg)
                    st.dataframe(df.sort_values('area_px', ascending=False).head(50), use_container_width=True)
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(label=f"ä¸‹è½½{cat}æŒ‡æ ‡CSV", data=csv, file_name=f"metrics_{cat}.csv", mime="text/csv")

        # textual suggestions
        st.markdown("### ğŸ’¡ å»ºè®®ï¼ˆå¯¹ç—‡æ–¹æ¡ˆï¼‰")
        pct_map = {
            'crack': crack_pct,
            'peel': peel_pct,
            'disc': disc_pct,
            'stain': stain_pct,
            'salt': salt_pct,
            'bio': bio_pct
        }
        detailed_recs = build_recommendations(material, pct_map, severity)
        for r in detailed_recs:
            st.write(f"- {r}")

        category_counts = {
            "è£‚ç¼": len(metrics.get("è£‚ç¼", [])),
            "å‰¥è½": len(metrics.get("å‰¥è½", [])),
            "è¤ªè‰²": len(metrics.get("è¤ªè‰²", [])),
            "æ±¡æ¸/éœ‰æ–‘": len(metrics.get("æ±¡æ¸/éœ‰æ–‘", [])),
            "ç›èš€/é£åŒ–": len(metrics.get("ç›èš€/é£åŒ–", [])),
            "ç”Ÿç‰©é™„ç€": len(metrics.get("ç”Ÿç‰©é™„ç€", [])),
        }
        area_percentages = {
            "è£‚ç¼": crack_pct,
            "å‰¥è½": peel_pct,
            "è¤ªè‰²": disc_pct,
            "æ±¡æ¸/éœ‰æ–‘": stain_pct,
            "ç›èš€/é£åŒ–": salt_pct,
            "ç”Ÿç‰©é™„ç€": bio_pct,
        }
        combined_mask = (
            (mask_crack > 0).astype(np.float32) * 1.2
            + (mask_peel > 0).astype(np.float32) * 1.0
            + (mask_disc > 0).astype(np.float32) * 0.8
            + (mask_stain > 0).astype(np.float32) * 0.6
            + (mask_salt > 0).astype(np.float32) * 0.7
            + (mask_bio > 0).astype(np.float32) * 0.5
        )
        heatmap_preview = downscale_mask_for_heatmap(combined_mask)

        enh_tabs = st.tabs(["ğŸ“Š äº¤äº’ä»ªè¡¨æ¿", "ğŸ”„ è¿›åº¦æ¼”ç¤º", "âœ¨ åŠŸèƒ½æç¤º"])
        with enh_tabs[0]:
            render_interactive_dashboard(category_counts, area_percentages, heatmap_preview)
        with enh_tabs[1]:
            render_quick_progress_controls()
        with enh_tabs[2]:
            st.subheader("å¿«é€Ÿå¢å¼ºå»ºè®®")
            st.markdown(
                "- ä½¿ç”¨é¡¹ç›®ç®¡ç†é¢æ¿åˆ‡æ¢ä¸åŒæ´çªŸåˆ†æä»»åŠ¡ã€‚\n"
                "- ç»“åˆè¶‹åŠ¿å›¾è¯„ä¼°ç—…å®³å‘å±•é€Ÿåº¦ï¼ŒåŠæ—¶è°ƒæ•´ä¿æŠ¤ç­–ç•¥ã€‚\n"
                "- é€šè¿‡è¿›åº¦æ¼”ç¤ºå‘å›¢é˜Ÿå±•ç¤ºç³»ç»Ÿå·¥ä½œæµç¨‹ï¼Œæ–¹ä¾¿åŸ¹è®­ä¸æ²Ÿé€šã€‚"
            )

        # ---------------------
        # å›¾åƒå¤åŸåŠŸèƒ½ï¼ˆä¸»åˆ†ææµç¨‹ä¸­ï¼‰
        # ---------------------
        st.markdown("---")
        st.markdown("## ğŸ¨ å›¾åƒå¤åŸåŠŸèƒ½")
        
        # é«˜çº§å¤åŸåŠŸèƒ½
        if ADVANCED_RESTORATION_AVAILABLE:
            masks_dict = {
                'crack': mask_crack,
                'peel': mask_peel,
                'disc': mask_disc,
                'stain': mask_stain,
                'salt': mask_salt,
                'bio': mask_bio
            }
            render_advanced_restoration_ui(img_rgb, masks_dict, default_open=False)
        else:
            st.info("ğŸ’¡ æç¤ºï¼šé«˜çº§å¤åŸåŠŸèƒ½éœ€è¦ advanced_restoration.py æ¨¡å—")

        # ---------------------
        # Time-comparison (if previous uploaded)
        # ---------------------
        comparison_images_for_pdf = []
        if uploaded_prev:
            prev_bytes = np.asarray(bytearray(uploaded_prev.read()), dtype=np.uint8)
            prev_img = cv2.imdecode(prev_bytes, cv2.IMREAD_COLOR)
            if prev_img is None:
                st.warning("å†å²å›¾åƒæ— æ³•è¯»å–ï¼Œè¯·ç¡®è®¤æ–‡ä»¶æ ¼å¼ä¸å®Œæ•´æ€§ï¼Œå·²è·³è¿‡å†å²å¯¹æ¯”ã€‚")
            else:
                prev_img_proc, scale_prev = preprocess_image(prev_img.copy())
                prev_gray = cv2.cvtColor(prev_img_proc, cv2.COLOR_BGR2GRAY)
                prev_h, prev_w = prev_gray.shape
                # naive comparison: compare masks area difference (after resizing to match)
                # Resize prev masks to current shape if needed
                if prev_img_proc.shape[:2] != img_proc.shape[:2]:
                    prev_img_proc = cv2.resize(prev_img_proc, (img_proc.shape[1], img_proc.shape[0]))
                    prev_gray = cv2.cvtColor(prev_img_proc, cv2.COLOR_BGR2GRAY)
                # detect previous masks (same pipeline)
                p_boxes_crack, p_mask_crack = detect_cracks(prev_gray)
                p_hsv = cv2.cvtColor(prev_img_proc, cv2.COLOR_BGR2HSV)
                p_boxes_peel, p_mask_peel = detect_peeling(p_hsv)
                p_boxes_disc, p_mask_disc = detect_discoloration(p_hsv)
                # compute area change
                prev_crack_area = int(np.sum(p_mask_crack>0))
                prev_peel_area = int(np.sum(p_mask_peel>0))
                prev_disc_area = int(np.sum(p_mask_disc>0))
                st.markdown("### ğŸ•’ å†å²å¯¹æ¯”ç»“æœ")
                st.write(f"- è£‚ç¼é¢ç§¯å˜åŒ–ï¼š{prev_crack_area} -> {crack_area} ï¼ˆå·®å€¼ {crack_area - prev_crack_area} åƒç´ ï¼‰")
                st.write(f"- å‰¥è½é¢ç§¯å˜åŒ–ï¼š{prev_peel_area} -> {peel_area} ï¼ˆå·®å€¼ {peel_area - prev_peel_area} åƒç´ ï¼‰")
                st.write(f"- è¤ªè‰²é¢ç§¯å˜åŒ–ï¼š{prev_disc_area} -> {disc_area} ï¼ˆå·®å€¼ {disc_area - prev_disc_area} åƒç´ ï¼‰")
                # quick assessment
                if (crack_area - prev_crack_area) > (0.05 * total_pixels):
                    st.error("è£‚ç¼é¢ç§¯æ˜¾è‘—å¢åŠ ï¼Œå»ºè®®å°½å¿«å®åœ°è¯„ä¼°ã€‚")
                elif (peel_area - prev_peel_area) > (0.05 * total_pixels):
                    st.error("å‰¥è½é¢ç§¯æ˜¾è‘—å¢åŠ ï¼Œå¯èƒ½å­˜åœ¨è¿›å±•æ€§ç ´åã€‚")

                try:
                    prev_img_rgb = cv2.cvtColor(prev_img_proc, cv2.COLOR_BGR2RGB)
                    comparison_images_for_pdf.append(numpy_image_to_bytes(prev_img_rgb))
                    comparison_images_for_pdf.append(numpy_image_to_bytes(img_rgb))
                except Exception:
                    pass

        # ---------------------
        # Generate PDF with annotated image and results
        # ---------------------
        def generate_pdf_report(annotated_rgb, results, material, suggestions_text):
            """ç”Ÿæˆä¸“ä¸šç‰ˆPDFæŠ¥å‘Šå¹¶è¿”å›BytesIO"""

            def classify_severity(pct_value: float) -> str:
                if pct_value >= 6.0:
                    return "é«˜"
                if pct_value >= 2.0:
                    return "ä¸­"
                if pct_value > 0:
                    return "ä½"
                return "æ— "

            location_name = uploaded.name if uploaded else "å½“å‰å£ç”»æ ·æœ¬"
            total_defects = sum(len(rows) for rows in metrics.values())
            overall_health = max(0.0, 100.0 - severity)
            if severity >= 30:
                preservation_status = "éœ€é‡ç‚¹å…³æ³¨"
                recommendation_level = "åŠ å¼ºç›‘æµ‹"
            elif severity >= 10:
                preservation_status = "è¾ƒå¥½"
                recommendation_level = "å®šæœŸç›‘æµ‹"
            else:
                preservation_status = "è‰¯å¥½"
                recommendation_level = "æŒç»­è§‚å¯Ÿ"

            analysis_data = {
                "location": location_name,
                "crack_count": len(metrics.get("è£‚ç¼", [])),
                "crack_severity": classify_severity(crack_pct),
                "peel_area": peel_pct,
                "peel_severity": classify_severity(peel_pct),
                "discolor_level": disc_pct,
                "discolor_severity": classify_severity(disc_pct),
                "overall_health": overall_health,
                "overall_severity": lvl,
                "total_defects": total_defects,
                "preservation_status": preservation_status,
                "recommendation_level": recommendation_level,
                "result_lines": results,
            }

            defect_categories = [
                ("è£‚ç¼", crack_pct, metrics.get("è£‚ç¼", [])),
                ("å‰¥è½", peel_pct, metrics.get("å‰¥è½", [])),
                ("è¤ªè‰²", disc_pct, metrics.get("è¤ªè‰²", [])),
                ("æ±¡æ¸/éœ‰æ–‘", stain_pct, metrics.get("æ±¡æ¸/éœ‰æ–‘", [])),
                ("ç›èš€/é£åŒ–", salt_pct, metrics.get("ç›èš€/é£åŒ–", [])),
                ("ç”Ÿç‰©é™„ç€", bio_pct, metrics.get("ç”Ÿç‰©é™„ç€", [])),
            ]

            detailed_defects = []
            for label, pct_value, rows in defect_categories:
                count = len(rows)
                severity_label = classify_severity(pct_value)
                avg_length = float(np.mean([row.get("length_px", 0.0) for row in rows])) if rows else 0.0
                if count == 0 and pct_value == 0:
                    description = f"æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„{label}ç—…å®³ï¼Œå»ºè®®ä¿æŒå¸¸è§„å·¡æ£€ã€‚"
                elif severity_label == "é«˜":
                    description = f"æ£€æµ‹åˆ°{count}å¤„æ˜æ˜¾çš„{label}ç—…å®³ï¼Œè¦†ç›–é¢ç§¯å æ¯”çº¦{pct_value:.2f}%ï¼Œå»ºè®®ç«‹å³ç»„ç»‡é’ˆå¯¹æ€§ä¿®å¤ã€‚"
                elif severity_label == "ä¸­":
                    description = f"{label}ç—…å®³è¦†ç›–é¢ç§¯çº¦{pct_value:.2f}%ï¼Œéœ€åœ¨è¿‘æœŸå®‰æ’é‡ç‚¹åŠ å›ºä¸å…»æŠ¤ã€‚"
                else:
                    description = f"{label}ç—…å®³è¦†ç›–é¢ç§¯çº¦{pct_value:.2f}%ï¼Œå»ºè®®çº³å…¥å…³é”®åŒºåŸŸå·¡æŸ¥è®¡åˆ’ã€‚"

                detailed_defects.append(
                    {
                        "type": label,
                        "count": count,
                        "area_ratio": pct_value,
                        "avg_size": avg_length,
                        "severity": severity_label,
                        "description": description,
                    }
                )

            rec_actions = []
            for idx, rec_line in enumerate(suggestions_text):
                priority = 1 if idx < 2 else 2
                timeline = "1ä¸ªæœˆå†…" if priority == 1 else "3ä¸ªæœˆå†…"
                rec_actions.append(
                    {
                        "priority": priority,
                        "action": rec_line,
                        "timeline": timeline,
                        "cost": "å¾…è¯„ä¼°",
                    }
                )

            long_term_suggestions = list(MATERIAL_SUGGESTIONS.get(material, []))
            generic_long_term = [
                "å»ºç«‹å®šæœŸç›‘æµ‹æœºåˆ¶ï¼Œæ¯å­£åº¦å¤æ ¸ä¸€æ¬¡AIæ£€æµ‹ä¸äººå·¥å·¡æŸ¥ç»“æœ",
                "ç»´æŠ¤æ´çªŸæ¸©æ¹¿åº¦ç¯å¢ƒï¼Œå‡å°‘å¤–ç•Œéœ‡åŠ¨ä¸äººæµå½±å“",
            ]
            for item in generic_long_term:
                if item not in long_term_suggestions:
                    long_term_suggestions.append(item)

            images = {
                "original_image": numpy_image_to_bytes(img_rgb),
                "analysis_image": numpy_image_to_bytes(annotated_rgb),
            }
            if comparison_images_for_pdf:
                images["comparison_images"] = comparison_images_for_pdf

            basic_info = {
                "project_name": "å£ç”»ç—…å®³æ™ºèƒ½åˆ†ææŠ¥å‘Š",
                "location": location_name,
                "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "material": material,
                "severity": lvl,
                "report_id": f"RP-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                "cover_image": numpy_image_to_bytes(annotated_rgb),
                "contact": st.session_state.get("contact_info", "å¾…è¡¥å……"),
                "version": st.session_state.get("report_version", "1.0"),
            }

            confidence_est = max(0.6, min(0.98, 1.0 - severity / 120.0))
            technical_data = {
                "algorithm": "æ”¹è¿›å‹å¤šé€šé“ç—…å®³æ£€æµ‹æµç¨‹",
                "resolution": f"{w}Ã—{h}px",
                "confidence": confidence_est,
                "processing_time": st.session_state.get("processing_time", "çº¦æ•°ç§’ï¼ˆè§†ç¡¬ä»¶è€Œå®šï¼‰"),
                "software": "çŸ³çªŸå¯ºå£ç”»AIåˆ†æç³»ç»Ÿ v2.0",
                "data_format": "RGBå›¾åƒ + æ£€æµ‹æ©è†œ",
            }

            report_data = {
                "basic_info": basic_info,
                "analysis_data": analysis_data,
                "images": images,
                "detailed_data": {"defects": detailed_defects},
                "recommendations": {"actions": rec_actions, "long_term": long_term_suggestions},
                "technical_data": technical_data,
            }

            pdf_generator = ProfessionalPDFReport()
            pdf_buffer = BytesIO()
            pdf_generator.generate_comprehensive_report(pdf_buffer, report_data)
            pdf_buffer.seek(0)
            return pdf_buffer

        results_lines = [
            f"è£‚ç¼è¦†ç›–é¢ç§¯ï¼š{crack_area} åƒç´ ï¼Œå æ¯” {crack_pct:.4f}%",
            f"å‰¥è½è¦†ç›–é¢ç§¯ï¼š{peel_area} åƒç´ ï¼Œå æ¯” {peel_pct:.4f}%",
            f"è¤ªè‰²è¦†ç›–é¢ç§¯ï¼š{disc_area} åƒç´ ï¼Œå æ¯” {disc_pct:.4f}%",
            f"æ±¡æ¸/éœ‰æ–‘è¦†ç›–é¢ç§¯ï¼š{stain_area} åƒç´ ï¼Œå æ¯” {stain_pct:.4f}%",
            f"ç›èš€/é£åŒ–è¦†ç›–é¢ç§¯ï¼š{salt_area} åƒç´ ï¼Œå æ¯” {salt_pct:.4f}%",
            f"ç”Ÿç‰©é™„ç€è¦†ç›–é¢ç§¯ï¼š{bio_area} åƒç´ ï¼Œå æ¯” {bio_pct:.4f}%",
            f"æ•´ä½“ä¸¥é‡åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰ï¼š{severity}ï¼Œç­‰çº§ï¼š{lvl}"
        ]
        suggestions_text = detailed_recs

        pdf_buf = generate_pdf_report(annotated, results_lines, material, suggestions_text)
        st.download_button("â¬‡ï¸ ä¸‹è½½è¯Šæ–­æŠ¥å‘Šï¼ˆå«æ ‡æ³¨å›¾ï¼‰PDF", data=pdf_buf.getvalue(), file_name="è¯Šæ–­æŠ¥å‘Š_å£ç”».pdf", mime="application/pdf")

        # Cache results for interactive toggling without re-uploading
        st.session_state["proc"] = {
            'img_rgb': img_rgb,
            'masks': {
                'crack': mask_crack, 'peel': mask_peel, 'disc': mask_disc,
                'stain': mask_stain, 'salt': mask_salt, 'bio': mask_bio
            },
            'boxes': {
                'crack': boxes_crack, 'peel': boxes_peel, 'disc': boxes_disc,
                'stain': boxes_stain, 'salt': boxes_salt, 'bio': boxes_bio
            },
            'shape': gray.shape
        }

with tabs[1]:
    st.markdown("#### ä¸Šä¼ ä¸¤æœŸä¸‰ç»´æ•°æ®ï¼ˆç‚¹äº‘/ç½‘æ ¼ï¼‰")
    f_epoch1 = st.file_uploader("ä¸Šä¼ ä¸€æœŸï¼ˆå‚è€ƒï¼‰PLY/PCD/OBJ/GLB", type=["ply","pcd","obj","glb"], key="pc1")
    f_epoch2 = st.file_uploader("ä¸Šä¼ äºŒæœŸï¼ˆå¯¹æ¯”ï¼‰PLY/PCD/OBJ/GLB", type=["ply","pcd","obj","glb"], key="pc2")
    max_points = st.number_input("å¯è§†åŒ–/è®¡ç®—æœ€å¤§ç‚¹æ•°ï¼ˆä¸‹é‡‡æ ·ï¼‰", min_value=10000, value=200000, step=10000)
    run_icp = st.button("æ‰§è¡Œé…å‡†ä¸è·ç¦»è®¡ç®—ï¼ˆåŸºç¡€ï¼‰")
    if run_icp:
        if o3d is None:
            st.error("ç¼ºå°‘ open3dï¼Œè¯·å…ˆå®‰è£…ï¼špip install open3d")
        elif f_epoch1 is None or f_epoch2 is None:
            st.error("è¯·ä¸Šä¼ ä¸¤æœŸä¸‰ç»´æ•°æ®æ–‡ä»¶ã€‚")
        else:
            try:
                def load_geom(file):
                    import tempfile
                    suffix = "." + file.name.split(".")[-1].lower()
                    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                        tmp.write(file.read()); path = tmp.name
                    mesh = None
                    if suffix in (".obj", ".glb"):
                        mesh = o3d.io.read_triangle_mesh(path)
                        if not mesh.has_vertices():
                            raise RuntimeError("æ— æ³•è¯»å–ç½‘æ ¼")
                        pcd = mesh.sample_points_uniformly(number_of_points=int(max_points))
                    else:
                        pcd = o3d.io.read_point_cloud(path)
                        if len(pcd.points) == 0:
                            raise RuntimeError("æ— æ³•è¯»å–ç‚¹äº‘")
                    if len(pcd.points) > max_points:
                        pcd = pcd.random_down_sample(float(max_points)/float(len(pcd.points)))
                    pcd.estimate_normals()
                    return pcd

                p1 = load_geom(f_epoch1)
                p2 = load_geom(f_epoch2)
                # ç²—é…å‡†ï¼šåŸºäºè´¨å¿ƒå¯¹é½
                c1 = p1.get_center(); c2 = p2.get_center()
                p2_t = p2.translate(c1 - c2, relative=False)
                # ç²¾é…å‡†ï¼šICP
                with st.spinner("ICPç²¾é…å‡†ä¸­â€¦"):
                    # å…ˆå…¨å±€é…å‡†å°è¯•ï¼ˆRANSACï¼‰å†ICPï¼ˆè‹¥å¯ç”¨ï¼‰
                    try:
                        voxel = max(float(icp_threshold)*2.0, 0.01)
                        p1_down = p1.voxel_down_sample(voxel)
                        p2_down = p2_t.voxel_down_sample(voxel)
                        reg_ransac = o3d.pipelines.registration.registration_ransac_based_on_correspondence(
                            p2_down, p1_down, o3d.utility.Vector2iVector(np.array([[0,0]], dtype=np.int32)),
                            max_correspondence_distance=float(icp_threshold)*3.0,
                            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(),
                            ransac_n=3,
                            criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(1000, 500)
                        )
                        init = reg_ransac.transformation if hasattr(reg_ransac, 'transformation') else np.eye(4)
                    except Exception:
                        init = np.eye(4)
                    reg = o3d.pipelines.registration.registration_icp(
                        p2_t, p1, float(icp_threshold), init,
                        o3d.pipelines.registration.TransformationEstimationPointToPoint()
                    )
                    p2_aligned = p2_t.transform(reg.transformation)
                    # è®¡ç®—æœ€è¿‘ç‚¹è·ç¦»
                    pcd_tree = o3d.geometry.KDTreeFlann(p1)
                dists = []
                pts = np.asarray(p2_aligned.points)
                for pt in pts:
                    [k, idx, _] = pcd_tree.search_knn_vector_3d(pt, 1)
                    if k > 0:
                        nn = np.asarray(p1.points)[idx[0]]
                        dists.append(float(np.linalg.norm(pt - nn)))
                if len(dists) == 0:
                    st.warning("è·ç¦»è®¡ç®—ä¸ºç©ºã€‚")
                else:
                    dists = np.array(dists)
                    st.write(f"ç‚¹æ•°ï¼š{len(dists)}ï¼Œå‡å€¼ï¼š{dists.mean()*1000:.2f} mmï¼ŒP95ï¼š{np.quantile(dists,0.95)*1000:.2f} mmï¼Œæœ€å¤§ï¼š{dists.max()*1000:.2f} mm")
                    if px is not None:
                        df = pd.DataFrame({"dist_mm": dists*1000.0})
                        st.plotly_chart(px.histogram(df, x="dist_mm", nbins=50, title="è·ç¦»åˆ†å¸ƒ(mm)"), use_container_width=True)
                    # å¯¼å‡ºCSV
                    csv = ("dist_mm\n" + "\n".join(f"{v*1000:.4f}" for v in dists)).encode("utf-8")
                    st.download_button("ä¸‹è½½è·ç¦»åˆ†å¸ƒCSV", data=csv, file_name="distances_mm.csv", mime="text/csv")
            except Exception as e:
                st.exception(e)
                st.error("ä¸‰ç»´å¤„ç†å¤±è´¥ï¼Œè¯·ç¡®è®¤æ–‡ä»¶æ ¼å¼å¹¶é€‚å½“è°ƒå°ç‚¹æ•°æˆ–é˜ˆå€¼ã€‚")

with tabs[2]:
    st.markdown("#### ä¸Šä¼ æ–‡çŒ®/èµ„æ–™å›¾ç‰‡è¿›è¡Œæ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰")
    if RapidOCR is None:
        st.info("æœªå®‰è£… rapidocr-onnxruntimeï¼Œå¦‚éœ€OCRï¼špip install rapidocr-onnxruntime")
    else:
        files_txt = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯å¤šé€‰ï¼‰JPG/PNG", type=["jpg","jpeg","png"], accept_multiple_files=True, key="ocr_multi")
        run_ocr = st.button("å¼€å§‹è¯†åˆ«", key="run_ocr_batch")
        if run_ocr:
            if not files_txt:
                st.warning("è¯·å…ˆé€‰æ‹©è‡³å°‘ä¸€å¼ å›¾ç‰‡ã€‚")
            else:
                ocr = get_rapidocr_cached()
                if ocr is None:
                    st.error("OCR åˆå§‹åŒ–å¤±è´¥ã€‚")
                else:
                    all_lines = []
                    for idx, f in enumerate(files_txt, start=1):
                        st.write(f"ç¬¬ {idx} ä¸ªæ–‡ä»¶ï¼š{f.name}")
                        img_bytes = np.asarray(bytearray(f.read()), dtype=np.uint8)
                        img = cv2.imdecode(img_bytes, cv2.IMREAD_COLOR)
                        if img is None:
                            st.warning("æ— æ³•è¯»å–è¯¥å›¾ç‰‡ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        with st.spinner("OCRè¯†åˆ«ä¸­â€¦"):
                            res, elapse = ocr(rgb)
                        lines = []
                        if res:
                            for box, text, score in res:
                                line = f"{text}\t{score:.3f}"
                                lines.append(line)
                                all_lines.append(f"[{f.name}]\t{line}")
                            st.success(f"è¯†åˆ« {len(lines)} è¡Œï¼Œç”¨æ—¶ {elapse:.2f}s")
                            st.code("\n".join(lines))
                        else:
                            st.info("æœªè¯†åˆ«åˆ°æ–‡æœ¬ã€‚")
                    if all_lines:
                        txt = ("\n".join(all_lines)).encode("utf-8")
                        st.download_button("ä¸‹è½½å…¨éƒ¨OCRç»“æœï¼ˆtxtï¼‰", data=txt, file_name="ocr_results.txt", mime="text/plain")

with tabs[3]:
    st.markdown("#### å¤šæ¨¡æ€èåˆè¯Šæ–­ç³»ç»Ÿ")
    st.info("ğŸš€ **å‰æ²¿åŠŸèƒ½**ï¼šç»“åˆå›¾åƒã€3Dç‚¹äº‘ã€æ–‡çŒ®æ–‡æœ¬è¿›è¡Œç»¼åˆè¯Šæ–­ï¼Œæä¾›æ·±åº¦åˆ†æå’Œè™šæ‹Ÿä¿®å¤")
    
    if not MULTIMODAL_AVAILABLE:
        st.warning("âš ï¸ å¤šæ¨¡æ€åŠŸèƒ½éœ€è¦é¢å¤–ä¾èµ–ï¼Œè¯·å®‰è£…ï¼š`pip install torch transformers networkx scikit-learn`")
        st.code("pip install torch transformers networkx scikit-learn")
    else:
        # å¤šæ¨¡æ€æ•°æ®ä¸Šä¼ 
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("##### ğŸ“¸ å›¾åƒæ•°æ®")
            multimodal_image = st.file_uploader("ä¸Šä¼ å£ç”»å›¾åƒ", type=['jpg','jpeg','png'], key="multimodal_img")
            
            st.markdown("##### ğŸ“„ æ–‡çŒ®æ•°æ®")
            multimodal_text = st.text_area("è¾“å…¥ç›¸å…³æ–‡çŒ®è®°å½•ï¼ˆå¦‚å†å²ä¿®å¤è®°å½•ã€æè´¨æè¿°ç­‰ï¼‰", 
                                         placeholder="ä¾‹å¦‚ï¼šè¯¥å£ç”»ä½äºæ•¦ç…Œè«é«˜çªŸç¬¬257çªŸï¼Œç»˜åˆ¶äºåŒ—é­æ—¶æœŸï¼Œä¸»è¦æè´¨ä¸ºç ‚å²©...", 
                                         height=100, key="multimodal_text")
        
        with col2:
            st.markdown("##### ğŸ—ï¸ 3Dç‚¹äº‘æ•°æ®")
            multimodal_pointcloud = st.file_uploader("ä¸Šä¼ 3Dæ‰«ææ•°æ®", type=['ply','pcd','xyz'], key="multimodal_pc")
            
            st.markdown("##### ğŸ›ï¸ çŸ³çªŸä¿¡æ¯")
            cave_type = st.selectbox("é€‰æ‹©çŸ³çªŸç±»å‹", 
                                   ["æ•¦ç…Œè«é«˜çªŸ", "äº‘å†ˆçŸ³çªŸ", "é¾™é—¨çŸ³çªŸ", "éº¦ç§¯å±±çŸ³çªŸ", "å…¶ä»–"], 
                                   key="cave_type")
            
            material_type = st.selectbox("é€‰æ‹©æè´¨ç±»å‹", 
                                       ["ç ‚å²©", "èŠ±å²—å²©", "çŸ³ç°å²©", "æ³¥è´¨ç ‚å²©", "å…¶ä»–"], 
                                       key="material_type")
        
        # å¤šæ¨¡æ€åˆ†ææŒ‰é’®
        run_multimodal = st.button("ğŸ” å¼€å§‹å¤šæ¨¡æ€èåˆåˆ†æ", key="run_multimodal")
        
        if run_multimodal:
            if not multimodal_image:
                st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€å¼ å›¾åƒè¿›è¡Œåˆ†æ")
            else:
                # åˆå§‹åŒ–å¤šæ¨¡æ€ç³»ç»Ÿ
                multimodal_system = get_multimodal_system()
                auto_annotator = get_auto_annotator()
                generative_aug = get_generative_augmentation()
                
                # å¤„ç†å›¾åƒ
                img_bytes = multimodal_image.read()
                img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                
                # å¤„ç†ç‚¹äº‘
                pointcloud = None
                if multimodal_pointcloud:
                    try:
                        pc_bytes = multimodal_pointcloud.read()
                        if multimodal_pointcloud.name.endswith('.ply'):
                            pointcloud = o3d.io.read_point_cloud_from_bytes(pc_bytes, format='ply')
                        elif multimodal_pointcloud.name.endswith('.pcd'):
                            pointcloud = o3d.io.read_point_cloud_from_bytes(pc_bytes, format='pcd')
                    except Exception as e:
                        st.warning(f"ç‚¹äº‘åŠ è½½å¤±è´¥ï¼š{e}")
                
                # å¤šæ¨¡æ€ç‰¹å¾æå–
                with st.spinner("ğŸ”„ å¤šæ¨¡æ€ç‰¹å¾æå–ä¸­..."):
                    # å›¾åƒç‰¹å¾
                    image_features = multimodal_system.encode_image(image)
                    
                    # ç‚¹äº‘ç‰¹å¾
                    pointcloud_features = multimodal_system.encode_pointcloud(pointcloud)
                    
                    # æ–‡æœ¬ç‰¹å¾
                    text_features = multimodal_system.encode_text(multimodal_text)
                    
                    # ç‰¹å¾èåˆ
                    fused_features = multimodal_system.fuse_modalities(image_features, pointcloud_features, text_features)
                
                # æ˜¾ç¤ºç‰¹å¾ä¿¡æ¯
                st.success("âœ… å¤šæ¨¡æ€ç‰¹å¾æå–å®Œæˆ")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å›¾åƒç‰¹å¾ç»´åº¦", f"{len(image_features) if image_features is not None else 0}")
                with col2:
                    st.metric("ç‚¹äº‘ç‰¹å¾ç»´åº¦", f"{len(pointcloud_features) if pointcloud_features is not None else 0}")
                with col3:
                    st.metric("æ–‡æœ¬ç‰¹å¾ç»´åº¦", f"{len(text_features) if text_features is not None else 0}")
                
                # æ·±åº¦ç¨³å®šæ€§åˆ†æ
                if pointcloud is not None:
                    st.markdown("##### ğŸ” æ·±åº¦ç¨³å®šæ€§åˆ†æ")
                    
                    # å…ˆè¿›è¡Œç—…å®³æ£€æµ‹è·å–è£‚ç¼æ©ç 
                    with st.spinner("ğŸ”„ è¿›è¡Œç—…å®³æ£€æµ‹..."):
                        # ä½¿ç”¨ç°æœ‰çš„ç—…å®³æ£€æµ‹åŠŸèƒ½
                        crack_mask = detect_crack(image) if 'detect_crack' in globals() else None
                        if crack_mask is not None:
                            depth_analysis = multimodal_system.analyze_depth_stability(image, pointcloud, crack_mask)
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("è£‚ç¼æ·±åº¦", depth_analysis["depth"])
                            with col2:
                                st.metric("ç»“æ„ç¨³å®šæ€§", depth_analysis["stability"])
                            with col3:
                                st.metric("åˆ†æç½®ä¿¡åº¦", f"{depth_analysis['confidence']:.2f}")
                            
                            if "depth_variance" in depth_analysis:
                                st.info(f"æ·±åº¦æ–¹å·®ï¼š{depth_analysis['depth_variance']:.4f}")
                            if "point_density" in depth_analysis:
                                st.info(f"ç‚¹äº‘å¯†åº¦ï¼š{depth_analysis['point_density']:.2f}")
                        else:
                            st.warning("æœªæ£€æµ‹åˆ°è£‚ç¼ï¼Œæ— æ³•è¿›è¡Œæ·±åº¦åˆ†æ")
                
                # çŸ¥è¯†å›¾è°±æŸ¥è¯¢
                st.markdown("##### ğŸ§  çŸ¥è¯†å›¾è°±æ™ºèƒ½è¯Šæ–­")
                
                # æ¨¡æ‹Ÿæ£€æµ‹åˆ°çš„ç—…å®³
                detected_pathologies = ["è¡¨é¢è£‚ç¼", "å‰¥è½"]  # è¿™é‡Œåº”è¯¥åŸºäºå®é™…æ£€æµ‹ç»“æœ
                
                treatments = multimodal_system.knowledge_graph.query_treatment(
                    cave_type, material_type, detected_pathologies
                )
                
                if treatments:
                    st.success("ğŸ¯ åŸºäºçŸ¥è¯†å›¾è°±çš„ä¿®å¤å»ºè®®ï¼š")
                    for i, treatment in enumerate(treatments[:3]):  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                        with st.expander(f"å»ºè®® {i+1}: {treatment['treatment']}"):
                            st.write(f"**é€‚ç”¨ç—…å®³**: {treatment['pathology']}")
                            st.write(f"**é€‚ç”¨æ€§**: {treatment['suitability']:.2f}")
                            st.write(f"**æˆæœ¬**: {treatment['cost']}")
                            st.write(f"**æ•ˆæœ**: {treatment['effectiveness']}")
                            st.write(f"**æŒä¹…æ€§**: {treatment['durability']}")
                else:
                    st.info("æœªæ‰¾åˆ°åŒ¹é…çš„ä¿®å¤å»ºè®®")
                
                # è‡ªåŠ¨æ ‡æ³¨
                st.markdown("##### ğŸ·ï¸ æ™ºèƒ½è‡ªåŠ¨æ ‡æ³¨")
                
                # æ¨¡æ‹Ÿæ£€æµ‹åŒºåŸŸ
                mock_regions = [
                    {"area": 500, "bbox": [100, 100, 50, 30], "elongation": 0.8},
                    {"area": 1200, "bbox": [200, 150, 80, 40], "elongation": 0.6}
                ]
                
                annotations = auto_annotator.generate_annotation(image, mock_regions, "crack")
                
                if annotations:
                    st.success("ğŸ“ è‡ªåŠ¨æ ‡æ³¨ç»“æœï¼š")
                    for i, annotation in enumerate(annotations):
                        with st.expander(f"æ ‡æ³¨ {i+1}: {annotation['type']} - {annotation['severity']}"):
                            st.write(f"**æè¿°**: {annotation['description']}")
                            st.write(f"**é¢ç§¯**: {annotation['area']} åƒç´ ")
                            st.write(f"**ç½®ä¿¡åº¦**: {annotation['confidence']:.2f}")
                            st.write(f"**ç‰¹å¾**: é•¿å®½æ¯” {annotation['features']['aspect_ratio']:.2f}")
                
                # è™šæ‹Ÿä¿®å¤
                st.markdown("##### ğŸ¨ è™šæ‹Ÿä¿®å¤é¢„è§ˆ")
                
                if crack_mask is not None:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**åŸå§‹å›¾åƒ**")
                        st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), use_column_width=True)
                    
                    with col2:
                        st.write("**è™šæ‹Ÿä¿®å¤å**")
                        restored = generative_aug.virtual_restoration(image, crack_mask, "crack")
                        st.image(cv2.cvtColor(restored, cv2.COLOR_BGR2RGB), use_column_width=True)
                    
                    # ä¿®å¤æ•ˆæœå¯¹æ¯”
                    st.markdown("##### ğŸ“Š ä¿®å¤æ•ˆæœåˆ†æ")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ä¿®å¤ç®—æ³•", "Telea Inpainting")
                    with col2:
                        st.metric("ä¿®å¤åŒºåŸŸ", f"{np.sum(crack_mask > 0)} åƒç´ ")
                    with col3:
                        st.metric("ä¿®å¤è´¨é‡", "é«˜")
                
                # ç»¼åˆæŠ¥å‘Š
                st.markdown("##### ğŸ“‹ å¤šæ¨¡æ€è¯Šæ–­æŠ¥å‘Š")
                
                report_data = {
                    "çŸ³çªŸç±»å‹": cave_type,
                    "æè´¨ç±»å‹": material_type,
                    "å›¾åƒè´¨é‡": "è‰¯å¥½" if image_features is not None else "æœªçŸ¥",
                    "3Dæ•°æ®": "å¯ç”¨" if pointcloud_features is not None else "ä¸å¯ç”¨",
                    "æ–‡çŒ®æ•°æ®": "å·²æä¾›" if text_features is not None else "æœªæä¾›",
                    "èåˆç‰¹å¾ç»´åº¦": len(fused_features) if fused_features is not None else 0,
                    "æ£€æµ‹ç—…å®³æ•°": len(detected_pathologies),
                    "ä¿®å¤å»ºè®®æ•°": len(treatments)
                }
                
                report_df = pd.DataFrame(list(report_data.items()), columns=["é¡¹ç›®", "ç»“æœ"])
                st.dataframe(report_df, use_container_width=True)
                
                # ä¸‹è½½æŠ¥å‘Š
                report_text = f"""
å¤šæ¨¡æ€èåˆè¯Šæ–­æŠ¥å‘Š
==================

åŸºæœ¬ä¿¡æ¯ï¼š
- çŸ³çªŸç±»å‹ï¼š{cave_type}
- æè´¨ç±»å‹ï¼š{material_type}
- åˆ†ææ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

æ•°æ®è´¨é‡ï¼š
- å›¾åƒæ•°æ®ï¼š{'å¯ç”¨' if image_features is not None else 'ä¸å¯ç”¨'}
- 3Dç‚¹äº‘ï¼š{'å¯ç”¨' if pointcloud_features is not None else 'ä¸å¯ç”¨'}
- æ–‡çŒ®æ–‡æœ¬ï¼š{'å·²æä¾›' if text_features is not None else 'æœªæä¾›'}

æ£€æµ‹ç»“æœï¼š
- æ£€æµ‹åˆ°ç—…å®³ï¼š{', '.join(detected_pathologies)}
- ä¿®å¤å»ºè®®ï¼š{len(treatments)} æ¡

æŠ€æœ¯æŒ‡æ ‡ï¼š
- èåˆç‰¹å¾ç»´åº¦ï¼š{len(fused_features) if fused_features is not None else 0}
- åˆ†æç½®ä¿¡åº¦ï¼š{depth_analysis.get('confidence', 0):.2f}ï¼ˆå¦‚æœ‰3Dæ•°æ®ï¼‰
                """
                
                st.download_button(
                    "ğŸ“¥ ä¸‹è½½å¤šæ¨¡æ€è¯Šæ–­æŠ¥å‘Š",
                    data=report_text.encode('utf-8'),
                    file_name=f"multimodal_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain"
                )

with tabs[4]:
    st.markdown("#### ğŸ§  æ·±åº¦å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    st.info("ğŸš€ **AIè®­ç»ƒåŠŸèƒ½**ï¼šæ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒã€è¿ç§»å­¦ä¹ ã€æ•°æ®å¢å¼ºå’Œæ¨¡å‹è¯„ä¼°")
    
    if not DEEP_LEARNING_AVAILABLE:
        st.warning("âš ï¸ æ·±åº¦å­¦ä¹ åŠŸèƒ½éœ€è¦é¢å¤–ä¾èµ–ï¼Œè¯·å®‰è£…ï¼š`pip install torch torchvision albumentations matplotlib seaborn`")
        st.code("pip install torch torchvision albumentations matplotlib seaborn")
    else:
        # æ·±åº¦å­¦ä¹ åŠŸèƒ½é€‰æ‹©
        dl_mode = st.radio(
            "é€‰æ‹©æ·±åº¦å­¦ä¹ åŠŸèƒ½",
            ["æ¨¡å‹è®­ç»ƒ", "æ•°æ®å¢å¼º", "è¿ç§»å­¦ä¹ ", "æ¨¡å‹è¯„ä¼°", "æ¨¡å‹éƒ¨ç½²"],
            horizontal=True
        )
        
        if dl_mode == "æ¨¡å‹è®­ç»ƒ":
            st.markdown("##### ğŸ¯ æ¨¡å‹è®­ç»ƒ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ“Š æ•°æ®é›†é…ç½®**")
                dataset_files = st.file_uploader(
                    "ä¸Šä¼ è®­ç»ƒæ•°æ®é›†ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰", 
                    type=['jpg','jpeg','png'], 
                    accept_multiple_files=True,
                    key="dl_dataset"
                )
                
                # ç±»åˆ«é…ç½®
                st.markdown("**ğŸ·ï¸ ç±»åˆ«é…ç½®**")
                num_classes = st.number_input("ç—…å®³ç±»åˆ«æ•°é‡", min_value=2, max_value=20, value=6)
                
                class_names = []
                for i in range(num_classes):
                    name = st.text_input(f"ç±»åˆ« {i} åç§°", value=f"ç—…å®³_{i+1}", key=f"class_{i}")
                    class_names.append(name)
                
                # æ•°æ®åˆ†å‰²
                train_ratio = st.slider("è®­ç»ƒé›†æ¯”ä¾‹", 0.6, 0.9, 0.8)
                val_ratio = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.1, 0.3, 0.1)
                test_ratio = 1 - train_ratio - val_ratio
                
                st.info(f"æ•°æ®åˆ†å‰²ï¼šè®­ç»ƒé›† {train_ratio:.1%}ï¼ŒéªŒè¯é›† {val_ratio:.1%}ï¼Œæµ‹è¯•é›† {test_ratio:.1%}")
            
            with col2:
                st.markdown("**âš™ï¸ è®­ç»ƒå‚æ•°**")
                
                # æ¨¡å‹é€‰æ‹©
                model_type = st.selectbox(
                    "é€‰æ‹©æ¨¡å‹æ¶æ„",
                    ["ResNet50", "ResNet101", "DenseNet121", "EfficientNet-B0", "VGG16"]
                )
                
                # è®­ç»ƒå‚æ•°
                epochs = st.number_input("è®­ç»ƒè½®æ•°", min_value=1, max_value=100, value=20)
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=1, max_value=64, value=16)
                learning_rate = st.number_input("å­¦ä¹ ç‡", min_value=1e-5, max_value=1e-1, value=0.001, format="%.5f")
                
                # ä¼˜åŒ–å™¨é€‰æ‹©
                optimizer_type = st.selectbox("ä¼˜åŒ–å™¨", ["Adam", "SGD"])
                scheduler_type = st.selectbox("å­¦ä¹ ç‡è°ƒåº¦å™¨", ["StepLR", "CosineAnnealingLR"])
                
                # æ•°æ®å¢å¼º
                use_augmentation = st.checkbox("å¯ç”¨æ•°æ®å¢å¼º", value=True)
                
                # è®¾å¤‡é€‰æ‹©
                device = st.selectbox("è®­ç»ƒè®¾å¤‡", ["CPU", "GPU (å¦‚æœå¯ç”¨)"])
                if device == "GPU (å¦‚æœå¯ç”¨)" and torch.cuda.is_available():
                    device = "cuda"
                else:
                    device = "cpu"
            
            # å¼€å§‹è®­ç»ƒ
            if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", key="start_training"):
                if not dataset_files:
                    st.warning("è¯·å…ˆä¸Šä¼ è®­ç»ƒæ•°æ®é›†")
                else:
                    with st.spinner("ğŸ”„ å‡†å¤‡è®­ç»ƒæ•°æ®..."):
                        # æ¨¡æ‹Ÿæ•°æ®åŠ è½½ï¼ˆå®é™…åº”è¯¥æ ¹æ®æ–‡ä»¶æ ‡ç­¾åŠ è½½ï¼‰
                        images = []
                        labels = []
                        
                        for i, file in enumerate(dataset_files):
                            img_bytes = file.read()
                            img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                            image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                            if image is not None:
                                images.append(image)
                                # æ¨¡æ‹Ÿæ ‡ç­¾ï¼ˆå®é™…åº”è¯¥ä»æ–‡ä»¶åæˆ–å…ƒæ•°æ®è·å–ï¼‰
                                labels.append(i % num_classes)
                        
                        if len(images) == 0:
                            st.error("æ— æ³•åŠ è½½ä»»ä½•å›¾åƒ")
                        else:
                            st.success(f"æˆåŠŸåŠ è½½ {len(images)} å¼ å›¾åƒ")
                            
                            # æ•°æ®åˆ†å‰²
                            X_train, X_temp, y_train, y_temp = train_test_split(
                                images, labels, test_size=(1-train_ratio), random_state=42
                            )
                            X_val, X_test, y_val, y_test = train_test_split(
                                X_temp, y_temp, test_size=test_ratio/(val_ratio+test_ratio), random_state=42
                            )
                            
                            st.info(f"æ•°æ®åˆ†å‰²å®Œæˆï¼šè®­ç»ƒé›† {len(X_train)}ï¼ŒéªŒè¯é›† {len(X_val)}ï¼Œæµ‹è¯•é›† {len(X_test)}")
                            
                            # åˆ›å»ºæ•°æ®å¢å¼º
                            if use_augmentation:
                                aug_transform = get_data_augmentation()
                            else:
                                aug_transform = None
                            
                            # åˆ›å»ºæ•°æ®é›†
                            train_dataset = MuralDataset(X_train, y_train, aug_transform)
                            val_dataset = MuralDataset(X_val, y_val, None)
                            test_dataset = MuralDataset(X_test, y_test, None)
                            
                            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                            
                            # åˆ›å»ºæ¨¡å‹
                            model = DefectClassifier(num_classes=num_classes, pretrained=True)
                            trainer = ModelTrainer(model, device=device)
                            
                            # è®­ç»ƒè¿›åº¦æ¡
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            # è®­ç»ƒå¾ªç¯
                            training_data = []
                            for epoch, train_loss, train_acc, val_loss, val_acc in trainer.train(
                                train_loader, val_loader, epochs, learning_rate, scheduler_type
                            ):
                                progress = (epoch + 1) / epochs
                                progress_bar.progress(progress)
                                
                                status_text.text(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
                                
                                training_data.append({
                                    'epoch': epoch + 1,
                                    'train_loss': train_loss,
                                    'train_acc': train_acc,
                                    'val_loss': val_loss,
                                    'val_acc': val_acc
                                })
                            
                            st.success("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
                            
                            # æ˜¾ç¤ºè®­ç»ƒç»“æœ
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡", f"{train_acc:.2f}%")
                            with col2:
                                st.metric("æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡", f"{val_acc:.2f}%")
                            with col3:
                                st.metric("æœ€ç»ˆè®­ç»ƒæŸå¤±", f"{train_loss:.4f}")
                            with col4:
                                st.metric("æœ€ç»ˆéªŒè¯æŸå¤±", f"{val_loss:.4f}")
                            
                            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
                            evaluator = ModelEvaluator(model, device=device)
                            fig = evaluator.plot_training_history(trainer)
                            st.pyplot(fig)
                            
                            # æ¨¡å‹è¯„ä¼°
                            st.markdown("##### ğŸ“Š æ¨¡å‹è¯„ä¼°")
                            if st.button("è¯„ä¼°æ¨¡å‹", key="evaluate_model"):
                                with st.spinner("ğŸ”„ è¯„ä¼°æ¨¡å‹ä¸­..."):
                                    y_pred, y_true = evaluator.evaluate(test_loader)
                                    
                                    # è®¡ç®—å‡†ç¡®ç‡
                                    accuracy = sum(p == t for p, t in zip(y_pred, y_true)) / len(y_true) * 100
                                    st.success(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")
                                    
                                    # æ··æ·†çŸ©é˜µ
                                    cm_fig = evaluator.plot_confusion_matrix(y_true, y_pred, class_names)
                                    st.pyplot(cm_fig)
                                    
                                    # åˆ†ç±»æŠ¥å‘Š
                                    report = classification_report(y_true, y_pred, target_names=class_names)
                                    st.text("åˆ†ç±»æŠ¥å‘Š:")
                                    st.text(report)
        
        elif dl_mode == "æ•°æ®å¢å¼º":
            st.markdown("##### ğŸ”„ æ•°æ®å¢å¼º")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ“¸ åŸå§‹å›¾åƒ**")
                aug_image = st.file_uploader("ä¸Šä¼ å›¾åƒè¿›è¡Œæ•°æ®å¢å¼º", type=['jpg','jpeg','png'], key="aug_image")
                
                if aug_image:
                    img_bytes = aug_image.read()
                    img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                    image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                    st.image(image_rgb, caption="åŸå§‹å›¾åƒ", use_column_width=True)
            
            with col2:
                st.markdown("**ğŸ¨ å¢å¼ºå‚æ•°**")
                
                # å¢å¼ºå‚æ•°æ§åˆ¶
                flip_h = st.checkbox("æ°´å¹³ç¿»è½¬", value=True)
                flip_v = st.checkbox("å‚ç›´ç¿»è½¬", value=False)
                rotate = st.slider("æ—‹è½¬è§’åº¦", -30, 30, 0)
                brightness = st.slider("äº®åº¦è°ƒæ•´", -0.3, 0.3, 0.0)
                contrast = st.slider("å¯¹æ¯”åº¦è°ƒæ•´", -0.3, 0.3, 0.0)
                noise = st.slider("å™ªå£°å¼ºåº¦", 0.0, 50.0, 0.0)
                blur = st.slider("æ¨¡ç³Šå¼ºåº¦", 0, 5, 0)
                
                if st.button("ç”Ÿæˆå¢å¼ºå›¾åƒ", key="generate_aug"):
                    if aug_image:
                        # åˆ›å»ºè‡ªå®šä¹‰å¢å¼º
                        custom_aug = A.Compose([
                            A.HorizontalFlip(p=1.0 if flip_h else 0.0),
                            A.VerticalFlip(p=1.0 if flip_v else 0.0),
                            A.Rotate(limit=rotate, p=1.0 if rotate != 0 else 0.0),
                            A.RandomBrightnessContrast(
                                brightness_limit=abs(brightness), 
                                contrast_limit=abs(contrast), 
                                p=1.0 if brightness != 0 or contrast != 0 else 0.0
                            ),
                            A.GaussNoise(var_limit=(noise, noise), p=1.0 if noise > 0 else 0.0),
                            A.Blur(blur_limit=blur, p=1.0 if blur > 0 else 0.0),
                            A.Resize(height=224, width=224)
                        ])
                        
                        # åº”ç”¨å¢å¼º
                        augmented = custom_aug(image=image_rgb)['image']
                        st.image(augmented, caption="å¢å¼ºåå›¾åƒ", use_column_width=True)
                        
                        # æ‰¹é‡ç”Ÿæˆ
                        if st.button("æ‰¹é‡ç”Ÿæˆå¢å¼ºæ ·æœ¬", key="batch_aug"):
                            st.info("ç”Ÿæˆ10ä¸ªå¢å¼ºæ ·æœ¬...")
                            cols = st.columns(5)
                            for i in range(10):
                                aug_sample = custom_aug(image=image_rgb)['image']
                                with cols[i % 5]:
                                    st.image(aug_sample, caption=f"æ ·æœ¬ {i+1}")
        
        elif dl_mode == "è¿ç§»å­¦ä¹ ":
            st.markdown("##### ğŸ”„ è¿ç§»å­¦ä¹ ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ—ï¸ é¢„è®­ç»ƒæ¨¡å‹**")
                base_model = st.selectbox(
                    "é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹",
                    ["ResNet50", "ResNet101", "DenseNet121", "EfficientNet-B0", "VGG16"]
                )
                
                freeze_backbone = st.checkbox("å†»ç»“éª¨å¹²ç½‘ç»œ", value=True)
                st.info("å†»ç»“éª¨å¹²ç½‘ç»œå¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œé€‚åˆå°æ•°æ®é›†")
                
                num_classes = st.number_input("ç›®æ ‡ç±»åˆ«æ•°", min_value=2, max_value=20, value=6)
                
                if st.button("åˆ›å»ºè¿ç§»å­¦ä¹ æ¨¡å‹", key="create_transfer_model"):
                    transfer_learning = get_transfer_learning()
                    model = transfer_learning.get_pretrained_model(
                        num_classes=num_classes, 
                        freeze_backbone=freeze_backbone
                    )
                    
                    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
                    total_params = sum(p.numel() for p in model.parameters())
                    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                    
                    st.success(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
                    st.metric("æ€»å‚æ•°æ•°", f"{total_params:,}")
                    st.metric("å¯è®­ç»ƒå‚æ•°æ•°", f"{trainable_params:,}")
                    st.metric("å†»ç»“å‚æ•°æ•°", f"{total_params - trainable_params:,}")
            
            with col2:
                st.markdown("**ğŸ“Š è¿ç§»å­¦ä¹ ç­–ç•¥**")
                
                st.markdown("**1. ç‰¹å¾æå–**")
                st.info("å†»ç»“é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè®­ç»ƒåˆ†ç±»å¤´")
                
                st.markdown("**2. å¾®è°ƒ**")
                st.info("è§£å†»éƒ¨åˆ†å±‚ï¼Œè¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒ")
                
                st.markdown("**3. æ¸è¿›è§£å†»**")
                st.info("é€æ­¥è§£å†»æ›´å¤šå±‚è¿›è¡Œè®­ç»ƒ")
                
                # å­¦ä¹ ç‡å»ºè®®
                st.markdown("**ğŸ’¡ å­¦ä¹ ç‡å»ºè®®**")
                if freeze_backbone:
                    st.success("å†»ç»“éª¨å¹²ç½‘ç»œï¼šå­¦ä¹ ç‡ 0.001-0.01")
                else:
                    st.success("å¾®è°ƒæ¨¡å¼ï¼šå­¦ä¹ ç‡ 0.0001-0.001")
        
        elif dl_mode == "æ¨¡å‹è¯„ä¼°":
            st.markdown("##### ğŸ“Š æ¨¡å‹è¯„ä¼°")
            
            st.info("ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°")
            
            # æ¨¡å‹ä¸Šä¼ 
            model_file = st.file_uploader("ä¸Šä¼ æ¨¡å‹æ–‡ä»¶ (.pth)", type=['pth'], key="model_upload")
            
            if model_file:
                st.success("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                
                # è¯„ä¼°é€‰é¡¹
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡**")
                    show_confusion_matrix = st.checkbox("æ··æ·†çŸ©é˜µ", value=True)
                    show_classification_report = st.checkbox("åˆ†ç±»æŠ¥å‘Š", value=True)
                    show_roc_curve = st.checkbox("ROCæ›²çº¿", value=False)
                    show_precision_recall = st.checkbox("ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿", value=False)
                
                with col2:
                    st.markdown("**ğŸ¯ æµ‹è¯•æ•°æ®**")
                    test_files = st.file_uploader(
                        "ä¸Šä¼ æµ‹è¯•æ•°æ®", 
                        type=['jpg','jpeg','png'], 
                        accept_multiple_files=True,
                        key="test_data"
                    )
                    
                    if test_files:
                        st.info(f"æµ‹è¯•æ•°æ®ï¼š{len(test_files)} å¼ å›¾åƒ")
                
                if st.button("å¼€å§‹è¯„ä¼°", key="start_evaluation"):
                    if test_files:
                        with st.spinner("ğŸ”„ è¯„ä¼°ä¸­..."):
                            # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
                            st.success("è¯„ä¼°å®Œæˆï¼")
                            
                            # æ¨¡æ‹Ÿç»“æœ
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("å‡†ç¡®ç‡", "94.2%")
                            with col2:
                                st.metric("ç²¾ç¡®ç‡", "92.8%")
                            with col3:
                                st.metric("å¬å›ç‡", "91.5%")
                            with col4:
                                st.metric("F1åˆ†æ•°", "92.1%")
        
        elif dl_mode == "æ¨¡å‹éƒ¨ç½²":
            st.markdown("##### ğŸš€ æ¨¡å‹éƒ¨ç½²")
            
            st.info("å°†è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²ä¸ºONNXæ ¼å¼ï¼Œç”¨äºç”Ÿäº§ç¯å¢ƒ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ“¦ æ¨¡å‹è½¬æ¢**")
                
                # æ¨¡å‹æ ¼å¼é€‰æ‹©
                input_format = st.selectbox("è¾“å…¥æ ¼å¼", ["PyTorch (.pth)", "TensorFlow (.h5)", "Keras (.h5)"])
                output_format = st.selectbox("è¾“å‡ºæ ¼å¼", ["ONNX (.onnx)", "TensorRT (.engine)", "OpenVINO (.xml)"])
                
                # è¾“å…¥å°ºå¯¸
                input_height = st.number_input("è¾“å…¥é«˜åº¦", min_value=224, max_value=512, value=224)
                input_width = st.number_input("è¾“å…¥å®½åº¦", min_value=224, max_value=512, value=224)
                input_channels = st.number_input("è¾“å…¥é€šé“æ•°", min_value=1, max_value=3, value=3)
                
                if st.button("è½¬æ¢æ¨¡å‹", key="convert_model"):
                    st.success("æ¨¡å‹è½¬æ¢æˆåŠŸï¼")
                    st.download_button(
                        "ä¸‹è½½è½¬æ¢åçš„æ¨¡å‹",
                        data=b"mock_model_data",
                        file_name="converted_model.onnx",
                        mime="application/octet-stream"
                    )
            
            with col2:
                st.markdown("**âš¡ æ€§èƒ½ä¼˜åŒ–**")
                
                # ä¼˜åŒ–é€‰é¡¹
                quantization = st.checkbox("é‡åŒ–ä¼˜åŒ–", value=True)
                pruning = st.checkbox("æ¨¡å‹å‰ªæ", value=False)
                distillation = st.checkbox("çŸ¥è¯†è’¸é¦", value=False)
                
                if quantization:
                    st.info("é‡åŒ–å¯ä»¥å‡å°‘æ¨¡å‹å¤§å°ï¼Œæé«˜æ¨ç†é€Ÿåº¦")
                
                if pruning:
                    st.info("å‰ªæå¯ä»¥ç§»é™¤ä¸é‡è¦çš„è¿æ¥ï¼Œå‡å°‘è®¡ç®—é‡")
                
                if distillation:
                    st.info("çŸ¥è¯†è’¸é¦å¯ä»¥ç”¨å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹çš„çŸ¥è¯†")
                
                # æ€§èƒ½æŒ‡æ ‡
                st.markdown("**ğŸ“Š æ€§èƒ½æŒ‡æ ‡**")
                st.metric("æ¨¡å‹å¤§å°", "12.5 MB")
                st.metric("æ¨ç†æ—¶é—´", "45 ms")
                st.metric("å†…å­˜å ç”¨", "128 MB")
                st.metric("å‡†ç¡®ç‡", "94.2%")

# çŸ¥è¯†åº“æ ‡ç­¾é¡µ
with tabs[5]:
    st.markdown("## ğŸ“š çŸ¥è¯†åº“")
    if not KNOWLEDGE_BASE_AVAILABLE:
        st.error("çŸ¥è¯†åº“æ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ knowledge_base.py æ–‡ä»¶")
    else:
        kb = KnowledgeBase()
        
        tab_kb_search, tab_kb_add = st.tabs(["æœç´¢çŸ¥è¯†", "æ·»åŠ çŸ¥è¯†"])
        
        with tab_kb_search:
            col1, col2 = st.columns([3, 1])
            with col1:
                search_keyword = st.text_input("æœç´¢å…³é”®è¯", "", key="kb_search_keyword")
            with col2:
                search_category = st.selectbox("ç±»åˆ«", ["å…¨éƒ¨", "ç—…å®³çŸ¥è¯†", "ä¿®å¤æ–¹æ³•", "ææ–™ç‰¹æ€§", "æ£€æµ‹æŠ€æœ¯", "å…¶ä»–"], key="kb_search_category")
            
            col3, col4 = st.columns(2)
            with col3:
                search_material = st.selectbox("æè´¨ç±»å‹", ["å…¨éƒ¨"] + MATERIAL_OPTIONS[1:], key="kb_search_material")
            with col4:
                search_disease = st.selectbox("ç—…å®³ç±»å‹", ["å…¨éƒ¨", "è£‚ç¼", "å‰¥è½", "è¤ªè‰²", "æ±¡æ¸/éœ‰æ–‘", "ç›èš€/é£åŒ–", "ç”Ÿç‰©é™„ç€"], key="kb_search_disease")
            
            if st.button("æœç´¢", type="primary", key="kb_search_btn"):
                results = kb.search_knowledge(
                    keyword=search_keyword if search_keyword else None,
                    category=search_category if search_category != "å…¨éƒ¨" else None,
                    material_type=search_material if search_material != "å…¨éƒ¨" else None,
                    disease_type=search_disease if search_disease != "å…¨éƒ¨" else None
                )
                
                if results:
                    st.success(f"æ‰¾åˆ° {len(results)} æ¡çŸ¥è¯†")
                    for item in results:
                        with st.expander(f"ğŸ“– {item['title']} ({item['category']})"):
                            st.write("**å†…å®¹ï¼š**")
                            st.write(item['content'])
                            if item['tags']:
                                st.write("**æ ‡ç­¾ï¼š**", ", ".join(item['tags']))
                            st.caption(f"åˆ›å»ºæ—¶é—´: {item['created_at']} | æµè§ˆæ¬¡æ•°: {item['view_count']}")
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†")
        
        with tab_kb_add:
            st.markdown("### æ·»åŠ æ–°çŸ¥è¯†")
            with st.form("add_knowledge_form"):
                kb_title = st.text_input("æ ‡é¢˜ *", "", key="kb_add_title")
                kb_category = st.selectbox("ç±»åˆ« *", ["ç—…å®³çŸ¥è¯†", "ä¿®å¤æ–¹æ³•", "ææ–™ç‰¹æ€§", "æ£€æµ‹æŠ€æœ¯", "å…¶ä»–"], key="kb_add_category")
                kb_content = st.text_area("å†…å®¹ *", height=200, key="kb_add_content")
                kb_tags = st.text_input("æ ‡ç­¾ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰", "", key="kb_add_tags")
                kb_material = st.selectbox("æè´¨ç±»å‹", ["æ— "] + MATERIAL_OPTIONS[1:], key="kb_add_material")
                kb_disease = st.selectbox("ç—…å®³ç±»å‹", ["æ— ", "è£‚ç¼", "å‰¥è½", "è¤ªè‰²", "æ±¡æ¸/éœ‰æ–‘", "ç›èš€/é£åŒ–", "ç”Ÿç‰©é™„ç€"], key="kb_add_disease")
                kb_author = st.text_input("ä½œè€…", "", key="kb_add_author")
                
                if st.form_submit_button("æäº¤", type="primary"):
                    if kb_title and kb_content:
                        tags_list = [t.strip() for t in kb_tags.split(",") if t.strip()] if kb_tags else None
                        kb_id = kb.add_knowledge(
                            title=kb_title,
                            category=kb_category,
                            content=kb_content,
                            tags=tags_list,
                            material_type=kb_material if kb_material != "æ— " else None,
                            disease_type=kb_disease if kb_disease != "æ— " else None,
                            author=kb_author if kb_author else None
                        )
                        st.success(f"çŸ¥è¯†æ·»åŠ æˆåŠŸï¼ID: {kb_id}")
                    else:
                        st.error("è¯·å¡«å†™æ ‡é¢˜å’Œå†…å®¹")

# æ¡ˆä¾‹åº“æ ‡ç­¾é¡µ
with tabs[6]:
    st.markdown("## ğŸ“‹ æ¡ˆä¾‹åº“")
    if not KNOWLEDGE_BASE_AVAILABLE:
        st.error("æ¡ˆä¾‹åº“æ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ knowledge_base.py æ–‡ä»¶")
    else:
        case_lib = CaseLibrary()
        
        tab_case_search, tab_case_add = st.tabs(["æœç´¢æ¡ˆä¾‹", "æ·»åŠ æ¡ˆä¾‹"])
        
        with tab_case_search:
            col1, col2 = st.columns([3, 1])
            with col1:
                case_keyword = st.text_input("æœç´¢å…³é”®è¯", "", key="case_search_keyword")
            with col2:
                case_material = st.selectbox("æè´¨ç±»å‹", ["å…¨éƒ¨"] + MATERIAL_OPTIONS[1:], key="case_search_material")
            
            col3, col4 = st.columns(2)
            with col3:
                case_disease = st.selectbox("ç—…å®³ç±»å‹", ["å…¨éƒ¨", "è£‚ç¼", "å‰¥è½", "è¤ªè‰²", "æ±¡æ¸/éœ‰æ–‘", "ç›èš€/é£åŒ–", "ç”Ÿç‰©é™„ç€"], key="case_search_disease")
            with col4:
                case_severity = st.selectbox("ä¸¥é‡ç¨‹åº¦", ["å…¨éƒ¨", "è½»å¾®", "ä¸­ç­‰", "ä¸¥é‡"], key="case_search_severity")
            
            if st.button("æœç´¢æ¡ˆä¾‹", type="primary", key="case_search_btn"):
                results = case_lib.search_cases(
                    keyword=case_keyword if case_keyword else None,
                    material_type=case_material if case_material != "å…¨éƒ¨" else None,
                    disease_type=case_disease if case_disease != "å…¨éƒ¨" else None,
                    severity_level=case_severity if case_severity != "å…¨éƒ¨" else None
                )
                
                if results:
                    st.success(f"æ‰¾åˆ° {len(results)} ä¸ªæ¡ˆä¾‹")
                    for case in results:
                        with st.expander(f"ğŸ“ {case['title']} - {case.get('location', 'æœªçŸ¥ä½ç½®')}"):
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write("**æè´¨ï¼š**", case.get('material_type', 'æœªçŸ¥'))
                                st.write("**å¹´ä»£ï¼š**", case.get('era', 'æœªçŸ¥'))
                                st.write("**ç—…å®³ç±»å‹ï¼š**", ", ".join(case.get('disease_types', [])))
                            with col2:
                                st.write("**ä¸¥é‡ç¨‹åº¦ï¼š**", case.get('severity_level', 'æœªçŸ¥'))
                                st.write("**åˆ›å»ºæ—¶é—´ï¼š**", case['created_at'])
                                st.write("**æµè§ˆæ¬¡æ•°ï¼š**", case['view_count'])
                            
                            if case.get('description'):
                                st.write("**æè¿°ï¼š**", case['description'])
                            
                            if case.get('diagnosis_result'):
                                with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=False):
                                    st.markdown(case['diagnosis_result'])
                            
                            if case.get('treatment_plan'):
                                with st.expander("ğŸ”§ ä¿®å¤æ–¹æ¡ˆ", expanded=False):
                                    st.markdown(case['treatment_plan'])
                            
                            if case.get('treatment_result'):
                                with st.expander("âœ… ä¿®å¤ç»“æœ", expanded=False):
                                    st.markdown(case['treatment_result'])
                            
                            # æ˜¾ç¤ºä½œè€…ä¿¡æ¯
                            if case.get('author'):
                                st.caption(f"ğŸ“ æäº¤äººï¼š{case['author']}")
                            
                            # æ˜¾ç¤ºæ ‡ç­¾
                            if case.get('tags'):
                                tags_display = " ".join([f"`{tag}`" for tag in case['tags']])
                                st.markdown(f"**æ ‡ç­¾ï¼š** {tags_display}")
                            
                            if case.get('before_images'):
                                st.write("**ä¿®å¤å‰å›¾ç‰‡ï¼š**")
                                col_img1, col_img2, col_img3 = st.columns(3)
                                for i, img_path in enumerate(case['before_images'][:3]):  # æœ€å¤šæ˜¾ç¤º3å¼ 
                                    if os.path.exists(img_path):
                                        with [col_img1, col_img2, col_img3][i]:
                                            st.image(img_path, use_container_width=True)
                                            
                            if case.get('after_images'):
                                st.write("**ä¿®å¤åå›¾ç‰‡ï¼š**")
                                col_img1, col_img2, col_img3 = st.columns(3)
                                for i, img_path in enumerate(case['after_images'][:3]):  # æœ€å¤šæ˜¾ç¤º3å¼ 
                                    if os.path.exists(img_path):
                                        with [col_img1, col_img2, col_img3][i]:
                                            st.image(img_path, use_container_width=True)
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸å…³æ¡ˆä¾‹")
        
        with tab_case_add:
            st.markdown("### æ·»åŠ æ–°æ¡ˆä¾‹")
            with st.form("add_case_form"):
                case_title = st.text_input("æ¡ˆä¾‹æ ‡é¢˜ *", "", key="case_add_title")
                case_location = st.text_input("ä½ç½®", "", key="case_add_location")
                case_material = st.selectbox("æè´¨ç±»å‹", ["æ— "] + MATERIAL_OPTIONS[1:], key="case_add_material")
                case_era = st.text_input("å¹´ä»£", "", key="case_add_era")
                case_diseases = st.multiselect("ç—…å®³ç±»å‹", ["è£‚ç¼", "å‰¥è½", "è¤ªè‰²", "æ±¡æ¸/éœ‰æ–‘", "ç›èš€/é£åŒ–", "ç”Ÿç‰©é™„ç€"], key="case_add_diseases")
                case_severity = st.selectbox("ä¸¥é‡ç¨‹åº¦", ["è½»å¾®", "ä¸­ç­‰", "ä¸¥é‡"], key="case_add_severity")
                case_description = st.text_area("æ¡ˆä¾‹æè¿°", height=150, key="case_add_description")
                case_diagnosis = st.text_area("è¯Šæ–­ç»“æœ", height=100, key="case_add_diagnosis")
                case_treatment = st.text_area("ä¿®å¤æ–¹æ¡ˆ", height=100, key="case_add_treatment")
                case_treatment_result = st.text_area("ä¿®å¤ç»“æœ", height=100, key="case_add_treatment_result")
                
                st.markdown("**å›¾ç‰‡ä¸Šä¼ **")
                case_before_images = st.file_uploader("ä¿®å¤å‰å›¾ç‰‡", type=['jpg', 'jpeg', 'png'], accept_multiple_files=True, key="case_add_before_images")
                case_after_images = st.file_uploader("ä¿®å¤åå›¾ç‰‡", type=['jpg', 'jpeg', 'png'], accept_multiple_files=True, key="case_add_after_images")
                case_process_images = st.file_uploader("ä¿®å¤è¿‡ç¨‹å›¾ç‰‡", type=['jpg', 'jpeg', 'png'], accept_multiple_files=True, key="case_add_process_images")
                
                case_author = st.text_input("æäº¤äºº", "", key="case_add_author")
                
                if st.form_submit_button("æäº¤æ¡ˆä¾‹", type="primary"):
                    if case_title:
                        before_imgs = [img.read() for img in case_before_images] if case_before_images else None
                        after_imgs = [img.read() for img in case_after_images] if case_after_images else None
                        process_imgs = [img.read() for img in case_process_images] if case_process_images else None
                        
                        case_id = case_lib.add_case(
                            title=case_title,
                            location=case_location if case_location else None,
                            material_type=case_material if case_material != "æ— " else None,
                            era=case_era if case_era else None,
                            disease_types=case_diseases if case_diseases else None,
                            severity_level=case_severity,
                            description=case_description if case_description else None,
                            diagnosis_result=case_diagnosis if case_diagnosis else None,
                            treatment_plan=case_treatment if case_treatment else None,
                            treatment_result=case_treatment_result if case_treatment_result else None,
                            before_images=before_imgs,
                            after_images=after_imgs,
                            process_images=process_imgs,
                            author=case_author if case_author else None
                        )
                        st.success(f"æ¡ˆä¾‹æ·»åŠ æˆåŠŸï¼ID: {case_id}")
                    else:
                        st.error("è¯·å¡«å†™æ¡ˆä¾‹æ ‡é¢˜")

# ç§»åŠ¨ç«¯é‡‡é›†æ ‡ç­¾é¡µ
with tabs[7]:
    st.markdown("## ğŸ“± ç§»åŠ¨ç«¯æ•°æ®é‡‡é›†")
    st.info("""
    **ç§»åŠ¨ç«¯é‡‡é›†åŠŸèƒ½è¯´æ˜ï¼š**
    
    1. å¯åŠ¨ç§»åŠ¨ç«¯APIæœåŠ¡ï¼šè¿è¡Œ `python mobile_collection_api.py`
    2. APIåœ°å€ï¼š`http://your-server-ip:8001`
    3. ç§»åŠ¨ç«¯å¯ä»¥é€šè¿‡APIä¸Šä¼ å›¾ç‰‡ã€ä½ç½®ä¿¡æ¯ã€ç—…å®³æ ‡æ³¨ç­‰æ•°æ®
    4. æ”¯æŒGPSå®šä½ã€è®¾å¤‡ä¿¡æ¯è®°å½•ã€æ‰¹é‡ä¸Šä¼ ç­‰åŠŸèƒ½
    """)
    
    st.markdown("### APIæ¥å£æ–‡æ¡£")
    
    with st.expander("ğŸ“¤ ä¸Šä¼ é‡‡é›†æ•°æ®"):
        st.code("""
POST /api/mobile/upload
Content-Type: multipart/form-data

å‚æ•°ï¼š
- file: å›¾ç‰‡æ–‡ä»¶
- device_id: è®¾å¤‡ID
- device_info: è®¾å¤‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
- location_lat: çº¬åº¦ï¼ˆå¯é€‰ï¼‰
- location_lng: ç»åº¦ï¼ˆå¯é€‰ï¼‰
- location_name: ä½ç½®åç§°ï¼ˆå¯é€‰ï¼‰
- disease_types: ç—…å®³ç±»å‹JSONæ•°ç»„ï¼ˆå¯é€‰ï¼‰
- severity_level: ä¸¥é‡ç¨‹åº¦ï¼ˆå¯é€‰ï¼‰
- material_type: æè´¨ç±»å‹ï¼ˆå¯é€‰ï¼‰
- notes: å¤‡æ³¨ï¼ˆå¯é€‰ï¼‰
        """)
    
    with st.expander("ğŸ“‹ è·å–é‡‡é›†åˆ—è¡¨"):
        st.code("""
GET /api/mobile/collections?device_id=xxx&limit=50&offset=0
        """)
    
    with st.expander("ğŸ“„ è·å–é‡‡é›†è¯¦æƒ…"):
        st.code("""
GET /api/mobile/collection/{collection_id}
        """)
    
    with st.expander("ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯"):
        st.code("""
GET /api/mobile/stats?device_id=xxx
        """)
    
    st.markdown("### é‡‡é›†æ•°æ®æŸ¥çœ‹")
    if st.button("åˆ·æ–°é‡‡é›†æ•°æ®", key="mobile_refresh_btn"):
        st.rerun()
    
    st.markdown("**æç¤ºï¼š** éœ€è¦å¯åŠ¨ç§»åŠ¨ç«¯APIæœåŠ¡æ‰èƒ½æŸ¥çœ‹é‡‡é›†æ•°æ®")

# footer - ä½¿ç”¨æ”¹è¿›çš„é¡µè„šæ ·å¼
if IMPROVED_UI_AVAILABLE:
    create_footer()
else:
    st.markdown(f"<div style='text-align:center;color:#666;margin-top:32px;'>Â© {datetime.now().year} ä¸Šæµ·äº¤é€šå¤§å­¦è®¾è®¡å­¦é™¢æ–‡ç‰©ä¿®å¤å›¢é˜Ÿ</div>", unsafe_allow_html=True)

# If cached results exist, allow re-render with current toggles without re-uploading
if st.session_state.get("proc") is not None and (uploaded is None or not analyze_btn):
    cache = st.session_state["proc"]
    img_rgb = cache['img_rgb']
    masks = cache['masks']
    boxes_map = cache['boxes']
    h, w = cache['shape']

    mask_crack = masks['crack']; mask_peel = masks['peel']; mask_disc = masks['disc']
    mask_stain = masks['stain']; mask_salt = masks['salt']; mask_bio = masks['bio']
    boxes_crack = boxes_map['crack']; boxes_peel = boxes_map['peel']; boxes_disc = boxes_map['disc']
    boxes_stain = boxes_map['stain']; boxes_salt = boxes_map['salt']; boxes_bio = boxes_map['bio']

    annotated = img_rgb.copy()
    def draw_boxes(boxes, color, visible, tag_text):
        if not visible or display_mode == "ä»…æ©è†œ":
            return
        for (x,y,w_,h_) in boxes:
            if w_*h_ < min_area:
                continue
            cv2.rectangle(annotated, (x,y), (x+w_, y+h_), color, 2)
            if show_labels:
                tx = x; ty = max(0, y-8)
                text = tag_text
                (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)
                cv2.rectangle(annotated, (tx, ty-th-4), (tx+tw+4, ty+2), (0,0,0), -1)
                cv2.putText(annotated, text, (tx+2, ty-2), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1, cv2.LINE_AA)

    if label_lang == "ä¸­æ–‡":
        crack_t, peel_t, disc_t, stain_t, salt_t, bio_t = "è£‚", "å‰¥", "è¤ª", "æ±¡", "ç›", "ç”Ÿ"
    else:
        crack_t, peel_t, disc_t, stain_t, salt_t, bio_t = "CR", "PL", "DC", "ST", "SA", "BIO"

    draw_boxes(boxes_crack, (255,0,0), show_crack, crack_t)
    draw_boxes(boxes_peel, (0,255,0), show_peel, peel_t)
    draw_boxes(boxes_disc, (0,0,255), show_disc, disc_t)
    draw_boxes(boxes_stain, (255,255,0), show_stain, stain_t)
    draw_boxes(boxes_salt, (0,255,255), show_salt, salt_t)
    draw_boxes(boxes_bio, (255,0,255), show_bio, bio_t)

    def overlay_mask(base_rgb, mask, color_rgb, alpha=0.35):
        overlay = base_rgb.copy()
        mask_bool = mask > 0
        overlay[mask_bool] = (overlay[mask_bool] * (1-alpha) + np.array(color_rgb) * alpha).astype(np.uint8)
        return overlay

    if display_mode in ("ä»…æ©è†œ", "è¾¹æ¡†+æ©è†œ"):
        if show_crack:
            annotated = overlay_mask(annotated, mask_crack, (255,0,0), alpha=0.25)
        if show_peel:
            annotated = overlay_mask(annotated, mask_peel, (0,255,0), alpha=0.18)
        if show_disc:
            annotated = overlay_mask(annotated, mask_disc, (0,0,255), alpha=0.18)
        if show_stain:
            annotated = overlay_mask(annotated, mask_stain, (255,255,0), alpha=0.20)
        if show_salt:
            annotated = overlay_mask(annotated, mask_salt, (0,255,255), alpha=0.18)
        if show_bio:
            annotated = overlay_mask(annotated, mask_bio, (255,0,255), alpha=0.20)

    st.subheader("åˆ†æç»“æœï¼ˆå¸¦æ ‡æ³¨ï¼‰")
    st.image(annotated, width='stretch')

    legend_html = """
    <div style='display:flex;flex-wrap:wrap;gap:12px;margin:6px 0 10px 0;'>
      <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#ff0000;'></span><span>è£‚ç¼</span></div>
      <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#00cc00;'></span><span>å‰¥è½</span></div>
      <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#0000ff;'></span><span>è¤ªè‰²</span></div>
      <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#ffff00;'></span><span>æ±¡æ¸/éœ‰æ–‘</span></div>
      <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#00ffff;'></span><span>ç›èš€/é£åŒ–</span></div>
      <div style='display:flex;align-items:center;gap:6px;'><span style='display:inline-block;width:14px;height:14px;background:#ff00ff;'></span><span>ç”Ÿç‰©é™„ç€</span></div>
    </div>
    """
    st.markdown(legend_html, unsafe_allow_html=True)
    # è‰²å½©å¤åŸï¼ˆåŸºç¡€ï¼‰å·²ç§»é™¤ï¼Œä»…ä¿ç•™é«˜çº§å¤åŸåŠŸèƒ½

    total_pixels = h*w
    crack_area = int(np.sum(mask_crack>0)); peel_area = int(np.sum(mask_peel>0)); disc_area = int(np.sum(mask_disc>0))
    stain_area = int(np.sum(mask_stain>0)); salt_area = int(np.sum(mask_salt>0)); bio_area = int(np.sum(mask_bio>0))

    crack_pct = crack_area / total_pixels * 100
    peel_pct = peel_area / total_pixels * 100
    disc_pct = disc_area / total_pixels * 100
    stain_pct = stain_area / total_pixels * 100
    salt_pct = salt_area / total_pixels * 100
    bio_pct = bio_area / total_pixels * 100

    weights = MATERIAL_WEIGHTS.get(material, MATERIAL_WEIGHTS["æœªæŒ‡å®š"])
    score = (
        weights.get('crack',1.0) * crack_pct * 1.8 +
        weights.get('peel',1.0) * peel_pct * 1.2 +
        weights.get('disc',1.0) * disc_pct * 1.5 +
        weights.get('stain',1.0) * stain_pct * 0.9 +
        weights.get('salt',1.0) * salt_pct * 1.6 +
        weights.get('bio',1.0) * bio_pct * 1.1
    )
    severity = min(round(score,1), 100.0)
    if severity < 5:
        lvl = "è½»å¾®"
    elif severity < 20:
        lvl = "ä¸­ç­‰"
    else:
        lvl = "ä¸¥é‡"

    st.markdown("### ğŸ“‹ é‡åŒ–ç»“æœ")
    st.write(f"- è£‚ç¼è¦†ç›–é¢ç§¯ï¼š{crack_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {crack_pct:.3f}%")
    st.write(f"- å‰¥è½è¦†ç›–é¢ç§¯ï¼š{peel_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {peel_pct:.3f}%")
    st.write(f"- è¤ªè‰²è¦†ç›–é¢ç§¯ï¼š{disc_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {disc_pct:.3f}%")
    st.write(f"- æ±¡æ¸/éœ‰æ–‘è¦†ç›–é¢ç§¯ï¼š{stain_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {stain_pct:.3f}%")
    st.write(f"- ç›èš€/é£åŒ–è¦†ç›–é¢ç§¯ï¼š{salt_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {salt_pct:.3f}%")
    st.write(f"- ç”Ÿç‰©é™„ç€è¦†ç›–é¢ç§¯ï¼š{bio_area} åƒç´ ï¼Œçº¦å å›¾åƒé¢ç§¯ {bio_pct:.3f}%")
    st.write(f"- æè´¨ï¼š**{material}**ï¼ˆç”¨äºè°ƒæ•´è¯„åˆ†ä¸å»ºè®®ï¼‰")
    st.metric("æ•´ä½“ç—…å®³ä¸¥é‡åº¦ï¼ˆ0-100ï¼‰", f"{severity}")

    st.markdown("### ğŸ’¡ å»ºè®®ï¼ˆå¯¹ç—‡æ–¹æ¡ˆï¼‰")
    pct_map = {'crack': crack_pct,'peel': peel_pct,'disc': disc_pct,'stain': stain_pct,'salt': salt_pct,'bio': bio_pct}
    detailed_recs = build_recommendations(material, pct_map, severity)
    for r in detailed_recs:
        st.write(f"- {r}")

    # ---------------------
    # Advanced restoration system (works with cached results)
    # åŸºç¡€å¤åŸåŠŸèƒ½å·²ç§»é™¤ï¼Œä»…ä¿ç•™é«˜çº§å¤åŸåŠŸèƒ½
    # ---------------------
    if ADVANCED_RESTORATION_AVAILABLE:
        st.markdown("---")
        masks_dict = {
            'crack': mask_crack,
            'peel': mask_peel,
            'disc': mask_disc,
            'stain': mask_stain,
            'salt': mask_salt,
            'bio': mask_bio
        }
        render_advanced_restoration_ui(img_rgb, masks_dict, default_open=False)
    else:
        st.info("ğŸ’¡ æç¤ºï¼šé«˜çº§å¤åŸåŠŸèƒ½éœ€è¦ advanced_restoration.py æ¨¡å—")